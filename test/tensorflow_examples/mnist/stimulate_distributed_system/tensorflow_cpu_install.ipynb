{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 15:09:28.847610: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-15 15:09:28.886808: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-15 15:09:28.887702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 15:09:29.724558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 15:09:34.802561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-15 15:09:34.802994: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "# Set the file paths for the MNIST digit dataset files\n",
    "train_images_path = '../mnist_data/train-images-idx3-ubyte.gz'\n",
    "train_labels_path = '../mnist_data/train-labels-idx1-ubyte.gz'\n",
    "test_images_path = '../mnist_data/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_path = '../mnist_data/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "\n",
    "# Load the MNIST digit dataset files into numpy arrays\n",
    "with gzip.open(train_images_path, 'rb') as f:\n",
    "    x_train = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
    "    \n",
    "with gzip.open(train_labels_path, 'rb') as f:\n",
    "    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "with gzip.open(test_images_path, 'rb') as f:\n",
    "    x_test = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
    "    \n",
    "with gzip.open(test_labels_path, 'rb') as f:\n",
    "    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    \n",
    "    \n",
    "# to make the value on each pixel in range (0,1)\n",
    "x_train = x_train / 255    \n",
    "x_test = x_test / 255    \n",
    "\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
    "\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "# change the type to be ready for the training process\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "\n",
    "# with gpu:\n",
    "class LowLeNet(Model):\n",
    "    def __init__(self, input_shape=(32, 32, 1), nb_classes=10):\n",
    "        super().__init__()\n",
    "        self.create_model(input_shape = input_shape, nb_classes = nb_classes)\n",
    "        # initialize some useful properties to be able to emulate the behavior of a distributed sytem on a local machine\n",
    "        self.global_weights_path = './result/global'\n",
    "        self.parent_upload_folder = './result/local'\n",
    "        # weights related\n",
    "        self.global_weights = []\n",
    "        self.previous_weights = []\n",
    "        self.current_global_version = 1\n",
    "        self.current_global_path = ''\n",
    "        # info\n",
    "        self.dataset_size = 10000\n",
    "        self.worker_name = 'worker1'\n",
    "        self.result = []\n",
    "        self.direction = []\n",
    "\n",
    "        \n",
    "    def create_model(self, input_shape, nb_classes):\n",
    "        self.conv1 = Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"valid\")\n",
    "        self.avgpool1 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.conv2 = Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid')\n",
    "        self.avgpool2 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(120, activation='tanh')\n",
    "        self.dense2 = Dense(84, activation='tanh')\n",
    "        self.dense3 = Dense(nb_classes, activation='softmax')\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def compile(self, loss = \"categorical_crossentropy\", optimizer = \"Adam\", metric = \"accuracy\"):\n",
    "        # Now, for this model, support only this set of choice \n",
    "        if loss == \"categorical_crossentropy\":\n",
    "        # define loss = Categorical Crossentropy\n",
    "            self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        if optimizer == \"Adam\":\n",
    "        # define optimizer = Adam\n",
    "            self.optimizer = tf.keras.optimizers.Adam()\n",
    "        if metric == \"accuracy\":\n",
    "            # setting up metric = accuracy\n",
    "            self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "            self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "            self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "            self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = self(images, training=False)\n",
    "        t_loss = self.loss_object(labels, predictions)\n",
    "        self.test_loss(t_loss)\n",
    "        self.test_accuracy(labels, predictions)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.loss_object(labels, predictions)\n",
    "        # compute the gradient based on the loss\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        # update the weights based on some formula determined by the chosen algorithm\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # update training info\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(labels, predictions)\n",
    "\n",
    "    # Utils\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def load_pretrained_model(self, weights, train_ds):\n",
    "        num_of_layers= len(self.get_weights())\n",
    "        if num_of_layers < len(weights):\n",
    "            for images, labels in train_ds:\n",
    "                self.train_step(images, labels)\n",
    "                break\n",
    "        self.set_weights(weights)\n",
    "\n",
    "    # file related\n",
    "    def get_latest_global_version(self):\n",
    "        path = self.global_weights_path\n",
    "        num = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "        return num\n",
    "    \n",
    "    def get_latest_global_weights(self, version_num):\n",
    "        keyword = f\"version{version_num}\"\n",
    "        for file_name in os.listdir(self.global_weights_path):\n",
    "            if keyword in file_name:\n",
    "                file_path = os.path.join(self.global_weights_path, file_name)\n",
    "                self.current_global_path = file_path\n",
    "                break\n",
    "        with open(file_path, 'rb') as f:\n",
    "            weights = pickle.load(f)\n",
    "        return weights \n",
    "\n",
    "    def upload_weights_to_file(self, local_epoch):\n",
    "        # global_epoch_num = 0\n",
    "        file_name = f\"{self.worker_name}_{self.dataset_size}_{local_epoch}_{self.current_global_version}.pkl\"\n",
    "        path = self.parent_upload_folder\n",
    "        global_upload_folder_num = len([f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))])\n",
    "        folder_name = f\"epoch{global_upload_folder_num}\"\n",
    "        upload_path = os.path.join(self.parent_upload_folder, folder_name, file_name)\n",
    "        with open(upload_path, 'wb') as f:\n",
    "            pickle.dump(self.get_weights(), f)\n",
    "        return upload_path\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def merge(self):\n",
    "\n",
    "        # updating the value in each parameters of the local model\n",
    "        # calculate the different to decide the formula of updating\n",
    "            # the calculation is performed on each corresponding layout\n",
    "        # the different between the current weight and the previous weights\n",
    "        if len(self.previous_weights) < len(self.current_weights):\n",
    "            # applying when entering the training step with an initialized model (8 layers), not the pretrained one (18 layers)\n",
    "            # in the 1st batch of the 1st epoch\n",
    "            e_local = self.current_weights\n",
    "        else:\n",
    "            # perform calculation normally for all the other cases\n",
    "            e_local = [layer_a - layer_b for layer_a, layer_b in zip(self.current_weights, self.previous_weights)]\n",
    "\n",
    "        # the different between the global weights and the current weights\n",
    "        e_global = [layer_a - layer_b for layer_a, layer_b in zip(self.global_weights, self.current_weights)]\n",
    "\n",
    "        # check the dimension of these variables to see whether it fits one another\n",
    "        print(f\"total layers of previous weights: {len(self.previous_weights)}, total layers of current weights: {len(self.current_weights)}\")\n",
    "        print(f\"total layers of e_local: {len(e_local)}, total layers of e_global: {len(e_global)}\")\n",
    "            # get the direction list (matrix)\n",
    "        self.direction = [np.multiply(a, b) for a, b in zip(e_local, e_global)]\n",
    "        print(f\"total layers of direction: {len(self.direction)}\")\n",
    "\n",
    "        # calculate alpha variable to ready for the merging process\n",
    "        current_global_size = int(self.current_global_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "        alpha = (self.dataset_size) / (self.dataset_size + current_global_size)\n",
    "\n",
    "        # create a blank array to store the result\n",
    "        self.result = [np.zeros(layer.shape) for layer in self.current_weights]\n",
    "            # base on the direction, global weights and current local weights\n",
    "            # updating the value of each parameter to get the new local weights (from merging process)\n",
    "        # set the index to move to the next layer\n",
    "        t = 0\n",
    "        # check some param of the merging process\n",
    "        self.total = 0\n",
    "        self.different_direction = 0\n",
    "        self.same_direction = 0\n",
    "        # access each layer of these variables correspondingly \n",
    "        for (local_layer, global_layer, direction_layer) in zip(self.current_weights, self.global_weights, self.direction):\n",
    "            # print(local_layer.shape, global_layer.shape, direction_layer.shape)\n",
    "            # access each element in each layer\n",
    "            it = np.nditer([local_layer, global_layer, direction_layer], flags=['multi_index'])\n",
    "            for local_element, global_element, direction_element in it:\n",
    "                self.total += 1\n",
    "                index = it.multi_index\n",
    "\n",
    "                if direction_element >= 0:\n",
    "                    result_element = global_element\n",
    "                    self.different_direction += 1\n",
    "                else:\n",
    "                    result_element = (1 - alpha)*global_element + alpha * local_element\n",
    "                    self.same_direction += 1\n",
    "\n",
    "                self.result[t][index] = result_element\n",
    "            # move to the next layer\n",
    "            t +=1\n",
    "            # set the current weights of the model to be the result of the merging process\n",
    "        self.set_weights(self.result)\n",
    "        \n",
    "    # -------------------------------oOo-------------------------------------------\n",
    "    def train(self, train_ds, test_ds, EPOCHS = 5,):\n",
    "        # Training for multiple epoch\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Reset the metrics at the start of the next epoch\n",
    "            self.train_loss.reset_states()\n",
    "            self.train_accuracy.reset_states()\n",
    "            self.test_loss.reset_states()\n",
    "            self.test_accuracy.reset_states()\n",
    "            # self.second_weight = self.get_weights()\n",
    "\n",
    "            batch_num = 0\n",
    "            for images, labels in train_ds:\n",
    "                batch_num +=1\n",
    "                # record the local weights before it enter a new training batch\n",
    "                self.previous_weights = self.get_weights()\n",
    "                # begin the local training process of each batch\n",
    "                self.train_step(images, labels)\n",
    "                # check whether there is a new global model --> base on the number of files in the folder result/global/\n",
    "                latest_global_version = self.get_latest_global_version()\n",
    "                # if there is a new global version --> ready to merge with the local training result\n",
    "                if self.current_global_version < latest_global_version:\n",
    "                    print(f\"Merging process happens at epoch {epoch}, batch {batch_num} when receiving the global version {latest_global_version}, current global version {self.current_global_version}\")\n",
    "                    # update the current global version on local machine\n",
    "                    self.current_global_version = latest_global_version\n",
    "                    # load the new global weights \n",
    "                    self.global_weights = self.get_latest_global_weights(self.current_global_version)\n",
    "                    # get the current local weights --> the weights obtained after executing the self.train_step() function\n",
    "                    self.current_weights = self.get_weights()\n",
    "                    # check whether the number of layers of previous and current weights match\n",
    "                    if len(self.previous_weights) < len(self.current_weights):\n",
    "                        print(f\"The number of layers of previous weights: {len(self.previous_weights)}\")\n",
    "                        print(f\"The number of layers of current weights: {len(self.current_weights)}\")\n",
    "                        print(f\"The previous weights that mismatch in the number of layers with the current weights {self.previous_weights}\")\n",
    "                    # check whether the number of layers of the current weights and the global weights match\n",
    "                    if len(self.global_weights) != len(self.current_weights):\n",
    "                        print(f\"THERE IS A MISMATCH IN NUMBER OF LAYERS OF THE GLOBAL WEIGHTS AND CURRENT WEIGHTS\")\n",
    "                        print(f\"EXIT THE MERGING PROCESS\")\n",
    "                        print(f\"The number of layers of global weights: {len(self.global_weights)}\")\n",
    "                        print(f\"The number of layers of current weights: {len(self.current_weights)}\")\n",
    "                        break\n",
    "                    else:\n",
    "                    # merged based on previous local weights, current local weights and global weights\n",
    "                        self.merge()\n",
    "                        self.merged_weights = self.get_weights()\n",
    "\n",
    "                # else:\n",
    "                # if there is no new global model --> update the local weights normally \n",
    "                # --> no modification to the local weights\n",
    "                # do nothing\n",
    "    # -------------------------------oOo-------------------------------------------\n",
    "\n",
    "            for test_images, test_labels in test_ds:\n",
    "                self.test_step(test_images, test_labels)\n",
    "\n",
    "            print(\n",
    "                f'Epoch {epoch + 1}, '\n",
    "                f'Loss: {self.train_loss.result()}, '\n",
    "                f'Accuracy: {self.train_accuracy.result() * 100}, '\n",
    "                f'Test Loss: {self.test_loss.result()}, '\n",
    "                f'Test Accuracy: {self.test_accuracy.result() * 100}'\n",
    "                )\n",
    "\n",
    "            # updating local weight to appropriate directory\n",
    "            latest_global_version = self.get_latest_global_version()\n",
    "            upload_path = self.upload_weights_to_file(epoch)\n",
    "            print(f\"\\tFrom {self.worker_name} in local epoch {epoch + 1} that hold the global version {self.current_global_version}, uploading local model to the server in the path {upload_path}\")\n",
    "\n",
    "# #  create and compile model\n",
    "#     model= LowLeNet()\n",
    "#     model.compile()\n",
    "\n",
    "#     model.train(train_ds, test_ds, EPOCHS = 5)\n",
    "\n",
    "#     current_weights = model.get_weights() \n",
    "#     for layer in current_weights:\n",
    "#         print(layer.shape)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 15:09:40.113861: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-15 15:09:40.114195: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "/home/vtn_ubuntu/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-04-15 15:09:59.869397: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [10000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2269233763217926, Accuracy: 93.1816635131836, Test Loss: 0.10740773379802704, Test Accuracy: 96.73999786376953\n",
      "\tFrom worker1 in local epoch 1 that hold the global version 1, uploading local model to the server in the path ./result/local/epoch2/worker1_10000_0_1.pkl\n",
      "Epoch 2, Loss: 0.09251926839351654, Accuracy: 97.17166137695312, Test Loss: 0.07601872831583023, Test Accuracy: 97.54999542236328\n",
      "\tFrom worker1 in local epoch 2 that hold the global version 1, uploading local model to the server in the path ./result/local/epoch2/worker1_10000_1_1.pkl\n"
     ]
    }
   ],
   "source": [
    "model= LowLeNet()\n",
    "model.compile()\n",
    "\n",
    "model.train(train_ds, test_ds, EPOCHS = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 6)\n",
      "(6,)\n",
      "(5, 5, 6, 16)\n",
      "(16,)\n",
      "(256, 120)\n",
      "(120,)\n",
      "(120, 84)\n",
      "(84,)\n",
      "(84, 10)\n",
      "(10,)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_weights = model.get_weights() \n",
    "for layer in current_weights:\n",
    "    print(layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
