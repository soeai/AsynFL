{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:24:43.632600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 13:24:44.567347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:24:48.842041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-15 13:24:48.895513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-15 13:24:48.895562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "print(len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 5. 12.]\n",
      " [21. 32.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gpu = tf.device('/GPU:0')\n",
    "with gpu:\n",
    "    # Create two tensors and perform element-wise multiplication\n",
    "    a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "    b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "    c = tf.multiply(a, b)\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 5. 12.]\n",
      " [21. 32.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with gpu:\n",
    "    # Create two tensors and perform element-wise multiplication\n",
    "    a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "    b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "    c = tf.multiply(a, b)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import os\n",
    "\n",
    "# mnist_dir = './mnist_data/'\n",
    "# if not os.path.exists(mnist_dir):\n",
    "#     os.makedirs(mnist_dir)\n",
    "\n",
    "# # Download and save the MNIST data files\n",
    "# base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "# files = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "#          't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "# for file in files:\n",
    "#     url = base_url + file\n",
    "#     filepath = mnist_dir + file\n",
    "#     urllib.request.urlretrieve(url, filepath)\n",
    "#     print(f'Downloaded {file} to {mnist_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:25:10.693016: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2023-04-15 13:25:10.781233: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "# Set the file paths for the MNIST digit dataset files\n",
    "train_images_path = './mnist_data/train-images-idx3-ubyte.gz'\n",
    "train_labels_path = './mnist_data/train-labels-idx1-ubyte.gz'\n",
    "test_images_path = './mnist_data/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_path = './mnist_data/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "\n",
    "# Load the MNIST digit dataset files into numpy arrays\n",
    "with gzip.open(train_images_path, 'rb') as f:\n",
    "    x_train = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
    "    \n",
    "with gzip.open(train_labels_path, 'rb') as f:\n",
    "    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "with gzip.open(test_images_path, 'rb') as f:\n",
    "    x_test = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
    "    \n",
    "with gzip.open(test_labels_path, 'rb') as f:\n",
    "    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    \n",
    "    \n",
    "# to make the value on each pixel in range (0,1)\n",
    "x_train = x_train / 255    \n",
    "x_test = x_test / 255    \n",
    "\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
    "\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "# change the type to be ready for the training process\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "\n",
    "# with gpu:\n",
    "class LowLeNet(Model):\n",
    "    def __init__(self, input_shape=(32, 32, 1), nb_classes=10):\n",
    "        super().__init__()\n",
    "        self.create_model(input_shape = input_shape, nb_classes = nb_classes)\n",
    "        # initialize some useful properties to be able to emulate the behavior of a distributed sytem on a local machine\n",
    "        self.global_weights_path = './result/global'\n",
    "        self.parent_upload_folder = './result/local'\n",
    "        # weights related\n",
    "        self.global_weights = []\n",
    "        self.previous_weights = []\n",
    "        self.current_global_version = 1\n",
    "        self.current_global_path = ''\n",
    "        # info\n",
    "        self.dataset_size = 10000\n",
    "        self.worker_name = 'worker1'\n",
    "        self.result = []\n",
    "        self.direction = []\n",
    "\n",
    "        \n",
    "    def create_model(self, input_shape, nb_classes):\n",
    "        self.conv1 = Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"valid\")\n",
    "        self.avgpool1 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.conv2 = Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid')\n",
    "        self.avgpool2 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(120, activation='tanh')\n",
    "        self.dense2 = Dense(84, activation='tanh')\n",
    "        self.dense3 = Dense(nb_classes, activation='softmax')\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def compile(self, loss = \"categorical_crossentropy\", optimizer = \"Adam\", metric = \"accuracy\"):\n",
    "        # Now, for this model, support only this set of choice \n",
    "        if loss == \"categorical_crossentropy\":\n",
    "        # define loss = Categorical Crossentropy\n",
    "            self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        if optimizer == \"Adam\":\n",
    "        # define optimizer = Adam\n",
    "            self.optimizer = tf.keras.optimizers.Adam()\n",
    "        if metric == \"accuracy\":\n",
    "            # setting up metric = accuracy\n",
    "            self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "            self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "            self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "            self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = self(images, training=False)\n",
    "        t_loss = self.loss_object(labels, predictions)\n",
    "        self.test_loss(t_loss)\n",
    "        self.test_accuracy(labels, predictions)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.loss_object(labels, predictions)\n",
    "        # compute the gradient based on the loss\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        # update the weights based on some formula determined by the chosen algorithm\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # update training info\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(labels, predictions)\n",
    "\n",
    "    # Utils\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def load_pretrained_model(self, weights, train_ds):\n",
    "        num_of_layers= len(self.get_weights())\n",
    "        if num_of_layers < len(weights):\n",
    "            for images, labels in train_ds:\n",
    "                self.train_step(images, labels)\n",
    "                break\n",
    "        self.set_weights(weights)\n",
    "\n",
    "    # file related\n",
    "    def get_latest_global_version(self):\n",
    "        path = self.global_weights_path\n",
    "        num = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "        return num\n",
    "    \n",
    "    def get_latest_global_weights(self, version_num):\n",
    "        keyword = f\"version{version_num}\"\n",
    "        for file_name in os.listdir(self.global_weights_path):\n",
    "            if keyword in file_name:\n",
    "                file_path = os.path.join(self.global_weights_path, file_name)\n",
    "                self.current_global_path = file_path\n",
    "                break\n",
    "        with open(file_path, 'rb') as f:\n",
    "            weights = pickle.load(f)\n",
    "        return weights \n",
    "\n",
    "    def upload_weights_to_file(self, local_epoch):\n",
    "        # global_epoch_num = 0\n",
    "        file_name = f\"{self.worker_name}_{self.dataset_size}_{local_epoch}_{self.current_global_version}.pkl\"\n",
    "        path = self.parent_upload_folder\n",
    "        global_upload_folder_num = len([f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))])\n",
    "        folder_name = f\"epoch{global_upload_folder_num}\"\n",
    "        upload_path = os.path.join(self.parent_upload_folder, folder_name, file_name)\n",
    "        with open(upload_path, 'wb') as f:\n",
    "            pickle.dump(self.get_weights(), f)\n",
    "        return upload_path\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def merge(self):\n",
    "\n",
    "        # updating the value in each parameters of the local model\n",
    "        # calculate the different to decide the formula of updating\n",
    "            # the calculation is performed on each corresponding layout\n",
    "        # the different between the current weight and the previous weights\n",
    "        if len(self.previous_weights) < len(self.current_weights):\n",
    "            # applying when entering the training step with an initialized model (8 layers), not the pretrained one (18 layers)\n",
    "            # in the 1st batch of the 1st epoch\n",
    "            e_local = self.current_weights\n",
    "        else:\n",
    "            # perform calculation normally for all the other cases\n",
    "            e_local = [layer_a - layer_b for layer_a, layer_b in zip(self.current_weights, self.previous_weights)]\n",
    "\n",
    "        # the different between the global weights and the current weights\n",
    "        e_global = [layer_a - layer_b for layer_a, layer_b in zip(self.global_weights, self.current_weights)]\n",
    "\n",
    "        # check the dimension of these variables to see whether it fits one another\n",
    "        print(f\"total layers of previous weights: {len(self.previous_weights)}, total layers of current weights: {len(self.current_weights)}\")\n",
    "        print(f\"total layers of e_local: {len(e_local)}, total layers of e_global: {len(e_global)}\")\n",
    "            # get the direction list (matrix)\n",
    "        self.direction = [np.multiply(a, b) for a, b in zip(e_local, e_global)]\n",
    "        print(f\"total layers of direction: {len(self.direction)}\")\n",
    "\n",
    "        # calculate alpha variable to ready for the merging process\n",
    "        current_global_size = int(self.current_global_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "        alpha = (self.dataset_size) / (self.dataset_size + current_global_size)\n",
    "\n",
    "        # create a blank array to store the result\n",
    "        self.result = [np.zeros(layer.shape) for layer in self.current_weights]\n",
    "            # base on the direction, global weights and current local weights\n",
    "            # updating the value of each parameter to get the new local weights (from merging process)\n",
    "        # set the index to move to the next layer\n",
    "        t = 0\n",
    "        # check some param of the merging process\n",
    "        self.total = 0\n",
    "        self.different_direction = 0\n",
    "        self.same_direction = 0\n",
    "        # access each layer of these variables correspondingly \n",
    "        for (local_layer, global_layer, direction_layer) in zip(self.current_weights, self.global_weights, self.direction):\n",
    "            # print(local_layer.shape, global_layer.shape, direction_layer.shape)\n",
    "            # access each element in each layer\n",
    "            it = np.nditer([local_layer, global_layer, direction_layer], flags=['multi_index'])\n",
    "            for local_element, global_element, direction_element in it:\n",
    "                self.total += 1\n",
    "                index = it.multi_index\n",
    "\n",
    "                if direction_element >= 0:\n",
    "                    result_element = global_element\n",
    "                    self.different_direction += 1\n",
    "                else:\n",
    "                    result_element = (1 - alpha)*global_element + alpha * local_element\n",
    "                    self.same_direction += 1\n",
    "\n",
    "                self.result[t][index] = result_element\n",
    "            # move to the next layer\n",
    "            t +=1\n",
    "            # set the current weights of the model to be the result of the merging process\n",
    "        self.set_weights(self.result)\n",
    "        \n",
    "    # -------------------------------oOo-------------------------------------------\n",
    "    def train(self, train_ds, test_ds, EPOCHS = 5,):\n",
    "        # Training for multiple epoch\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Reset the metrics at the start of the next epoch\n",
    "            self.train_loss.reset_states()\n",
    "            self.train_accuracy.reset_states()\n",
    "            self.test_loss.reset_states()\n",
    "            self.test_accuracy.reset_states()\n",
    "            # self.second_weight = self.get_weights()\n",
    "\n",
    "            batch_num = 0\n",
    "            for images, labels in train_ds:\n",
    "                batch_num +=1\n",
    "                # record the local weights before it enter a new training batch\n",
    "                self.previous_weights = self.get_weights()\n",
    "                # begin the local training process of each batch\n",
    "                self.train_step(images, labels)\n",
    "                # check whether there is a new global model --> base on the number of files in the folder result/global/\n",
    "                latest_global_version = self.get_latest_global_version()\n",
    "                # if there is a new global version --> ready to merge with the local training result\n",
    "                if self.current_global_version < latest_global_version:\n",
    "                    print(f\"Merging process happens at epoch {epoch}, batch {batch_num} when receiving the global version {latest_global_version}, current global version {self.current_global_version}\")\n",
    "                    # update the current global version on local machine\n",
    "                    self.current_global_version = latest_global_version\n",
    "                    # load the new global weights \n",
    "                    self.global_weights = self.get_latest_global_weights(self.current_global_version)\n",
    "                    # get the current local weights --> the weights obtained after executing the self.train_step() function\n",
    "                    self.current_weights = self.get_weights()\n",
    "                    # check whether the number of layers of previous and current weights match\n",
    "                    if len(self.previous_weights) < len(self.current_weights):\n",
    "                        print(f\"The number of layers of previous weights: {len(self.previous_weights)}\")\n",
    "                        print(f\"The number of layers of current weights: {len(self.current_weights)}\")\n",
    "                        print(f\"The previous weights that mismatch in the number of layers with the current weights {self.previous_weights}\")\n",
    "                    # check whether the number of layers of the current weights and the global weights match\n",
    "                    if len(self.global_weights) != len(self.current_weights):\n",
    "                        print(f\"THERE IS A MISMATCH IN NUMBER OF LAYERS OF THE GLOBAL WEIGHTS AND CURRENT WEIGHTS\")\n",
    "                        print(f\"EXIT THE MERGING PROCESS\")\n",
    "                        print(f\"The number of layers of global weights: {len(self.global_weights)}\")\n",
    "                        print(f\"The number of layers of current weights: {len(self.current_weights)}\")\n",
    "                        break\n",
    "                    else:\n",
    "                    # merged based on previous local weights, current local weights and global weights\n",
    "                        self.merge()\n",
    "                        self.merged_weights = self.get_weights()\n",
    "\n",
    "                # else:\n",
    "                # if there is no new global model --> update the local weights normally \n",
    "                # --> no modification to the local weights\n",
    "                # do nothing\n",
    "    # -------------------------------oOo-------------------------------------------\n",
    "\n",
    "            for test_images, test_labels in test_ds:\n",
    "                self.test_step(test_images, test_labels)\n",
    "\n",
    "            print(\n",
    "                f'Epoch {epoch + 1}, '\n",
    "                f'Loss: {self.train_loss.result()}, '\n",
    "                f'Accuracy: {self.train_accuracy.result() * 100}, '\n",
    "                f'Test Loss: {self.test_loss.result()}, '\n",
    "                f'Test Accuracy: {self.test_accuracy.result() * 100}'\n",
    "                )\n",
    "\n",
    "            # updating local weight to appropriate directory\n",
    "            latest_global_version = self.get_latest_global_version()\n",
    "            upload_path = self.upload_weights_to_file(epoch)\n",
    "            print(f\"\\tFrom {self.worker_name} in local epoch {epoch + 1} that hold the global version {self.current_global_version}, uploading local model to the server in the path {upload_path}\")\n",
    "\n",
    "# #  create and compile model\n",
    "#     model= LowLeNet()\n",
    "#     model.compile()\n",
    "\n",
    "#     model.train(train_ds, test_ds, EPOCHS = 5)\n",
    "\n",
    "#     current_weights = model.get_weights() \n",
    "#     for layer in current_weights:\n",
    "#         print(layer.shape)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:25:17.764628: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2023-04-15 13:25:17.910489: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-15 13:25:17.910816: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "/home/vtn_ubuntu/miniconda3/envs/fedasync/lib/python3.9/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    }
   ],
   "source": [
    "model= LowLeNet()\n",
    "model.compile()\n",
    "\n",
    "model.train(train_ds, test_ds, EPOCHS = 5)\n",
    "\n",
    "current_weights = model.get_weights() \n",
    "for layer in current_weights:\n",
    "    print(layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 12:54:46.442817: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-15 12:54:46.443169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "/home/vtn_ubuntu/miniconda3/envs/fedasync/lib/python3.9/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with gpu:\n",
    "#  create and compile model\n",
    "    model= LowLeNet()\n",
    "    model.compile()\n",
    "\n",
    "    model.train(train_ds, test_ds, EPOCHS = 5)\n",
    "\n",
    "    current_weights = model.get_weights() \n",
    "    for layer in current_weights:\n",
    "        print(layer.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "\n",
    "with gpu:\n",
    "    class LowLeNet(Model):\n",
    "        def __init__(self, input_shape=(32, 32, 1), nb_classes=10):\n",
    "            super().__init__()\n",
    "            self.create_model(input_shape = input_shape, nb_classes = nb_classes)\n",
    "            # initialize some useful properties to be able to emulate the behavior of a distributed sytem on a local machine\n",
    "            self.global_weights_path = './result/global'\n",
    "            self.parent_upload_folder = './result/local'\n",
    "            # weights related\n",
    "            self.global_weights = []\n",
    "            self.previous_weights = []\n",
    "            self.current_global_version = 1\n",
    "            self.current_global_path = ''\n",
    "            # info\n",
    "            self.dataset_size = 10000\n",
    "            self.worker_name = 'worker1'\n",
    "            self.result = []\n",
    "            self.direction = []\n",
    "\n",
    "            \n",
    "        def create_model(self, input_shape, nb_classes):\n",
    "            self.conv1 = Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"valid\")\n",
    "            self.avgpool1 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "            self.conv2 = Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid')\n",
    "            self.avgpool2 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "            self.flatten = Flatten()\n",
    "            self.dense1 = Dense(120, activation='tanh')\n",
    "            self.dense2 = Dense(84, activation='tanh')\n",
    "            self.dense3 = Dense(nb_classes, activation='softmax')\n",
    "            \n",
    "\n",
    "        def call(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.avgpool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.avgpool2(x)\n",
    "            x = self.flatten(x)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "            x = self.dense3(x)\n",
    "            return x\n",
    "        \n",
    "        def compile(self, loss = \"categorical_crossentropy\", optimizer = \"Adam\", metric = \"accuracy\"):\n",
    "            # Now, for this model, support only this set of choice \n",
    "            if loss == \"categorical_crossentropy\":\n",
    "            # define loss = Categorical Crossentropy\n",
    "                self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            if optimizer == \"Adam\":\n",
    "            # define optimizer = Adam\n",
    "                self.optimizer = tf.keras.optimizers.Adam()\n",
    "            if metric == \"accuracy\":\n",
    "                # setting up metric = accuracy\n",
    "                self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "                self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "                self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "                self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "        @tf.function\n",
    "        def test_step(self, images, labels):\n",
    "        # training=False is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions = self(images, training=False)\n",
    "            t_loss = self.loss_object(labels, predictions)\n",
    "            self.test_loss(t_loss)\n",
    "            self.test_accuracy(labels, predictions)\n",
    "            \n",
    "        @tf.function\n",
    "        def train_step(self, images, labels):\n",
    "            with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "                predictions = self(images, training=True)\n",
    "                loss = self.loss_object(labels, predictions)\n",
    "            # compute the gradient based on the loss\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            # update the weights based on some formula determined by the chosen algorithm\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            # update training info\n",
    "            self.train_loss(loss)\n",
    "            self.train_accuracy(labels, predictions)\n",
    "\n",
    "        # Utils\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        def load_pretrained_model(self, weights, train_ds):\n",
    "            num_of_layers= len(self.get_weights())\n",
    "            if num_of_layers < len(weights):\n",
    "                for images, labels in train_ds:\n",
    "                    self.train_step(images, labels)\n",
    "                    break\n",
    "            self.set_weights(weights)\n",
    "\n",
    "        # file related\n",
    "        def get_latest_global_version(self):\n",
    "            path = self.global_weights_path\n",
    "            num = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "            return num\n",
    "        \n",
    "        def get_latest_global_weights(self, version_num):\n",
    "            keyword = f\"version{version_num}\"\n",
    "            for file_name in os.listdir(self.global_weights_path):\n",
    "                if keyword in file_name:\n",
    "                    file_path = os.path.join(self.global_weights_path, file_name)\n",
    "                    self.current_global_path = file_path\n",
    "                    break\n",
    "            with open(file_path, 'rb') as f:\n",
    "                weights = pickle.load(f)\n",
    "            return weights \n",
    "\n",
    "        def upload_weights_to_file(self, local_epoch):\n",
    "            # global_epoch_num = 0\n",
    "            file_name = f\"{self.worker_name}_{self.dataset_size}_{local_epoch}_{self.current_global_version}.pkl\"\n",
    "            path = self.parent_upload_folder\n",
    "            global_upload_folder_num = len([f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))])\n",
    "            folder_name = f\"epoch{global_upload_folder_num}\"\n",
    "            upload_path = os.path.join(self.parent_upload_folder, folder_name, file_name)\n",
    "            with open(upload_path, 'wb') as f:\n",
    "                pickle.dump(self.get_weights(), f)\n",
    "            return upload_path\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        def merge(self):\n",
    "\n",
    "            # updating the value in each parameters of the local model\n",
    "            # calculate the different to decide the formula of updating\n",
    "                # the calculation is performed on each corresponding layout\n",
    "            # the different between the current weight and the previous weights\n",
    "            if len(self.previous_weights) < len(self.current_weights):\n",
    "                # applying when entering the training step with an initialized model (8 layers), not the pretrained one (18 layers)\n",
    "                # in the 1st batch of the 1st epoch\n",
    "                e_local = self.current_weights\n",
    "            else:\n",
    "                # perform calculation normally for all the other cases\n",
    "                e_local = [layer_a - layer_b for layer_a, layer_b in zip(self.current_weights, self.previous_weights)]\n",
    "\n",
    "            # the different between the global weights and the current weights\n",
    "            e_global = [layer_a - layer_b for layer_a, layer_b in zip(self.global_weights, self.current_weights)]\n",
    "\n",
    "            # check the dimension of these variables to see whether it fits one another\n",
    "            print(f\"total layers of previous weights: {len(self.previous_weights)}, total layers of current weights: {len(self.current_weights)}\")\n",
    "            print(f\"total layers of e_local: {len(e_local)}, total layers of e_global: {len(e_global)}\")\n",
    "                # get the direction list (matrix)\n",
    "            self.direction = [np.multiply(a, b) for a, b in zip(e_local, e_global)]\n",
    "            print(f\"total layers of direction: {len(self.direction)}\")\n",
    "\n",
    "            # calculate alpha variable to ready for the merging process\n",
    "            current_global_size = int(self.current_global_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            alpha = (self.dataset_size) / (self.dataset_size + current_global_size)\n",
    "\n",
    "            # create a blank array to store the result\n",
    "            self.result = [np.zeros(layer.shape) for layer in self.current_weights]\n",
    "                # base on the direction, global weights and current local weights\n",
    "                # updating the value of each parameter to get the new local weights (from merging process)\n",
    "            # set the index to move to the next layer\n",
    "            t = 0\n",
    "            # check some param of the merging process\n",
    "            self.total = 0\n",
    "            self.different_direction = 0\n",
    "            self.same_direction = 0\n",
    "            # access each layer of these variables correspondingly \n",
    "            for (local_layer, global_layer, direction_layer) in zip(self.current_weights, self.global_weights, self.direction):\n",
    "                # print(local_layer.shape, global_layer.shape, direction_layer.shape)\n",
    "                # access each element in each layer\n",
    "                it = np.nditer([local_layer, global_layer, direction_layer], flags=['multi_index'])\n",
    "                for local_element, global_element, direction_element in it:\n",
    "                    self.total += 1\n",
    "                    index = it.multi_index\n",
    "\n",
    "                    if direction_element >= 0:\n",
    "                        result_element = global_element\n",
    "                        self.different_direction += 1\n",
    "                    else:\n",
    "                        result_element = (1 - alpha)*global_element + alpha * local_element\n",
    "                        self.same_direction += 1\n",
    "\n",
    "                    self.result[t][index] = result_element\n",
    "                # move to the next layer\n",
    "                t +=1\n",
    "                # set the current weights of the model to be the result of the merging process\n",
    "            self.set_weights(self.result)\n",
    "            \n",
    "        # -------------------------------oOo-------------------------------------------\n",
    "        def train(self, train_ds, test_ds, EPOCHS = 5,):\n",
    "            # Training for multiple epoch\n",
    "            for epoch in range(EPOCHS):\n",
    "                # Reset the metrics at the start of the next epoch\n",
    "                self.train_loss.reset_states()\n",
    "                self.train_accuracy.reset_states()\n",
    "                self.test_loss.reset_states()\n",
    "                self.test_accuracy.reset_states()\n",
    "                # self.second_weight = self.get_weights()\n",
    "\n",
    "                batch_num = 0\n",
    "                for images, labels in train_ds:\n",
    "                    batch_num +=1\n",
    "                    # record the local weights before it enter a new training batch\n",
    "                    self.previous_weights = self.get_weights()\n",
    "                    # begin the local training process of each batch\n",
    "                    self.train_step(images, labels)\n",
    "                    # check whether there is a new global model --> base on the number of files in the folder result/global/\n",
    "                    latest_global_version = self.get_latest_global_version()\n",
    "                    # if there is a new global version --> ready to merge with the local training result\n",
    "                    if self.current_global_version < latest_global_version:\n",
    "                        print(f\"Merging process happens at epoch {epoch}, batch {batch_num} when receiving the global version {latest_global_version}, current global version {self.current_global_version}\")\n",
    "                        # update the current global version on local machine\n",
    "                        self.current_global_version = latest_global_version\n",
    "                        # load the new global weights \n",
    "                        self.global_weights = self.get_latest_global_weights(self.current_global_version)\n",
    "                        # get the current local weights --> the weights obtained after executing the self.train_step() function\n",
    "                        self.current_weights = self.get_weights()\n",
    "                        # check whether the number of layers of previous and current weights match\n",
    "                        if len(self.previous_weights) < len(self.current_weights):\n",
    "                            print(f\"The number of layers of previous weights: {len(self.previous_weights)}\")\n",
    "                            print(f\"The number of layers of current weights: {len(self.current_weights)}\")\n",
    "                            print(f\"The previous weights that mismatch in the number of layers with the current weights {self.previous_weights}\")\n",
    "                        # check whether the number of layers of the current weights and the global weights match\n",
    "                        if len(self.global_weights) != len(self.current_weights):\n",
    "                            print(f\"THERE IS A MISMATCH IN NUMBER OF LAYERS OF THE GLOBAL WEIGHTS AND CURRENT WEIGHTS\")\n",
    "                            print(f\"EXIT THE MERGING PROCESS\")\n",
    "                            print(f\"The number of layers of global weights: {len(self.global_weights)}\")\n",
    "                            print(f\"The number of layers of current weights: {len(self.current_weights)}\")\n",
    "                            break\n",
    "                        else:\n",
    "                        # merged based on previous local weights, current local weights and global weights\n",
    "                            self.merge()\n",
    "                            self.merged_weights = self.get_weights()\n",
    "\n",
    "                    # else:\n",
    "                    # if there is no new global model --> update the local weights normally \n",
    "                    # --> no modification to the local weights\n",
    "                    # do nothing\n",
    "        # -------------------------------oOo-------------------------------------------\n",
    "\n",
    "                for test_images, test_labels in test_ds:\n",
    "                    self.test_step(test_images, test_labels)\n",
    "\n",
    "                print(\n",
    "                    f'Epoch {epoch + 1}, '\n",
    "                    f'Loss: {self.train_loss.result()}, '\n",
    "                    f'Accuracy: {self.train_accuracy.result() * 100}, '\n",
    "                    f'Test Loss: {self.test_loss.result()}, '\n",
    "                    f'Test Accuracy: {self.test_accuracy.result() * 100}'\n",
    "                    )\n",
    "\n",
    "                # updating local weight to appropriate directory\n",
    "                latest_global_version = self.get_latest_global_version()\n",
    "                upload_path = self.upload_weights_to_file(epoch)\n",
    "                print(f\"\\tFrom {self.worker_name} in local epoch {epoch + 1} that hold the global version {self.current_global_version}, uploading local model to the server in the path {upload_path}\")\n",
    " \n",
    "# #  create and compile model\n",
    "#     model= LowLeNet()\n",
    "#     model.compile()\n",
    "\n",
    "#     model.train(train_ds, test_ds, EPOCHS = 5)\n",
    "\n",
    "#     current_weights = model.get_weights() \n",
    "#     for layer in current_weights:\n",
    "#         print(layer.shape)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
