{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:28:04.255533: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-22 11:28:04.282457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-22 11:28:04.727440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save testing set\n",
    "test_image = test_images / 255.0\n",
    "x_test = test_images.reshape(test_images.shape[0], -1)\n",
    "y_test = test_labels.reshape(-1)\n",
    "test_data = np.column_stack((x_test, y_test))\n",
    "\n",
    "# Save test set as a pickle file\n",
    "with open('../../../data/cifar_data/test_set.pickle', 'wb') as f:\n",
    "    pickle.dump((test_data), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = train_images.reshape(train_images.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = train_labels.reshape(-1)\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# # Save training set as a pickle file\n",
    "# with open('../../../data/cifar_data/training_set.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_data), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  62,  63, ..., 123,  92,  72],\n",
       "       [154, 177, 187, ..., 143, 133, 144],\n",
       "       [255, 255, 255, ...,  80,  86,  84],\n",
       "       ...,\n",
       "       [ 35, 178, 235, ...,  12,  31,  50],\n",
       "       [189, 211, 240, ..., 195, 190, 171],\n",
       "       [229, 229, 239, ..., 163, 163, 161]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20833,  4166, 12500,  4166,  8335])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Number of chunks\n",
    "n_chunks = 5\n",
    "\n",
    "# Generate sizes following a power-law distribution\n",
    "sizes = np.random.zipf(1.5, n_chunks)\n",
    "\n",
    "# Normalize the sizes so that their sum equals the number of training samples\n",
    "sizes = (sizes / sizes.sum() * len(train_data)).astype(int)\n",
    "\n",
    "# Ensure that the sum of sizes is equal to the total number of training samples\n",
    "sizes[-1] += len(train_data) - sizes.sum()\n",
    "\n",
    "\n",
    "\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices at which to split the training data\n",
    "split_indices = np.cumsum(sizes)[:-1]\n",
    "\n",
    "# Split the training data into chunks of different sizes\n",
    "chunks = np.split(train_data, split_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_path = \"../../../data/cifar_data/5_chunks/\"\n",
    "isExist = os.path.exists(save_path)\n",
    "if not isExist:\n",
    "   os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "# Store size and label distribution of each chunk, and save each chunk as a pickle file\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = len(chunk)\n",
    "    info['label_distribution'] = get_label_distribution(chunk)\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save chunk as a pickle file\n",
    "    with open(f'{save_path}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk   size                                 label_distribution\n",
      "0      1  20833  {0: 2099, 1: 2099, 2: 2083, 3: 2076, 4: 2101, ...\n",
      "1      2   4166  {0: 422, 1: 425, 2: 413, 3: 425, 4: 395, 5: 43...\n",
      "2      3  12500  {0: 1256, 1: 1241, 2: 1234, 3: 1227, 4: 1235, ...\n",
      "3      4   4166  {0: 421, 1: 410, 2: 438, 3: 447, 4: 434, 5: 40...\n",
      "4      5   8335  {0: 802, 1: 825, 2: 832, 3: 825, 4: 835, 5: 86...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{save_path}/5_chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
