{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 05:30:52.941490: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-02 05:30:52.977084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-02 05:30:53.582349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# run locally without install asynfed package\n",
    "# root = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n",
    "sys.path.append(root)\n",
    "\n",
    "\n",
    "# tensorflow \n",
    "from asynfed.client_v2.frameworks.tensorflow.tensorflow_framework import TensorflowFramework\n",
    "from resnet18 import Resnet18\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"client_id\": \"234-gpu-1\",\n",
    "    \"queue_consumer\": {\n",
    "        'exchange_name': 'asynfl_exchange',\n",
    "        'exchange_type': 'topic',\n",
    "        'queue_name': 'server_queue',\n",
    "        'routing_key': 'client.#',\n",
    "        'end_point': 'amqps://gocktdwu:jYQBoATqKHRqXaV4O9TahpPcbd8xjcaw@armadillo.rmq.cloudamqp.com/gocktdwu'\n",
    "    },\n",
    "    \"queue_producer\": {\n",
    "        'exchange_name': 'asynfl_exchange',\n",
    "        'exchange_type': 'topic',\n",
    "        'queue_name': 'server_consumer',\n",
    "        'routing_key': 'server.#',\n",
    "        'end_point': \"amqps://gocktdwu:jYQBoATqKHRqXaV4O9TahpPcbd8xjcaw@armadillo.rmq.cloudamqp.com/gocktdwu\"\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"dataset\": \"cifar10\",\n",
    "        \"model\": \"resnet18\",\n",
    "\n",
    "        \"regularization\": \"l2\",\n",
    "        \"lambda_value\": 5e-4,\n",
    "        \"learning_rate\": 1e-3,\n",
    "\n",
    "        # setup differently for different device\n",
    "        \"gpu_index\": 0,\n",
    "        \"chunk_index\": 1,\n",
    "\n",
    "        \"qod\": 0.45,\n",
    "        \"batch_size\": 64,\n",
    "        \"epoch\": 200,\n",
    "\n",
    "        \"tracking_point\": 2000,\n",
    "        \"sleeping_time\": 10,\n",
    "        \"delta_time\": 1000000\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "config tensorflow using gpu successfully\n",
      "********************\n",
      "********************\n",
      "==> Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 05:31:05.232361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22301 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:19:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "Training for the total number of epoch 200 with batch_size 128 for datasize of 50000\n",
      "********************\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 05:31:07.471697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [50000,32,32,3]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-02 05:31:07.471938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-02 05:31:11.377341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-02 05:31:12.330801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-02 05:31:13.296510: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fbd469bc050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-02 05:31:13.296531: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-07-02 05:31:13.370403: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-07-02 05:31:29.796399: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [10000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Acc: 38.27 -- Train Loss 3.9105777740478516 Test Acc 44.35  Test Loss 1.5151580572128296\n",
      "Epoch 2 - Train Acc: 51.65 -- Train Loss 3.5630154609680176 Test Acc 53.86  Test Loss 1.2750625610351562\n",
      "Epoch 3 - Train Acc: 58.70 -- Train Loss 3.372438907623291 Test Acc 58.44  Test Loss 1.216694951057434\n",
      "Epoch 4 - Train Acc: 63.48 -- Train Loss 3.238180637359619 Test Acc 56.47  Test Loss 1.2350311279296875\n",
      "Epoch 5 - Train Acc: 66.63 -- Train Loss 3.1420679092407227 Test Acc 59.30  Test Loss 1.192697525024414\n",
      "Epoch 6 - Train Acc: 69.45 -- Train Loss 3.0574536323547363 Test Acc 65.85  Test Loss 0.9974250197410583\n",
      "Epoch 7 - Train Acc: 71.58 -- Train Loss 2.987840414047241 Test Acc 69.92  Test Loss 0.8565183281898499\n",
      "Epoch 8 - Train Acc: 73.97 -- Train Loss 2.921764373779297 Test Acc 74.09  Test Loss 0.7322002649307251\n",
      "Epoch 9 - Train Acc: 75.57 -- Train Loss 2.8658366203308105 Test Acc 75.19  Test Loss 0.7061163187026978\n",
      "Epoch 10 - Train Acc: 77.23 -- Train Loss 2.8132710456848145 Test Acc 76.21  Test Loss 0.6912876963615417\n",
      "Epoch 11 - Train Acc: 78.27 -- Train Loss 2.7695693969726562 Test Acc 74.51  Test Loss 0.7524418830871582\n",
      "Epoch 12 - Train Acc: 79.33 -- Train Loss 2.7352135181427 Test Acc 76.79  Test Loss 0.693627119064331\n",
      "Epoch 13 - Train Acc: 80.37 -- Train Loss 2.6997435092926025 Test Acc 77.27  Test Loss 0.6583876013755798\n",
      "Epoch 14 - Train Acc: 81.30 -- Train Loss 2.6663999557495117 Test Acc 78.04  Test Loss 0.6574161052703857\n",
      "Epoch 15 - Train Acc: 81.99 -- Train Loss 2.633312940597534 Test Acc 78.45  Test Loss 0.6263647675514221\n",
      "Epoch 16 - Train Acc: 82.90 -- Train Loss 2.605029821395874 Test Acc 78.68  Test Loss 0.6206381320953369\n",
      "Epoch 17 - Train Acc: 83.53 -- Train Loss 2.57820725440979 Test Acc 80.04  Test Loss 0.5793120265007019\n",
      "Epoch 18 - Train Acc: 84.07 -- Train Loss 2.5534462928771973 Test Acc 78.41  Test Loss 0.641255795955658\n",
      "Epoch 19 - Train Acc: 84.72 -- Train Loss 2.5285143852233887 Test Acc 79.11  Test Loss 0.6215615272521973\n",
      "Epoch 20 - Train Acc: 85.73 -- Train Loss 2.499753713607788 Test Acc 80.31  Test Loss 0.5788665413856506\n",
      "Epoch 21 - Train Acc: 85.94 -- Train Loss 2.4820163249969482 Test Acc 80.22  Test Loss 0.5920717716217041\n",
      "Epoch 22 - Train Acc: 86.33 -- Train Loss 2.4623401165008545 Test Acc 81.53  Test Loss 0.560452401638031\n",
      "Epoch 23 - Train Acc: 87.13 -- Train Loss 2.435725212097168 Test Acc 81.48  Test Loss 0.554568350315094\n",
      "Epoch 24 - Train Acc: 87.38 -- Train Loss 2.4172933101654053 Test Acc 81.98  Test Loss 0.5480152368545532\n",
      "Epoch 25 - Train Acc: 87.90 -- Train Loss 2.397372245788574 Test Acc 76.95  Test Loss 0.7456128597259521\n",
      "Epoch 26 - Train Acc: 88.31 -- Train Loss 2.376096725463867 Test Acc 82.14  Test Loss 0.5545644760131836\n",
      "Epoch 27 - Train Acc: 88.57 -- Train Loss 2.361429452896118 Test Acc 81.80  Test Loss 0.5425844192504883\n",
      "Epoch 28 - Train Acc: 89.02 -- Train Loss 2.338433265686035 Test Acc 81.00  Test Loss 0.5876194834709167\n",
      "Epoch 29 - Train Acc: 89.38 -- Train Loss 2.3222789764404297 Test Acc 81.67  Test Loss 0.570590078830719\n",
      "Epoch 30 - Train Acc: 89.56 -- Train Loss 2.3070902824401855 Test Acc 82.09  Test Loss 0.5738095641136169\n",
      "Epoch 31 - Train Acc: 89.97 -- Train Loss 2.295229911804199 Test Acc 83.20  Test Loss 0.5125459432601929\n",
      "Epoch 32 - Train Acc: 90.30 -- Train Loss 2.275103807449341 Test Acc 80.39  Test Loss 0.6646037697792053\n",
      "Epoch 33 - Train Acc: 90.50 -- Train Loss 2.261523723602295 Test Acc 83.63  Test Loss 0.5070800185203552\n",
      "Epoch 34 - Train Acc: 90.91 -- Train Loss 2.244166135787964 Test Acc 82.15  Test Loss 0.5767795443534851\n",
      "Epoch 35 - Train Acc: 91.12 -- Train Loss 2.2272815704345703 Test Acc 83.63  Test Loss 0.5203530788421631\n",
      "Epoch 36 - Train Acc: 91.48 -- Train Loss 2.212979555130005 Test Acc 85.22  Test Loss 0.4864543378353119\n",
      "Epoch 37 - Train Acc: 91.90 -- Train Loss 2.1946146488189697 Test Acc 82.77  Test Loss 0.5640267729759216\n",
      "Epoch 38 - Train Acc: 92.04 -- Train Loss 2.180708408355713 Test Acc 84.41  Test Loss 0.49998271465301514\n",
      "Epoch 39 - Train Acc: 92.37 -- Train Loss 2.1649820804595947 Test Acc 83.12  Test Loss 0.5589362382888794\n",
      "Epoch 40 - Train Acc: 92.28 -- Train Loss 2.1569645404815674 Test Acc 84.44  Test Loss 0.5100756883621216\n",
      "Epoch 41 - Train Acc: 93.04 -- Train Loss 2.136824607849121 Test Acc 83.32  Test Loss 0.566878616809845\n",
      "Epoch 42 - Train Acc: 93.05 -- Train Loss 2.126539707183838 Test Acc 85.60  Test Loss 0.47507983446121216\n",
      "Epoch 43 - Train Acc: 93.09 -- Train Loss 2.114015817642212 Test Acc 84.43  Test Loss 0.5254460573196411\n",
      "Epoch 44 - Train Acc: 93.60 -- Train Loss 2.0944817066192627 Test Acc 83.33  Test Loss 0.5923755764961243\n",
      "Epoch 45 - Train Acc: 93.65 -- Train Loss 2.089050531387329 Test Acc 85.27  Test Loss 0.4884388744831085\n",
      "Epoch 46 - Train Acc: 93.76 -- Train Loss 2.0785253047943115 Test Acc 85.98  Test Loss 0.47227510809898376\n",
      "Epoch 47 - Train Acc: 94.05 -- Train Loss 2.06088924407959 Test Acc 83.77  Test Loss 0.5916751623153687\n",
      "Epoch 48 - Train Acc: 94.24 -- Train Loss 2.0534281730651855 Test Acc 84.48  Test Loss 0.5819646716117859\n",
      "Epoch 49 - Train Acc: 94.57 -- Train Loss 2.037858724594116 Test Acc 84.88  Test Loss 0.5169945359230042\n",
      "Epoch 50 - Train Acc: 94.66 -- Train Loss 2.02750301361084 Test Acc 84.38  Test Loss 0.5723435282707214\n",
      "Epoch 51 - Train Acc: 94.79 -- Train Loss 2.0153872966766357 Test Acc 84.76  Test Loss 0.5507432818412781\n",
      "Epoch 52 - Train Acc: 94.91 -- Train Loss 2.0027239322662354 Test Acc 84.95  Test Loss 0.5172672867774963\n",
      "Epoch 53 - Train Acc: 95.13 -- Train Loss 1.9913042783737183 Test Acc 85.35  Test Loss 0.52720707654953\n",
      "Epoch 54 - Train Acc: 95.26 -- Train Loss 1.9813059568405151 Test Acc 86.12  Test Loss 0.494518905878067\n",
      "Epoch 55 - Train Acc: 95.44 -- Train Loss 1.9701416492462158 Test Acc 84.93  Test Loss 0.5635472536087036\n",
      "Epoch 56 - Train Acc: 95.77 -- Train Loss 1.956518292427063 Test Acc 85.40  Test Loss 0.5160719752311707\n",
      "Epoch 57 - Train Acc: 96.00 -- Train Loss 1.9454678297042847 Test Acc 85.23  Test Loss 0.5570602416992188\n",
      "Epoch 58 - Train Acc: 95.87 -- Train Loss 1.9390456676483154 Test Acc 85.18  Test Loss 0.572102427482605\n",
      "Epoch 59 - Train Acc: 96.12 -- Train Loss 1.926526427268982 Test Acc 86.10  Test Loss 0.5130459070205688\n",
      "Epoch 60 - Train Acc: 96.26 -- Train Loss 1.9175471067428589 Test Acc 85.07  Test Loss 0.6115250587463379\n",
      "Epoch 61 - Train Acc: 96.30 -- Train Loss 1.909875750541687 Test Acc 85.72  Test Loss 0.5534983277320862\n",
      "Epoch 62 - Train Acc: 96.55 -- Train Loss 1.8958646059036255 Test Acc 85.62  Test Loss 0.5741916298866272\n",
      "Epoch 63 - Train Acc: 96.68 -- Train Loss 1.8877794742584229 Test Acc 86.12  Test Loss 0.5420185327529907\n",
      "Epoch 64 - Train Acc: 96.61 -- Train Loss 1.8818339109420776 Test Acc 84.12  Test Loss 0.6611601710319519\n",
      "Epoch 65 - Train Acc: 96.71 -- Train Loss 1.8724759817123413 Test Acc 85.71  Test Loss 0.5633543133735657\n",
      "Epoch 66 - Train Acc: 97.12 -- Train Loss 1.8583166599273682 Test Acc 85.92  Test Loss 0.5562385320663452\n",
      "Epoch 67 - Train Acc: 96.87 -- Train Loss 1.8552237749099731 Test Acc 85.95  Test Loss 0.5501567125320435\n",
      "Epoch 68 - Train Acc: 97.08 -- Train Loss 1.8444005250930786 Test Acc 85.76  Test Loss 0.5881053805351257\n",
      "Epoch 69 - Train Acc: 97.10 -- Train Loss 1.8366695642471313 Test Acc 86.26  Test Loss 0.5566696524620056\n",
      "Epoch 70 - Train Acc: 97.49 -- Train Loss 1.8236719369888306 Test Acc 85.31  Test Loss 0.5959017872810364\n",
      "Epoch 71 - Train Acc: 97.40 -- Train Loss 1.8185521364212036 Test Acc 86.90  Test Loss 0.5358258485794067\n",
      "Epoch 72 - Train Acc: 97.41 -- Train Loss 1.8120554685592651 Test Acc 87.01  Test Loss 0.512944221496582\n",
      "Epoch 73 - Train Acc: 97.64 -- Train Loss 1.8012397289276123 Test Acc 86.32  Test Loss 0.5689703822135925\n",
      "Epoch 74 - Train Acc: 97.69 -- Train Loss 1.7930928468704224 Test Acc 87.27  Test Loss 0.5142006874084473\n",
      "Epoch 75 - Train Acc: 97.62 -- Train Loss 1.7880078554153442 Test Acc 85.96  Test Loss 0.5759919881820679\n",
      "Epoch 76 - Train Acc: 97.91 -- Train Loss 1.7756719589233398 Test Acc 86.84  Test Loss 0.5504635572433472\n",
      "Epoch 77 - Train Acc: 97.90 -- Train Loss 1.7704030275344849 Test Acc 87.44  Test Loss 0.5319116115570068\n",
      "Epoch 78 - Train Acc: 97.98 -- Train Loss 1.76254141330719 Test Acc 87.03  Test Loss 0.5424327254295349\n",
      "Epoch 79 - Train Acc: 98.07 -- Train Loss 1.7523435354232788 Test Acc 87.35  Test Loss 0.5492885708808899\n",
      "Epoch 80 - Train Acc: 98.30 -- Train Loss 1.7422583103179932 Test Acc 87.49  Test Loss 0.5105790495872498\n",
      "Epoch 81 - Train Acc: 98.20 -- Train Loss 1.7386536598205566 Test Acc 87.35  Test Loss 0.51374351978302\n",
      "Epoch 82 - Train Acc: 98.24 -- Train Loss 1.7303138971328735 Test Acc 86.20  Test Loss 0.5927789211273193\n",
      "Epoch 83 - Train Acc: 98.32 -- Train Loss 1.7248950004577637 Test Acc 88.11  Test Loss 0.5200542211532593\n",
      "Epoch 84 - Train Acc: 98.25 -- Train Loss 1.7188448905944824 Test Acc 87.81  Test Loss 0.5241865515708923\n",
      "Epoch 85 - Train Acc: 98.49 -- Train Loss 1.7103220224380493 Test Acc 86.93  Test Loss 0.5544620752334595\n",
      "Epoch 86 - Train Acc: 98.41 -- Train Loss 1.7044572830200195 Test Acc 86.99  Test Loss 0.5634176135063171\n",
      "Epoch 87 - Train Acc: 98.60 -- Train Loss 1.6949787139892578 Test Acc 87.58  Test Loss 0.5323860049247742\n",
      "Epoch 88 - Train Acc: 98.51 -- Train Loss 1.6910440921783447 Test Acc 86.47  Test Loss 0.6116766333580017\n",
      "Epoch 89 - Train Acc: 98.55 -- Train Loss 1.684777021408081 Test Acc 86.37  Test Loss 0.6244513988494873\n",
      "Epoch 90 - Train Acc: 98.53 -- Train Loss 1.680737853050232 Test Acc 87.79  Test Loss 0.529736340045929\n",
      "Epoch 91 - Train Acc: 98.70 -- Train Loss 1.6708587408065796 Test Acc 87.73  Test Loss 0.5421263575553894\n",
      "Epoch 92 - Train Acc: 98.70 -- Train Loss 1.6649396419525146 Test Acc 86.37  Test Loss 0.6143513917922974\n",
      "Epoch 93 - Train Acc: 98.66 -- Train Loss 1.6605608463287354 Test Acc 85.95  Test Loss 0.6516539454460144\n",
      "Epoch 94 - Train Acc: 98.65 -- Train Loss 1.6537142992019653 Test Acc 85.89  Test Loss 0.6393232941627502\n",
      "Epoch 95 - Train Acc: 98.84 -- Train Loss 1.645781397819519 Test Acc 86.98  Test Loss 0.5927928686141968\n",
      "Epoch 96 - Train Acc: 98.73 -- Train Loss 1.6430082321166992 Test Acc 85.75  Test Loss 0.6313412189483643\n",
      "Epoch 97 - Train Acc: 98.96 -- Train Loss 1.6321722269058228 Test Acc 86.80  Test Loss 0.6081057786941528\n",
      "Epoch 98 - Train Acc: 99.05 -- Train Loss 1.6251933574676514 Test Acc 86.77  Test Loss 0.606267511844635\n",
      "Epoch 99 - Train Acc: 98.98 -- Train Loss 1.62054443359375 Test Acc 87.45  Test Loss 0.5811662077903748\n",
      "Epoch 100 - Train Acc: 98.96 -- Train Loss 1.615397334098816 Test Acc 87.37  Test Loss 0.5646369457244873\n",
      "Epoch 101 - Train Acc: 98.92 -- Train Loss 1.6116302013397217 Test Acc 87.32  Test Loss 0.5602036714553833\n",
      "Epoch 102 - Train Acc: 99.08 -- Train Loss 1.6040796041488647 Test Acc 87.73  Test Loss 0.5581023693084717\n",
      "Epoch 103 - Train Acc: 99.24 -- Train Loss 1.59398353099823 Test Acc 87.80  Test Loss 0.5669510364532471\n",
      "Epoch 104 - Train Acc: 99.14 -- Train Loss 1.5913763046264648 Test Acc 86.77  Test Loss 0.6255074739456177\n",
      "Epoch 105 - Train Acc: 99.15 -- Train Loss 1.586511492729187 Test Acc 87.87  Test Loss 0.5566036701202393\n",
      "Epoch 106 - Train Acc: 99.15 -- Train Loss 1.5816576480865479 Test Acc 88.35  Test Loss 0.5618997812271118\n",
      "Epoch 107 - Train Acc: 99.16 -- Train Loss 1.5751734972000122 Test Acc 88.25  Test Loss 0.5645014047622681\n",
      "Epoch 108 - Train Acc: 99.26 -- Train Loss 1.5686631202697754 Test Acc 87.67  Test Loss 0.5869582891464233\n",
      "Epoch 109 - Train Acc: 99.21 -- Train Loss 1.5639667510986328 Test Acc 86.86  Test Loss 0.6096078157424927\n",
      "Epoch 110 - Train Acc: 99.18 -- Train Loss 1.560700535774231 Test Acc 88.29  Test Loss 0.5493274927139282\n",
      "Epoch 111 - Train Acc: 99.26 -- Train Loss 1.5530048608779907 Test Acc 87.62  Test Loss 0.5561012625694275\n",
      "Epoch 112 - Train Acc: 99.30 -- Train Loss 1.5480232238769531 Test Acc 88.21  Test Loss 0.5520509481430054\n",
      "Epoch 113 - Train Acc: 99.26 -- Train Loss 1.5436047315597534 Test Acc 87.88  Test Loss 0.5919586420059204\n",
      "Epoch 114 - Train Acc: 99.42 -- Train Loss 1.5349332094192505 Test Acc 88.25  Test Loss 0.5317710041999817\n",
      "Epoch 115 - Train Acc: 99.40 -- Train Loss 1.531593918800354 Test Acc 87.86  Test Loss 0.593723475933075\n",
      "Epoch 116 - Train Acc: 99.36 -- Train Loss 1.52683687210083 Test Acc 88.62  Test Loss 0.5302807092666626\n",
      "Epoch 117 - Train Acc: 99.35 -- Train Loss 1.5229251384735107 Test Acc 87.64  Test Loss 0.5755058526992798\n",
      "Epoch 118 - Train Acc: 99.46 -- Train Loss 1.5163826942443848 Test Acc 86.37  Test Loss 0.6517500877380371\n",
      "Epoch 119 - Train Acc: 99.40 -- Train Loss 1.5125970840454102 Test Acc 87.78  Test Loss 0.5629730224609375\n",
      "Epoch 120 - Train Acc: 99.47 -- Train Loss 1.5070593357086182 Test Acc 88.73  Test Loss 0.5159537196159363\n",
      "Epoch 121 - Train Acc: 99.51 -- Train Loss 1.500537395477295 Test Acc 88.58  Test Loss 0.5304285287857056\n",
      "Epoch 122 - Train Acc: 99.44 -- Train Loss 1.4974169731140137 Test Acc 88.55  Test Loss 0.5214633345603943\n",
      "Epoch 123 - Train Acc: 99.57 -- Train Loss 1.4902663230895996 Test Acc 88.18  Test Loss 0.5658003687858582\n",
      "Epoch 124 - Train Acc: 99.52 -- Train Loss 1.4872729778289795 Test Acc 87.34  Test Loss 0.6269763112068176\n",
      "Epoch 125 - Train Acc: 99.57 -- Train Loss 1.4812650680541992 Test Acc 88.73  Test Loss 0.5429908633232117\n",
      "Epoch 126 - Train Acc: 99.56 -- Train Loss 1.4772412776947021 Test Acc 88.23  Test Loss 0.5676001906394958\n",
      "Epoch 127 - Train Acc: 99.56 -- Train Loss 1.472859501838684 Test Acc 87.80  Test Loss 0.5835608243942261\n",
      "Epoch 128 - Train Acc: 99.59 -- Train Loss 1.468044400215149 Test Acc 88.48  Test Loss 0.5759957432746887\n",
      "Epoch 129 - Train Acc: 99.58 -- Train Loss 1.4632165431976318 Test Acc 88.13  Test Loss 0.5850650072097778\n",
      "Epoch 130 - Train Acc: 99.57 -- Train Loss 1.4590954780578613 Test Acc 88.57  Test Loss 0.5661100149154663\n",
      "Epoch 131 - Train Acc: 99.59 -- Train Loss 1.4554780721664429 Test Acc 88.53  Test Loss 0.5559492707252502\n",
      "Epoch 132 - Train Acc: 99.59 -- Train Loss 1.4507687091827393 Test Acc 87.67  Test Loss 0.6177712082862854\n",
      "Epoch 133 - Train Acc: 99.70 -- Train Loss 1.4442157745361328 Test Acc 88.32  Test Loss 0.5863927602767944\n",
      "Epoch 134 - Train Acc: 99.59 -- Train Loss 1.4423905611038208 Test Acc 88.96  Test Loss 0.5354391932487488\n",
      "Epoch 135 - Train Acc: 99.63 -- Train Loss 1.4378101825714111 Test Acc 88.94  Test Loss 0.5557140111923218\n",
      "Epoch 136 - Train Acc: 99.65 -- Train Loss 1.4327431917190552 Test Acc 88.49  Test Loss 0.5768980383872986\n",
      "Epoch 137 - Train Acc: 99.64 -- Train Loss 1.4290988445281982 Test Acc 88.37  Test Loss 0.5607694983482361\n",
      "Epoch 138 - Train Acc: 99.66 -- Train Loss 1.4244598150253296 Test Acc 88.22  Test Loss 0.5694577693939209\n",
      "Epoch 139 - Train Acc: 99.74 -- Train Loss 1.4188060760498047 Test Acc 88.88  Test Loss 0.5364189147949219\n",
      "Epoch 140 - Train Acc: 99.73 -- Train Loss 1.414994239807129 Test Acc 89.02  Test Loss 0.526059091091156\n",
      "Epoch 141 - Train Acc: 99.74 -- Train Loss 1.410519003868103 Test Acc 88.51  Test Loss 0.5905184149742126\n",
      "Epoch 142 - Train Acc: 99.75 -- Train Loss 1.4061623811721802 Test Acc 88.92  Test Loss 0.5340033769607544\n",
      "Epoch 143 - Train Acc: 99.67 -- Train Loss 1.4033781290054321 Test Acc 88.55  Test Loss 0.5674523711204529\n",
      "Epoch 144 - Train Acc: 99.73 -- Train Loss 1.3987417221069336 Test Acc 88.75  Test Loss 0.5663501620292664\n",
      "Epoch 145 - Train Acc: 99.71 -- Train Loss 1.3958094120025635 Test Acc 89.37  Test Loss 0.52571040391922\n",
      "Epoch 146 - Train Acc: 99.75 -- Train Loss 1.390921711921692 Test Acc 88.55  Test Loss 0.5816290974617004\n",
      "Epoch 147 - Train Acc: 99.75 -- Train Loss 1.3872606754302979 Test Acc 88.97  Test Loss 0.5438886284828186\n",
      "Epoch 148 - Train Acc: 99.69 -- Train Loss 1.3851196765899658 Test Acc 88.52  Test Loss 0.5524987578392029\n",
      "Epoch 149 - Train Acc: 99.70 -- Train Loss 1.3813945055007935 Test Acc 89.16  Test Loss 0.5238198041915894\n",
      "Epoch 150 - Train Acc: 99.77 -- Train Loss 1.3760147094726562 Test Acc 89.13  Test Loss 0.5341010689735413\n",
      "Epoch 151 - Train Acc: 99.77 -- Train Loss 1.372276782989502 Test Acc 89.20  Test Loss 0.5529472231864929\n",
      "Epoch 152 - Train Acc: 99.75 -- Train Loss 1.368721604347229 Test Acc 89.18  Test Loss 0.5343316197395325\n",
      "Epoch 153 - Train Acc: 99.77 -- Train Loss 1.3648425340652466 Test Acc 88.37  Test Loss 0.6054825782775879\n",
      "Epoch 154 - Train Acc: 99.79 -- Train Loss 1.3602612018585205 Test Acc 88.97  Test Loss 0.5474929213523865\n",
      "Epoch 155 - Train Acc: 99.77 -- Train Loss 1.3574546575546265 Test Acc 88.79  Test Loss 0.5487163662910461\n",
      "Epoch 156 - Train Acc: 99.79 -- Train Loss 1.3534586429595947 Test Acc 89.21  Test Loss 0.5479308366775513\n",
      "Epoch 157 - Train Acc: 99.80 -- Train Loss 1.3503066301345825 Test Acc 89.06  Test Loss 0.5293773412704468\n",
      "Epoch 158 - Train Acc: 99.80 -- Train Loss 1.3463764190673828 Test Acc 88.91  Test Loss 0.5506781935691833\n",
      "Epoch 159 - Train Acc: 99.80 -- Train Loss 1.3431105613708496 Test Acc 89.20  Test Loss 0.5337067246437073\n",
      "Epoch 160 - Train Acc: 99.86 -- Train Loss 1.3383021354675293 Test Acc 89.27  Test Loss 0.5434114336967468\n",
      "Epoch 161 - Train Acc: 99.81 -- Train Loss 1.33596932888031 Test Acc 89.39  Test Loss 0.5319182872772217\n",
      "Epoch 162 - Train Acc: 99.82 -- Train Loss 1.3324605226516724 Test Acc 89.09  Test Loss 0.549435555934906\n",
      "Epoch 163 - Train Acc: 99.87 -- Train Loss 1.3279106616973877 Test Acc 88.59  Test Loss 0.5750546455383301\n",
      "Epoch 164 - Train Acc: 99.88 -- Train Loss 1.324080467224121 Test Acc 89.40  Test Loss 0.5328220725059509\n",
      "Epoch 165 - Train Acc: 99.89 -- Train Loss 1.3205348253250122 Test Acc 89.20  Test Loss 0.5222485661506653\n",
      "Epoch 166 - Train Acc: 99.90 -- Train Loss 1.3177125453948975 Test Acc 89.35  Test Loss 0.531276524066925\n",
      "Epoch 167 - Train Acc: 99.91 -- Train Loss 1.3134315013885498 Test Acc 89.02  Test Loss 0.5509322285652161\n",
      "Epoch 168 - Train Acc: 99.92 -- Train Loss 1.3103785514831543 Test Acc 88.99  Test Loss 0.5523174405097961\n",
      "Epoch 169 - Train Acc: 99.91 -- Train Loss 1.30735445022583 Test Acc 89.65  Test Loss 0.5178249478340149\n",
      "Epoch 170 - Train Acc: 99.92 -- Train Loss 1.304082989692688 Test Acc 89.26  Test Loss 0.5319223403930664\n",
      "Epoch 171 - Train Acc: 99.92 -- Train Loss 1.3008134365081787 Test Acc 89.57  Test Loss 0.5213671326637268\n",
      "Epoch 172 - Train Acc: 99.90 -- Train Loss 1.2980844974517822 Test Acc 89.08  Test Loss 0.5485690832138062\n",
      "Epoch 173 - Train Acc: 99.90 -- Train Loss 1.2952760457992554 Test Acc 89.68  Test Loss 0.5116685628890991\n",
      "Epoch 174 - Train Acc: 99.91 -- Train Loss 1.291540265083313 Test Acc 89.08  Test Loss 0.5491703152656555\n",
      "Epoch 175 - Train Acc: 99.90 -- Train Loss 1.2893950939178467 Test Acc 89.43  Test Loss 0.5284544229507446\n",
      "Epoch 176 - Train Acc: 99.93 -- Train Loss 1.285552978515625 Test Acc 89.28  Test Loss 0.5253234505653381\n",
      "Epoch 177 - Train Acc: 99.88 -- Train Loss 1.283423900604248 Test Acc 89.27  Test Loss 0.5272207856178284\n",
      "Epoch 178 - Train Acc: 99.89 -- Train Loss 1.2805522680282593 Test Acc 89.23  Test Loss 0.5376326441764832\n",
      "Epoch 179 - Train Acc: 99.93 -- Train Loss 1.2769415378570557 Test Acc 89.36  Test Loss 0.5420711636543274\n",
      "Epoch 180 - Train Acc: 99.91 -- Train Loss 1.2738597393035889 Test Acc 89.71  Test Loss 0.5088374614715576\n",
      "Epoch 181 - Train Acc: 99.92 -- Train Loss 1.2708064317703247 Test Acc 89.50  Test Loss 0.5352831482887268\n",
      "Epoch 182 - Train Acc: 99.94 -- Train Loss 1.2676342725753784 Test Acc 89.18  Test Loss 0.5544875264167786\n",
      "Epoch 183 - Train Acc: 99.91 -- Train Loss 1.2653111219406128 Test Acc 89.37  Test Loss 0.535873293876648\n",
      "Epoch 184 - Train Acc: 99.91 -- Train Loss 1.2627983093261719 Test Acc 89.56  Test Loss 0.5214693546295166\n",
      "Epoch 185 - Train Acc: 99.93 -- Train Loss 1.2598817348480225 Test Acc 89.23  Test Loss 0.5328375697135925\n",
      "Epoch 186 - Train Acc: 99.93 -- Train Loss 1.2571240663528442 Test Acc 89.41  Test Loss 0.5231285095214844\n",
      "Epoch 187 - Train Acc: 99.95 -- Train Loss 1.2539973258972168 Test Acc 89.62  Test Loss 0.5148105025291443\n",
      "Epoch 188 - Train Acc: 99.93 -- Train Loss 1.2515552043914795 Test Acc 89.24  Test Loss 0.5473410487174988\n",
      "Epoch 189 - Train Acc: 99.91 -- Train Loss 1.2490400075912476 Test Acc 89.99  Test Loss 0.5076013803482056\n",
      "Epoch 190 - Train Acc: 99.93 -- Train Loss 1.2464070320129395 Test Acc 89.77  Test Loss 0.5040668249130249\n",
      "Epoch 191 - Train Acc: 99.94 -- Train Loss 1.2434747219085693 Test Acc 89.88  Test Loss 0.5101787447929382\n",
      "Epoch 192 - Train Acc: 99.95 -- Train Loss 1.2406823635101318 Test Acc 89.41  Test Loss 0.5224546790122986\n",
      "Epoch 193 - Train Acc: 99.95 -- Train Loss 1.238263487815857 Test Acc 89.42  Test Loss 0.5234485864639282\n",
      "Epoch 194 - Train Acc: 99.95 -- Train Loss 1.235471487045288 Test Acc 89.15  Test Loss 0.5459974408149719\n",
      "Epoch 195 - Train Acc: 99.97 -- Train Loss 1.2326749563217163 Test Acc 89.44  Test Loss 0.5161551237106323\n",
      "Epoch 196 - Train Acc: 99.96 -- Train Loss 1.2304096221923828 Test Acc 89.75  Test Loss 0.5043054223060608\n",
      "Epoch 197 - Train Acc: 99.95 -- Train Loss 1.2280387878417969 Test Acc 89.43  Test Loss 0.5187590718269348\n",
      "Epoch 198 - Train Acc: 99.95 -- Train Loss 1.2256665229797363 Test Acc 89.89  Test Loss 0.501779317855835\n",
      "Epoch 199 - Train Acc: 99.97 -- Train Loss 1.2227963209152222 Test Acc 89.79  Test Loss 0.5010809302330017\n",
      "Epoch 200 - Train Acc: 99.97 -- Train Loss 1.2205075025558472 Test Acc 89.66  Test Loss 0.5137932896614075\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "gpu_index = 0\n",
    "\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[gpu_index], 'GPU')\n",
    "    print(\"config tensorflow using gpu successfully\")\n",
    "else:\n",
    "    print(\"There is no gpu or your tensorflow is not built in with gpu support\")\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "\n",
    "\n",
    "epoch = 200\n",
    "batch_size = 128\n",
    "patience = 2000\n",
    "\n",
    "\n",
    "print('==> Preparing data...')\n",
    "train_images, train_labels, test_images, test_labels = get_dataset()\n",
    "data_size = len(train_images)\n",
    "\n",
    "mean, std = get_mean_and_std(train_images)\n",
    "train_images = normalize(train_images, mean, std)\n",
    "test_images = normalize(test_images, mean, std)\n",
    "\n",
    "train_ds = dataset_generator(train_images, train_labels, batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).\\\n",
    "        batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# # define model\n",
    "# model = Resnet18(input_features = (32, 32, 3), output_features = 10, lr=1e-1, decay_steps=int(epoch * data_size / batch_size))\n",
    "# # define framework\n",
    "# tensorflow_framework = TensorflowFramework(model = model, epoch= 200, data_size= data_size, train_ds= train_ds, test_ds= test_ds, regularization='l2', delta_time= 10000, qod= 0.45)\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = Resnet18(input_features= (32, 32, 3), \n",
    "                 output_features= 10,\n",
    "                 lr=config['training_params']['learning_rate'],\n",
    "                 decay_steps=int(config['training_params']['epoch'] * data_size / config['training_params']['batch_size']))\n",
    "                #  decay_steps=int(Config.EPOCH * data_size / Config.BATCH_SIZE))\n",
    "# Define framework\n",
    "tensorflow_framework = TensorflowFramework(model=model, \n",
    "                                           data_size= data_size, \n",
    "                                           train_ds= train_ds, \n",
    "                                           test_ds= test_ds, \n",
    "                                           config=config)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables for early stopping check\n",
    "best_val_loss = float(\"inf\")\n",
    "# Number of epochs to wait before stopping training when performance worsens\n",
    "# already set patience above\n",
    "waiting = 0\n",
    "# training with 200 epoch or early stopping\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "print(f\"Training for the total number of epoch {epoch} with batch_size {batch_size} for datasize of {data_size}\")\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "for epoch in range(epoch):\n",
    "    tensorflow_framework.model.train_loss.reset_states()\n",
    "    tensorflow_framework.model.train_performance.reset_states()\n",
    "    tensorflow_framework.model.test_loss.reset_states()\n",
    "    tensorflow_framework.model.test_performance.reset_states()\n",
    "\n",
    "    for images, labels in tensorflow_framework.train_ds:\n",
    "        train_acc, train_loss= tensorflow_framework.fit(images, labels)\n",
    "\n",
    "    for test_images, test_labels in tensorflow_framework.test_ds:\n",
    "        test_acc, test_loss = tensorflow_framework.evaluate(test_images, test_labels)\n",
    "\n",
    "    print(\"Epoch {} - Train Acc: {:.2f} -- Train Loss {} Test Acc {:.2f}  Test Loss {}\".format(epoch+1,\n",
    "                                                                                       train_acc * 100,\n",
    "                                                                                       train_loss,\n",
    "                                                                                       test_acc * 100,\n",
    "                                                                                       test_loss))\n",
    "    \n",
    "    # After each epoch, check the validation loss\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        waiting = 0\n",
    "    else:\n",
    "        waiting += 1\n",
    "\n",
    "    if waiting >= patience:\n",
    "        print(\"Early stopping triggered - ending training.\")\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "save_location = \"weights1.pkl\"\n",
    "weights = model.get_weights()\n",
    "with open(save_location, 'wb') as f:\n",
    "    import pickle\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
