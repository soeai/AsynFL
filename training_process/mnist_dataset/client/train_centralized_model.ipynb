{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 11:52:19.797773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-25 11:52:21.241202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TensorflowImageDataPreprocessing():\n",
    "    def __init__(self, train_images_path: str, train_labels_path: str, height: int, width: int, batch_size = 32, shuffle_time = 10000, split = True, fract = 0.1, evaluate_images_path = None, evaluate_labels_path = None):\n",
    "        self.train_ds = None\n",
    "        self.test_ds = None\n",
    "        self.evaluate_ds = None\n",
    "        self.height = height\n",
    "        self. width = width\n",
    "\n",
    "        self.load_training_dataset(train_images_path, train_labels_path, batch_size, shuffle_time, split, fract = fract)\n",
    "        if evaluate_images_path != None and evaluate_labels_path != None:\n",
    "            self.load_evaluate_dataset(evaluate_images_path, evaluate_labels_path, batch_size)\n",
    "\n",
    "    def load_training_dataset(self, train_images_path: str, train_labels_path: str, batch_size: int, shuffle_time: int, split: bool, fract):\n",
    "        # Load the MNIST digit dataset files into numpy arrays\n",
    "        with gzip.open(train_images_path, 'rb') as f:\n",
    "            x = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, self.height, self.width)\n",
    "\n",
    "        with gzip.open(train_labels_path, 'rb') as f:\n",
    "            y = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        x = x / 255    \n",
    "        # shape of x and y now: ((size, 28, 28), (size,))\n",
    "        # Add a channels dimension\n",
    "        x = x[..., tf.newaxis].astype(\"float32\")\n",
    "        # shape of x and y now: ((size, 28, 28, 1), (size,))\n",
    "\n",
    "        # make it to be a smaller dataset\n",
    "        # Take a portion of the whole dataset as a way to simulate various data owner behavior\n",
    "        start = 0\n",
    "        datasize = 500\n",
    "        x = x[start: start + datasize]\n",
    "        y = y[start: start + datasize]\n",
    "        \n",
    "        if split:\n",
    "            # split training dataset into train set and test set (user may not choose this option if its training dataset is too small)\n",
    "            # Divide into training and testing set\n",
    "            datasize = len(x)\n",
    "            break_point = int(datasize * (1 - fract)) \n",
    "            x_train = x[: break_point]\n",
    "            x_test = x[break_point: ]\n",
    "            y_train = y[: break_point]\n",
    "            y_test = y[break_point: ]\n",
    "\n",
    "            # create ready datasets for training process\n",
    "            # train_ds and test_ds type: tensorflow.python.data.ops.batch_op._BatchDataset\n",
    "            self.train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuffle_time).batch(batch_size)\n",
    "            self.test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "        else:\n",
    "            # create a ready dataset for training process\n",
    "            # train_ds type: tensorflow.python.data.ops.batch_op._BatchDataset\n",
    "            self.train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuffle_time).batch(batch_size)\n",
    "\n",
    "\n",
    "    def load_evaluate_dataset(self, evaluate_images_path: str, evaluate_labels_path: str, batch_size: int):\n",
    "        with gzip.open(evaluate_images_path, 'rb') as f:\n",
    "            x = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, self.height, self.width)\n",
    "        with gzip.open(evaluate_labels_path, 'rb') as f:\n",
    "            y = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "        x = x / 255    \n",
    "        # shape of x and y now: ((size, 28, 28), (size,))\n",
    "        # Add a channels dimension\n",
    "        x = x[..., tf.newaxis].astype(\"float32\")\n",
    "        # shape of x and y now: ((size, 28, 28, 1), (size,))\n",
    "        self.evaluate_ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "\n",
    "\n",
    "class ModelWrapper(ABC):\n",
    "    # model, data_size, train_ds is require\n",
    "    # test_ds is optional\n",
    "\n",
    "    def __int__(self, model, data_size, qod, train_ds, test_ds):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    # input: a list of several numpy arrays (corresponding to several layers)\n",
    "    # expected result: set it to be the weights of the model\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "    # output: return weights as a list of numpy array\n",
    "    @abstractmethod\n",
    "    def get_weights(self):\n",
    "        pass\n",
    "\n",
    "    # output: performance and loss\n",
    "    @abstractmethod\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "\n",
    "    # output: precision and loss\n",
    "    @abstractmethod\n",
    "    def evaluate(self, x, y):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "'''\n",
    "    - This is a reference of how user can defined a tensorflow model\n",
    "        that can be used in our platform\n",
    "    - users can use our abstract class to define their own model\n",
    "        or they can design one by themselves\n",
    "        as long as it is inherited from class ModelWarapper\n",
    "        and satify all the requirements (at least provide data_size, train_ds, and \n",
    "        implement of all abstract functions)\n",
    "    - more frameworks can be found at asynfed/client/frameworks directory\n",
    "'''\n",
    "\n",
    "class TensorflowFramework(ModelWrapper):\n",
    "    # Tensorflow Model must be an inheritant of class tensorflow.keras.Model\n",
    "    # model, data_size, train_ds is required\n",
    "    # test_ds is optional\n",
    "    def __init__(self, model: Model, data_size: int = 10, qod: float = 0.9, train_ds = None, test_ds = None):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        - model must have an optimizer, a loss object, and trainining metric \n",
    "            model.optimizer\n",
    "            model.loss_object\n",
    "            model.train_performance\n",
    "            model.train_loss\n",
    "        - if there is a test_ds, must define testing metrics \n",
    "            similar as the way we define training metrics\n",
    "        - model must have function to get train_performanced and train_loss as numerical data\n",
    "            model.get_train_performanced()\n",
    "            model.get_train_loss()\n",
    "        - if there is a test_ds, must define similar functions \n",
    "            to get test_performanced and train_performanced as numerical data\n",
    "        - detail instruction on how to create a sequential model \n",
    "            for tensorflow framework can be found at tensorflow_sequential_model.py\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.data_size = data_size\n",
    "        self.qod = qod\n",
    "        self.train_ds = train_ds\n",
    "        self.test_ds = test_ds\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        return self.model.set_weights(weights)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.train_step(x, y)\n",
    "        return self.model.get_train_performance(), self.model.get_train_loss()\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.test_step(x, y)\n",
    "        return self.model.get_test_performance(), self.model.get_test_loss()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions = self.model(images, training=True)\n",
    "            loss = self.model.loss_object(labels, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.model.train_loss(loss)\n",
    "        self.model.train_performance(labels, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = self.model(images, training=False)\n",
    "        t_loss = self.model.loss_object(labels, predictions)\n",
    "        self.model.test_loss(t_loss)\n",
    "        self.model.test_performance(labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "'''\n",
    "- This abstract class is intended to help user on\n",
    "    how to create their own tensorflow sequential model\n",
    "    that can run on our flatform\n",
    "    you can also create your tensorflow model \n",
    "    in you own way\n",
    "\n",
    "- sample of how to create a specific tensorflow sequential model \n",
    "    can be found at project_dir/training_process/client/Lenet.py\n",
    "'''\n",
    "class TensorflowSequentialModel(Model):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.create_model(input_features, output_features)\n",
    "        # loss\n",
    "        self.loss_object = self.create_loss_object()\n",
    "        # optimizer\n",
    "        self.optimizer = self.create_optimizer()\n",
    "        # metric\n",
    "        self.train_performance, self.train_loss = self.create_train_metric()\n",
    "        self.test_performance, self.test_loss = self.create_test_metric()\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self, input_features, output_features):\n",
    "        '''\n",
    "        - a sequential tensorflow model consists of multiple layers \n",
    "            each layer is an instance of class tensorflow.keras.layers.Layer\n",
    "        - it can be the already defined layer as Dense, Flatten, Conv2D\n",
    "            or a custom layered defined by user\n",
    "        - input_features variable is the input for the first layer\n",
    "        - output_features variable is the output of the last layer\n",
    "        - a non return function\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def call(self, x):\n",
    "        '''\n",
    "        - must return x\n",
    "        - define the order of layers in which we pass the input feature (x) \n",
    "            from the first layer toward the last one\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def create_loss_object(self):\n",
    "        '''\n",
    "        - must return a loss object\n",
    "        - several loss objects can be found in tf.keras.losses \n",
    "            or can be a customized one\n",
    "        - below is how to use Categorical Crossentropy loss object defined by tensorflow\n",
    "            self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        '''\n",
    "        pass \n",
    "    \n",
    "    @abstractmethod\n",
    "    def create_optimizer(self):\n",
    "        '''\n",
    "        - must return an optimizer object\n",
    "        - optimizers in tf.keras.optimizers\n",
    "            or define a personalized one\n",
    "        - below is how to use Adam optimizer defined by tensorflow\n",
    "            self.optimizer = tf.keras.optimizers.Adam()\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_train_metric(self):\n",
    "        '''\n",
    "        - must return a train_performance object \n",
    "            and a train_loss object correspondingly \n",
    "        - metric in tf.keras.metrics\n",
    "            or can be self defined\n",
    "        - below is how to use Mean loss and Categorical Accuracy object provided by tensorflow\n",
    "            self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "            self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_test_metric(self):\n",
    "        # if there is a test dataset\n",
    "        # must return a test_performance object and a test_loss object correspondingly \n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_performance(self):\n",
    "        # return a float number\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_loss(self):\n",
    "        # return a float number\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_test_performance(self):\n",
    "        # return a float number\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_test_loss(self):\n",
    "        # return a float number\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "\n",
    "\n",
    "class LeNet(TensorflowSequentialModel):\n",
    "    def __init__(self, input_features= (32, 32, 1), output_features =10):\n",
    "        super().__init__(input_features= input_features, output_features= output_features)\n",
    "\n",
    "    def create_model(self, input_features, output_features):\n",
    "        self.conv1 = Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape= input_features,\n",
    "                            padding=\"valid\")\n",
    "        self.avgpool1 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.conv2 = Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid')\n",
    "        self.avgpool2 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(120, activation='tanh')\n",
    "        self.dense2 = Dense(84, activation='tanh')\n",
    "        self.dense3 = Dense(output_features, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def create_loss_object(self):\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        return loss_object\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        return optimizer\n",
    "\n",
    "    def create_train_metric(self):\n",
    "        train_performance = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        return train_performance, train_loss\n",
    "    \n",
    "    def create_test_metric(self):\n",
    "        test_performance = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "        test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "        return test_performance, test_loss\n",
    "    \n",
    "    def get_train_performance(self):\n",
    "        return float(self.train_performance.result())\n",
    "\n",
    "    def get_train_loss(self):\n",
    "        return float(self.train_loss.result())\n",
    "    \n",
    "    def get_test_performance(self):\n",
    "        return float(self.train_performance.result())\n",
    "\n",
    "    def get_test_loss(self):\n",
    "        return float(self.test_loss.result())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 11:52:24.639002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:24.673261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:24.673393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:24.677571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:24.677705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:24.677771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:26.531816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:26.532043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:26.532087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-06-25 11:52:26.532424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-25 11:52:26.532541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4551 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# ------------oOo--------------------\n",
    "# Preprocessing data\n",
    "# mnist dataset\n",
    "# Set the file paths for the MNIST digit dataset files\n",
    "train_images_path = os.path.join(root, os.getenv(\"mnist_x_train_path\"))\n",
    "train_labels_path = os.path.join(root, os.getenv(\"mnist_y_train_path\"))\n",
    "test_images_path = os.path.join(root, os.getenv(\"mnist_x_test_path\"))\n",
    "test_labels_path = os.path.join(root, os.getenv(\"mnist_y_test_path\"))\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# preprocessing data to be ready for low level tensorflow training process\n",
    "data_preprocessing = TensorflowImageDataPreprocessing(train_images_path=train_images_path, train_labels_path=train_labels_path, \n",
    "                                                      height = 28, width = 28, batch_size= batch_size, split=True, fract=0.2,\n",
    "                                                      evaluate_images_path=test_images_path, evaluate_labels_path=test_labels_path)\n",
    "# define dataset\n",
    "train_ds = data_preprocessing.train_ds\n",
    "test_ds = data_preprocessing.test_ds\n",
    "evaluate_ds = data_preprocessing.evaluate_ds\n",
    "# ------------oOo--------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define model\n",
    "lenet = LeNet(input_features = (32, 32, 1), output_features = 10)\n",
    "# define framework\n",
    "model = TensorflowFramework(model = lenet, data_size= 10000, qod= 0.5, train_ds= train_ds, test_ds= test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 11:52:27.277714: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [400,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-25 11:52:27.278205: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [400]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "/home/vtn_ubuntu/miniconda3/envs/asynfed/lib/python3.9/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-06-25 11:52:29.093678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-25 11:52:30.238070: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fa73c583ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-25 11:52:30.238137: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1060, Compute Capability 6.1\n",
      "2023-06-25 11:52:30.244207: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-25 11:52:30.407241: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-06-25 11:52:32.881384: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [100]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Accuracy: 20.749999582767487, Train Loss: 2.2331576347351074, Test Accuracy: 20.749999582767487Test Loss: 2.0697736740112305, \n",
      "Epoch 2, Train Accuracy: 39.625000953674316, Train Loss: 2.089745044708252, Test Accuracy: 39.625000953674316Test Loss: 1.9451007843017578, \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for images, labels in train_ds:\n",
    "    train_acc, train_loss = model.fit(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_acc, test_loss = model.evaluate(test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    # f'Loss: {model.train_loss.result()}, '\n",
    "    f'Train Accuracy: {train_acc * 100}, '\n",
    "    f'Train Loss: {train_loss}, '\n",
    "    f'Test Accuracy: {test_acc * 100}'\n",
    "    f'Test Loss: {test_loss}, '\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../server/testweight_v1.pkl\", \"wb\") as f:\n",
    "    import pickle \n",
    "    pickle.dump(model.get_weights(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
