{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 03:58:54.976395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 03:58:55.575679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04736183 0.05161072 0.05689929 0.05039335 0.04208938 0.05147302\n",
      " 0.05004039 0.04254451 0.05239361 0.05142695 0.05020426 0.03729472\n",
      " 0.05356149 0.05485727 0.0554516  0.04848357 0.04788524 0.04906771\n",
      " 0.05829911 0.048662  ]\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate an array of 20 random numbers around the mean using a normal distribution\n",
    "mean = 0.05\n",
    "std_dev = 0.005  # Small standard deviation to keep numbers close to the mean\n",
    "array = np.random.normal(mean, std_dev, 20)\n",
    "\n",
    "# Clip values to range [0,1]\n",
    "array = np.clip(array, 0, 1)\n",
    "\n",
    "# Normalize the array so the sum of all elements equals 1\n",
    "array /= np.sum(array)\n",
    "\n",
    "print(array)\n",
    "print(np.sum(array))  # This should print '1.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2368, 2581, 2845, 2520, 2104, 2574, 2502, 2127, 2620, 2571, 2510,\n",
       "        1865, 2678, 2743, 2773, 2424, 2394, 2453, 2915, 2433]),\n",
       " 50000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_samples = len(train_data)\n",
    "\n",
    "# Distribute the total number of samples proportionally and round to the nearest integer\n",
    "sizes = np.round(array * total_samples).astype(int)\n",
    "\n",
    "# Adjust the last element to ensure the sum equals to total_samples\n",
    "sizes[-1] = total_samples - np.sum(sizes[:-1])\n",
    "\n",
    "sizes, np.sum(sizes)  # This should print '50000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices at which to split the training data\n",
    "split_indices = np.cumsum(sizes)[:-1]\n",
    "\n",
    "# Split the training data into chunks of different sizes\n",
    "chunks = np.split(train_data, split_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"20_chunks_iid\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_folder = \"iid\"\n",
    "iid_folder = os.path.join(folder, iid_folder)\n",
    "if not os.path.exists(iid_folder):\n",
    "    os.makedirs(iid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "\n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{iid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chunk  size                                 label_distribution  \\\n",
      "0       1  2368  {0: 237, 1: 219, 2: 243, 3: 260, 4: 245, 5: 23...   \n",
      "1       2  2581  {0: 263, 1: 252, 2: 268, 3: 242, 4: 241, 5: 26...   \n",
      "2       3  2845  {0: 288, 1: 293, 2: 284, 3: 298, 4: 281, 5: 29...   \n",
      "3       4  2520  {0: 243, 1: 302, 2: 251, 3: 252, 4: 252, 5: 22...   \n",
      "4       5  2104  {0: 238, 1: 201, 2: 225, 3: 214, 4: 209, 5: 20...   \n",
      "5       6  2574  {0: 246, 1: 291, 2: 247, 3: 263, 4: 275, 5: 25...   \n",
      "6       7  2502  {0: 237, 1: 244, 2: 250, 3: 267, 4: 246, 5: 24...   \n",
      "7       8  2127  {0: 218, 1: 198, 2: 182, 3: 208, 4: 218, 5: 23...   \n",
      "8       9  2620  {0: 279, 1: 280, 2: 240, 3: 291, 4: 257, 5: 24...   \n",
      "9      10  2571  {0: 272, 1: 243, 2: 269, 3: 277, 4: 234, 5: 23...   \n",
      "10     11  2510  {0: 258, 1: 216, 2: 279, 3: 236, 4: 264, 5: 23...   \n",
      "11     12  1865  {0: 176, 1: 171, 2: 178, 3: 188, 4: 200, 5: 19...   \n",
      "12     13  2678  {0: 277, 1: 284, 2: 301, 3: 233, 4: 271, 5: 25...   \n",
      "13     14  2743  {0: 255, 1: 247, 2: 272, 3: 265, 4: 267, 5: 30...   \n",
      "14     15  2773  {0: 233, 1: 285, 2: 265, 3: 283, 4: 287, 5: 30...   \n",
      "15     16  2424  {0: 236, 1: 265, 2: 243, 3: 226, 4: 243, 5: 24...   \n",
      "16     17  2394  {0: 253, 1: 241, 2: 221, 3: 238, 4: 232, 5: 22...   \n",
      "17     18  2453  {0: 250, 1: 230, 2: 235, 3: 246, 4: 237, 5: 24...   \n",
      "18     19  2915  {0: 292, 1: 275, 2: 301, 3: 277, 4: 301, 5: 28...   \n",
      "19     20  2433  {0: 249, 1: 263, 2: 246, 3: 236, 4: 240, 5: 25...   \n",
      "\n",
      "                                    label_proportions  \n",
      "0   {0: 0.10008445945945946, 1: 0.0924831081081081...  \n",
      "1   {0: 0.1018984889577683, 1: 0.0976365749709415,...  \n",
      "2   {0: 0.10123022847100176, 1: 0.1029876977152899...  \n",
      "3   {0: 0.09642857142857143, 1: 0.1198412698412698...  \n",
      "4   {0: 0.11311787072243346, 1: 0.0955323193916349...  \n",
      "5   {0: 0.09557109557109557, 1: 0.1130536130536130...  \n",
      "6   {0: 0.09472422062350119, 1: 0.0975219824140687...  \n",
      "7   {0: 0.10249177244945933, 1: 0.0930888575458392...  \n",
      "8   {0: 0.1064885496183206, 1: 0.10687022900763359...  \n",
      "9   {0: 0.10579541034616881, 1: 0.0945157526254375...  \n",
      "10  {0: 0.10278884462151394, 1: 0.0860557768924302...  \n",
      "11  {0: 0.09436997319034852, 1: 0.0916890080428954...  \n",
      "12  {0: 0.10343539955190441, 1: 0.1060492905153099...  \n",
      "13  {0: 0.0929639081297849, 1: 0.09004739336492891...  \n",
      "14  {0: 0.08402452217814642, 1: 0.1027767760548142...  \n",
      "15  {0: 0.09735973597359736, 1: 0.1093234323432343...  \n",
      "16  {0: 0.10568086883876357, 1: 0.1006683375104427...  \n",
      "17  {0: 0.10191602119853241, 1: 0.0937627395026498...  \n",
      "18  {0: 0.10017152658662093, 1: 0.0943396226415094...  \n",
      "19  {0: 0.10234278668310727, 1: 0.1080969995889847...  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{iid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distribution_unbalanced(chunk, seed=None):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "\n",
    "    # Get unique labels in the chunk\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Assign random weights to each unique label\n",
    "    label_weights = np.random.rand(len(unique_labels))\n",
    "\n",
    "    # Normalize weights so they sum to 1\n",
    "    label_weights /= label_weights.sum()\n",
    "\n",
    "    # Create a new chunk with an unbalanced label distribution\n",
    "    unbalanced_chunk = np.empty((0, chunk.shape[1]), dtype=chunk.dtype)\n",
    "\n",
    "    for label, weight in zip(unique_labels, label_weights):\n",
    "        # Calculate the number of samples for this label\n",
    "        num_samples = int(weight * len(chunk))\n",
    "\n",
    "        # Get all samples with this label\n",
    "        label_samples = chunk[labels == label]\n",
    "\n",
    "        # Randomly select samples of this label\n",
    "        selected_samples = np.random.choice(len(label_samples), size=num_samples)\n",
    "\n",
    "        # Add selected samples to the new chunk\n",
    "        unbalanced_chunk = np.vstack([unbalanced_chunk, label_samples[selected_samples]])\n",
    "\n",
    "    return unbalanced_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "noniid_folder = \"noniid\"\n",
    "noniid_folder = os.path.join(folder, noniid_folder)\n",
    "if not os.path.exists(noniid_folder):\n",
    "    os.makedirs(noniid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store size and label distribution of each chunk, and save each chunk as a pickle file\n",
    "unbalanced_chunk_info = []\n",
    "# Now, make the distribution unbalanced for each chunk before saving\n",
    "for i, chunk in enumerate(chunks):\n",
    "    unbalanced_chunk = make_distribution_unbalanced(chunk, seed=i)\n",
    "\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    label_distribution = get_label_distribution(unbalanced_chunk)\n",
    "    chunk_size =len(unbalanced_chunk)\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    unbalanced_chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{noniid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(unbalanced_chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk   size                                 label_distribution  \\\n",
      "0      1   4996  {0: 445, 1: 580, 2: 489, 3: 442, 4: 344, 5: 52...   \n",
      "1      2   7495  {0: 994, 1: 1717, 3: 720, 4: 349, 5: 220, 6: 4...   \n",
      "2      3   9995  {0: 1215, 1: 72, 2: 1531, 3: 1213, 4: 1171, 5:...   \n",
      "3      4  12494  {0: 1472, 1: 1893, 2: 777, 3: 1365, 4: 2387, 5...   \n",
      "4      5  14996  {0: 2507, 1: 1418, 2: 2521, 3: 1853, 4: 1808, ...   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0: 0.08907125700560449, 1: 0.1160928742994395...  \n",
      "1  {0: 0.13262174783188793, 1: 0.2290860573715810...  \n",
      "2  {0: 0.12156078039019509, 1: 0.0072036018009004...  \n",
      "3  {0: 0.11781655194493357, 1: 0.1515127261085321...  \n",
      "4  {0: 0.16717791411042945, 1: 0.0945585489463857...  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "noniid_df = pd.DataFrame(unbalanced_chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(noniid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "noniid_df.to_csv(f\"{noniid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>size</th>\n",
       "      <th>label_distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8695</td>\n",
       "      <td>{0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4347</td>\n",
       "      <td>{0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>13043</td>\n",
       "      <td>{0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10869</td>\n",
       "      <td>{0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2181</td>\n",
       "      <td>{0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk   size                                 label_distribution\n",
       "0      1   2173  {0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...\n",
       "1      2   2173  {0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...\n",
       "2      3   8695  {0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...\n",
       "3      4   2173  {0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...\n",
       "4      5   4347  {0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...\n",
       "5      6   2173  {0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...\n",
       "6      7  13043  {0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...\n",
       "7      8   2173  {0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...\n",
       "8      9  10869  {0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...\n",
       "9     10   2181  {0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...\n",
       "1    {0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...\n",
       "2    {0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...\n",
       "3    {0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...\n",
       "4    {0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...\n",
       "5    {0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...\n",
       "6    {0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...\n",
       "7    {0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...\n",
       "8    {0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...\n",
       "9    {0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24...\n",
       "Name: label_distribution, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data2/chunk_1.pkl\n",
      "Dataset saved to data2/chunk_2.pkl\n",
      "Dataset saved to data2/chunk_3.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "train_size=4463 should be either positive and smaller than the number of samples 3765 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m diff \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     label, images \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(data_dict\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 61\u001b[0m     extra_samples, data_dict[label] \u001b[39m=\u001b[39m train_test_split(images, train_size\u001b[39m=\u001b[39;49mdiff, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m     chunk_images\u001b[39m.\u001b[39mextend(extra_samples)\n\u001b[1;32m     63\u001b[0m     chunk_labels\u001b[39m.\u001b[39mextend([label]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(extra_samples))\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2230\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtest_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2220\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2221\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_size, n_samples)\n\u001b[1;32m   2222\u001b[0m     )\n\u001b[1;32m   2224\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2225\u001b[0m     train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2226\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n_samples \u001b[39mor\u001b[39;00m train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2227\u001b[0m     \u001b[39mor\u001b[39;00m train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2228\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m   2229\u001b[0m ):\n\u001b[0;32m-> 2230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2232\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2233\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size, n_samples)\n\u001b[1;32m   2234\u001b[0m     )\n\u001b[1;32m   2236\u001b[0m \u001b[39mif\u001b[39;00m train_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m train_size_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2237\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid value for train_size: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size))\n",
      "\u001b[0;31mValueError\u001b[0m: train_size=4463 should be either positive and smaller than the number of samples 3765 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the proportion of each chunk\n",
    "chunk_sizes = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Define the proportion of each category within each chunk\n",
    "category_dist_in_chunks = [\n",
    "    [0.3, 0.2, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05, 0.025, 0.025],\n",
    "    [0.1, 0.2, 0.1, 0.15, 0.1, 0.1, 0.1, 0.05, 0.05, 0.1],\n",
    "    [0.1, 0.05, 0.15, 0.1, 0.2, 0.1, 0.1, 0.05, 0.1, 0.05],\n",
    "    [0.05, 0.1, 0.05, 0.2, 0.1, 0.15, 0.1, 0.1, 0.05, 0.1],\n",
    "    [0.1, 0.05, 0.1, 0.05, 0.15, 0.1, 0.2, 0.1, 0.1, 0.05]\n",
    "]\n",
    "\n",
    "# Normalize the proportions within each chunk\n",
    "category_dist_in_chunks = [np.array(dist)/sum(dist) for dist in category_dist_in_chunks]\n",
    "\n",
    "def save_to_pkl(images, labels, path):\n",
    "    \"\"\"Save the images and labels to a Pickle file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        # Combine images and labels into one list\n",
    "        data = [np.append(image.flatten(), label) for image, label in zip(images, labels)]\n",
    "        pickle.dump(data, f)\n",
    "    print(f'Dataset saved to {path}')\n",
    "\n",
    "# Load CIFAR10 data\n",
    "(train_images, train_labels), _ = datasets.cifar10.load_data()\n",
    "\n",
    "# Create a dictionary where the keys are the labels and the values are the images\n",
    "data_dict = defaultdict(list)\n",
    "for image, label in zip(train_images, train_labels):\n",
    "    data_dict[label[0]].append(image)\n",
    "\n",
    "chunk_info = []\n",
    "# Create chunks\n",
    "for i in range(5):\n",
    "    chunk_images = []\n",
    "    chunk_labels = []\n",
    "    total_samples = sum([len(images) for images in data_dict.values()])\n",
    "    chunk_size = int(total_samples * chunk_sizes[i])\n",
    "    \n",
    "    for label, images in data_dict.items():\n",
    "        n_samples = int(len(images) * category_dist_in_chunks[i][label])\n",
    "        # Split data for this category\n",
    "        chunk, data_dict[label] = train_test_split(images, train_size=n_samples, shuffle=True)\n",
    "        chunk_images.extend(chunk)\n",
    "        chunk_labels.extend([label]*len(chunk))\n",
    "    \n",
    "    # Correct for rounding errors\n",
    "    diff = chunk_size - len(chunk_images)\n",
    "    if diff != 0:\n",
    "        label, images = max(data_dict.items(), key=lambda x: len(x[1]))\n",
    "        extra_samples, data_dict[label] = train_test_split(images, train_size=diff, shuffle=True)\n",
    "        chunk_images.extend(extra_samples)\n",
    "        chunk_labels.extend([label]*len(extra_samples))\n",
    "    \n",
    "    # Save the chunk\n",
    "    save_to_pkl(chunk_images, chunk_labels, f'data2/chunk_{i+1}.pkl')\n",
    "\n",
    "    # Collect the chunk information\n",
    "    chunk_info.append({\n",
    "        'chunk': i+1,\n",
    "        'size': len(chunk_images),\n",
    "        'label_distribution': dict(Counter(chunk_labels))\n",
    "    })\n",
    "\n",
    "# Save the chunk information into a CSV file\n",
    "df = pd.DataFrame(chunk_info)\n",
    "df.to_csv('data2/chunk_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 09:44:12.508986: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-01 09:44:12.718013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 09:44:13.730198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 21:45:40.427380: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-01 21:45:40.516313: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 21:45:41.748562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19531.25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 200\n",
    "datasize = 12500\n",
    "batch_size = 128\n",
    "\n",
    "total_1 = epoch * datasize / batch_size\n",
    "total_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_lr_scheduler(total_update_times: int = 380, initial_learning_rate: float = 0.1):\n",
    "    lr_scheduler = tf.keras.experimental.CosineDecay(initial_learning_rate= initial_learning_rate, \n",
    "                                                    decay_steps= total_update_times)\n",
    "    return lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 21:47:20.307834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:20.337595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:20.337633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:20.339589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:20.339621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:20.339634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:21.302439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:21.302504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:21.302509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-01 21:47:21.302528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 21:47:21.302554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update time: 1, learning rate: 0.10000000149011612\n",
      "update time: 2, learning rate: 0.09999837726354599\n",
      "update time: 3, learning rate: 0.0999935194849968\n",
      "update time: 4, learning rate: 0.09998539835214615\n",
      "update time: 5, learning rate: 0.09997405111789703\n",
      "update time: 6, learning rate: 0.09995945543050766\n",
      "update time: 7, learning rate: 0.09994161128997803\n",
      "update time: 8, learning rate: 0.09992053359746933\n",
      "update time: 9, learning rate: 0.09989621490240097\n",
      "update time: 10, learning rate: 0.09986865520477295\n",
      "update time: 11, learning rate: 0.09983786195516586\n",
      "update time: 12, learning rate: 0.09980384260416031\n",
      "update time: 13, learning rate: 0.09976658225059509\n",
      "update time: 14, learning rate: 0.09972609579563141\n",
      "update time: 15, learning rate: 0.09968238323926926\n",
      "update time: 16, learning rate: 0.09963544458150864\n",
      "update time: 17, learning rate: 0.09958528727293015\n",
      "update time: 18, learning rate: 0.09953191131353378\n",
      "update time: 19, learning rate: 0.09947532415390015\n",
      "update time: 20, learning rate: 0.09941551834344864\n",
      "update time: 21, learning rate: 0.09935251623392105\n",
      "update time: 22, learning rate: 0.09928631037473679\n",
      "update time: 23, learning rate: 0.09921690076589584\n",
      "update time: 24, learning rate: 0.09914429485797882\n",
      "update time: 25, learning rate: 0.09906851500272751\n",
      "update time: 26, learning rate: 0.09898953884840012\n",
      "update time: 27, learning rate: 0.09890738129615784\n",
      "update time: 28, learning rate: 0.09882205724716187\n",
      "update time: 29, learning rate: 0.0987335592508316\n",
      "update time: 30, learning rate: 0.09864190220832825\n",
      "update time: 31, learning rate: 0.09854709357023239\n",
      "update time: 32, learning rate: 0.09844913333654404\n",
      "update time: 33, learning rate: 0.09834802150726318\n",
      "update time: 34, learning rate: 0.09824378043413162\n",
      "update time: 35, learning rate: 0.09813640266656876\n",
      "update time: 36, learning rate: 0.09802591055631638\n",
      "update time: 37, learning rate: 0.09791228920221329\n",
      "update time: 38, learning rate: 0.09779556840658188\n",
      "update time: 39, learning rate: 0.09767574071884155\n",
      "update time: 40, learning rate: 0.0975528284907341\n",
      "update time: 41, learning rate: 0.09742682427167892\n",
      "update time: 42, learning rate: 0.09729774296283722\n",
      "update time: 43, learning rate: 0.09716559201478958\n",
      "update time: 44, learning rate: 0.09703037887811661\n",
      "update time: 45, learning rate: 0.09689211845397949\n",
      "update time: 46, learning rate: 0.09675081819295883\n",
      "update time: 47, learning rate: 0.09660647064447403\n",
      "update time: 48, learning rate: 0.09645911306142807\n",
      "update time: 49, learning rate: 0.09630873054265976\n",
      "update time: 50, learning rate: 0.09615534543991089\n",
      "update time: 51, learning rate: 0.09599897265434265\n",
      "update time: 52, learning rate: 0.09583961218595505\n",
      "update time: 53, learning rate: 0.09567727148532867\n",
      "update time: 54, learning rate: 0.09551197290420532\n",
      "update time: 55, learning rate: 0.09534372389316559\n",
      "update time: 56, learning rate: 0.09517252445220947\n",
      "update time: 57, learning rate: 0.09499839693307877\n",
      "update time: 58, learning rate: 0.09482134878635406\n",
      "update time: 59, learning rate: 0.09464138746261597\n",
      "update time: 60, learning rate: 0.09445854276418686\n",
      "update time: 61, learning rate: 0.09427280724048615\n",
      "update time: 62, learning rate: 0.09408419579267502\n",
      "update time: 63, learning rate: 0.09389271587133408\n",
      "update time: 64, learning rate: 0.0936984047293663\n",
      "update time: 65, learning rate: 0.0935012474656105\n",
      "update time: 66, learning rate: 0.09330127388238907\n",
      "update time: 67, learning rate: 0.093098483979702\n",
      "update time: 68, learning rate: 0.09289290010929108\n",
      "update time: 69, learning rate: 0.09268452972173691\n",
      "update time: 70, learning rate: 0.09247340261936188\n",
      "update time: 71, learning rate: 0.09225950390100479\n",
      "update time: 72, learning rate: 0.09204287081956863\n",
      "update time: 73, learning rate: 0.091823510825634\n",
      "update time: 74, learning rate: 0.0916014313697815\n",
      "update time: 75, learning rate: 0.0913766548037529\n",
      "update time: 76, learning rate: 0.09114919602870941\n",
      "update time: 77, learning rate: 0.09091906994581223\n",
      "update time: 78, learning rate: 0.09068627655506134\n",
      "update time: 79, learning rate: 0.09045084565877914\n",
      "update time: 80, learning rate: 0.09021279960870743\n",
      "update time: 81, learning rate: 0.08997213840484619\n",
      "update time: 82, learning rate: 0.08972888439893723\n",
      "update time: 83, learning rate: 0.08948304504156113\n",
      "update time: 84, learning rate: 0.08923466503620148\n",
      "update time: 85, learning rate: 0.08898371458053589\n",
      "update time: 86, learning rate: 0.08873025327920914\n",
      "update time: 87, learning rate: 0.08847426623106003\n",
      "update time: 88, learning rate: 0.08821579068899155\n",
      "update time: 89, learning rate: 0.08795483410358429\n",
      "update time: 90, learning rate: 0.08769140392541885\n",
      "update time: 91, learning rate: 0.08742554485797882\n",
      "update time: 92, learning rate: 0.087157242000103\n",
      "update time: 93, learning rate: 0.08688654005527496\n",
      "update time: 94, learning rate: 0.08661343902349472\n",
      "update time: 95, learning rate: 0.08633796125650406\n",
      "update time: 96, learning rate: 0.08606012910604477\n",
      "update time: 97, learning rate: 0.08577995002269745\n",
      "update time: 98, learning rate: 0.08549745380878448\n",
      "update time: 99, learning rate: 0.08521264791488647\n",
      "update time: 100, learning rate: 0.0849255695939064\n",
      "update time: 101, learning rate: 0.08463621884584427\n",
      "update time: 102, learning rate: 0.08434461802244186\n",
      "update time: 103, learning rate: 0.08405078947544098\n",
      "update time: 104, learning rate: 0.0837547555565834\n",
      "update time: 105, learning rate: 0.08345653116703033\n",
      "update time: 106, learning rate: 0.08315613120794296\n",
      "update time: 107, learning rate: 0.08285357803106308\n",
      "update time: 108, learning rate: 0.08254890888929367\n",
      "update time: 109, learning rate: 0.08224210888147354\n",
      "update time: 110, learning rate: 0.08193322271108627\n",
      "update time: 111, learning rate: 0.08162226527929306\n",
      "update time: 112, learning rate: 0.0813092589378357\n",
      "update time: 113, learning rate: 0.08099421113729477\n",
      "update time: 114, learning rate: 0.08067716658115387\n",
      "update time: 115, learning rate: 0.080358125269413\n",
      "update time: 116, learning rate: 0.08003710955381393\n",
      "update time: 117, learning rate: 0.07971414923667908\n",
      "update time: 118, learning rate: 0.07938926666975021\n",
      "update time: 119, learning rate: 0.07906246185302734\n",
      "update time: 120, learning rate: 0.07873377948999405\n",
      "update time: 121, learning rate: 0.07840323448181152\n",
      "update time: 122, learning rate: 0.07807084172964096\n",
      "update time: 123, learning rate: 0.07773663848638535\n",
      "update time: 124, learning rate: 0.07740062475204468\n",
      "update time: 125, learning rate: 0.07706283777952194\n",
      "update time: 126, learning rate: 0.07672329246997833\n",
      "update time: 127, learning rate: 0.07638200372457504\n",
      "update time: 128, learning rate: 0.07603901624679565\n",
      "update time: 129, learning rate: 0.07569433748722076\n",
      "update time: 130, learning rate: 0.07534798979759216\n",
      "update time: 131, learning rate: 0.07500000298023224\n",
      "update time: 132, learning rate: 0.07465038448572159\n",
      "update time: 133, learning rate: 0.0742991715669632\n",
      "update time: 134, learning rate: 0.07394637912511826\n",
      "update time: 135, learning rate: 0.07359203696250916\n",
      "update time: 136, learning rate: 0.07323615998029709\n",
      "update time: 137, learning rate: 0.07287877053022385\n",
      "update time: 138, learning rate: 0.07251991331577301\n",
      "update time: 139, learning rate: 0.07215958088636398\n",
      "update time: 140, learning rate: 0.07179781049489975\n",
      "update time: 141, learning rate: 0.0714346319437027\n",
      "update time: 142, learning rate: 0.07107005268335342\n",
      "update time: 143, learning rate: 0.07070411741733551\n",
      "update time: 144, learning rate: 0.07033683359622955\n",
      "update time: 145, learning rate: 0.06996822357177734\n",
      "update time: 146, learning rate: 0.06959833204746246\n",
      "update time: 147, learning rate: 0.06922715902328491\n",
      "update time: 148, learning rate: 0.06885474175214767\n",
      "update time: 149, learning rate: 0.06848110258579254\n",
      "update time: 150, learning rate: 0.0681062564253807\n",
      "update time: 151, learning rate: 0.06773024797439575\n",
      "update time: 152, learning rate: 0.06735307723283768\n",
      "update time: 153, learning rate: 0.06697478145360947\n",
      "update time: 154, learning rate: 0.0665953978896141\n",
      "update time: 155, learning rate: 0.066214919090271\n",
      "update time: 156, learning rate: 0.06583339720964432\n",
      "update time: 157, learning rate: 0.06545084714889526\n",
      "update time: 158, learning rate: 0.06506729871034622\n",
      "update time: 159, learning rate: 0.06468276679515839\n",
      "update time: 160, learning rate: 0.06429728120565414\n",
      "update time: 161, learning rate: 0.06391087174415588\n",
      "update time: 162, learning rate: 0.0635235533118248\n",
      "update time: 163, learning rate: 0.06313537061214447\n",
      "update time: 164, learning rate: 0.0627463236451149\n",
      "update time: 165, learning rate: 0.06235644966363907\n",
      "update time: 166, learning rate: 0.061965782195329666\n",
      "update time: 167, learning rate: 0.061574336141347885\n",
      "update time: 168, learning rate: 0.061182137578725815\n",
      "update time: 169, learning rate: 0.06078920513391495\n",
      "update time: 170, learning rate: 0.060395579785108566\n",
      "update time: 171, learning rate: 0.06000128388404846\n",
      "update time: 172, learning rate: 0.05960633233189583\n",
      "update time: 173, learning rate: 0.05921075865626335\n",
      "update time: 174, learning rate: 0.05881459265947342\n",
      "update time: 175, learning rate: 0.05841784551739693\n",
      "update time: 176, learning rate: 0.058020561933517456\n",
      "update time: 177, learning rate: 0.0576227568089962\n",
      "update time: 178, learning rate: 0.05722444877028465\n",
      "update time: 179, learning rate: 0.056825678795576096\n",
      "update time: 180, learning rate: 0.05642646551132202\n",
      "update time: 181, learning rate: 0.05602683499455452\n",
      "update time: 182, learning rate: 0.05562680959701538\n",
      "update time: 183, learning rate: 0.05522642284631729\n",
      "update time: 184, learning rate: 0.05482569336891174\n",
      "update time: 185, learning rate: 0.054424650967121124\n",
      "update time: 186, learning rate: 0.05402332544326782\n",
      "update time: 187, learning rate: 0.05362173914909363\n",
      "update time: 188, learning rate: 0.05321991443634033\n",
      "update time: 189, learning rate: 0.05281788110733032\n",
      "update time: 190, learning rate: 0.052415668964385986\n",
      "update time: 191, learning rate: 0.052013296633958817\n",
      "update time: 192, learning rate: 0.0516107939183712\n",
      "update time: 193, learning rate: 0.05120818689465523\n",
      "update time: 194, learning rate: 0.050805505365133286\n",
      "update time: 195, learning rate: 0.05040276050567627\n",
      "update time: 196, learning rate: 0.04999999701976776\n",
      "update time: 197, learning rate: 0.049597229808568954\n",
      "update time: 198, learning rate: 0.049194496124982834\n",
      "update time: 199, learning rate: 0.048791807144880295\n",
      "update time: 200, learning rate: 0.04838920757174492\n",
      "update time: 201, learning rate: 0.047986697405576706\n",
      "update time: 202, learning rate: 0.047584328800439835\n",
      "update time: 203, learning rate: 0.0471821092069149\n",
      "update time: 204, learning rate: 0.04678008332848549\n",
      "update time: 205, learning rate: 0.046378251165151596\n",
      "update time: 206, learning rate: 0.0459766685962677\n",
      "update time: 207, learning rate: 0.0455753393471241\n",
      "update time: 208, learning rate: 0.04517430067062378\n",
      "update time: 209, learning rate: 0.04477357119321823\n",
      "update time: 210, learning rate: 0.04437318816781044\n",
      "update time: 211, learning rate: 0.043973159044981\n",
      "update time: 212, learning rate: 0.0435735322535038\n",
      "update time: 213, learning rate: 0.04317431151866913\n",
      "update time: 214, learning rate: 0.04277554899454117\n",
      "update time: 215, learning rate: 0.04237723350524902\n",
      "update time: 216, learning rate: 0.041979435831308365\n",
      "update time: 217, learning rate: 0.0415821447968483\n",
      "update time: 218, learning rate: 0.0411854051053524\n",
      "update time: 219, learning rate: 0.040789227932691574\n",
      "update time: 220, learning rate: 0.040393661707639694\n",
      "update time: 221, learning rate: 0.03999871015548706\n",
      "update time: 222, learning rate: 0.039604414254426956\n",
      "update time: 223, learning rate: 0.039210785180330276\n",
      "update time: 224, learning rate: 0.038817860186100006\n",
      "update time: 225, learning rate: 0.03842565789818764\n",
      "update time: 226, learning rate: 0.038034215569496155\n",
      "update time: 227, learning rate: 0.037643540650606155\n",
      "update time: 228, learning rate: 0.03725367411971092\n",
      "update time: 229, learning rate: 0.03686462715268135\n",
      "update time: 230, learning rate: 0.036476440727710724\n",
      "update time: 231, learning rate: 0.03608912229537964\n",
      "update time: 232, learning rate: 0.03570271655917168\n",
      "update time: 233, learning rate: 0.035317227244377136\n",
      "update time: 234, learning rate: 0.0349326990544796\n",
      "update time: 235, learning rate: 0.03454914316534996\n",
      "update time: 236, learning rate: 0.0341666005551815\n",
      "update time: 237, learning rate: 0.033785074949264526\n",
      "update time: 238, learning rate: 0.03340460732579231\n",
      "update time: 239, learning rate: 0.03302520886063576\n",
      "update time: 240, learning rate: 0.03264692425727844\n",
      "update time: 241, learning rate: 0.03226975351572037\n",
      "update time: 242, learning rate: 0.031893741339445114\n",
      "update time: 243, learning rate: 0.03151889517903328\n",
      "update time: 244, learning rate: 0.031145257875323296\n",
      "update time: 245, learning rate: 0.03077283501625061\n",
      "update time: 246, learning rate: 0.030401671305298805\n",
      "update time: 247, learning rate: 0.03003176860511303\n",
      "update time: 248, learning rate: 0.029663166031241417\n",
      "update time: 249, learning rate: 0.02929588034749031\n",
      "update time: 250, learning rate: 0.028929943218827248\n",
      "update time: 251, learning rate: 0.028565365821123123\n",
      "update time: 252, learning rate: 0.02820218913257122\n",
      "update time: 253, learning rate: 0.027840418741106987\n",
      "update time: 254, learning rate: 0.027480095624923706\n",
      "update time: 255, learning rate: 0.027121221646666527\n",
      "update time: 256, learning rate: 0.02676383964717388\n",
      "update time: 257, learning rate: 0.026407957077026367\n",
      "update time: 258, learning rate: 0.026053620502352715\n",
      "update time: 259, learning rate: 0.025700822472572327\n",
      "update time: 260, learning rate: 0.02534961700439453\n",
      "update time: 261, learning rate: 0.02499999664723873\n",
      "update time: 262, learning rate: 0.02465200237929821\n",
      "update time: 263, learning rate: 0.02430565096437931\n",
      "update time: 264, learning rate: 0.02396097593009472\n",
      "update time: 265, learning rate: 0.023617981001734734\n",
      "update time: 266, learning rate: 0.023276707157492638\n",
      "update time: 267, learning rate: 0.022937161847949028\n",
      "update time: 268, learning rate: 0.02259937860071659\n",
      "update time: 269, learning rate: 0.022263363003730774\n",
      "update time: 270, learning rate: 0.021929148584604263\n",
      "update time: 271, learning rate: 0.02159675396978855\n",
      "update time: 272, learning rate: 0.021266212686896324\n",
      "update time: 273, learning rate: 0.02093753032386303\n",
      "update time: 274, learning rate: 0.020610738545656204\n",
      "update time: 275, learning rate: 0.020285844802856445\n",
      "update time: 276, learning rate: 0.01996288076043129\n",
      "update time: 277, learning rate: 0.01964186690747738\n",
      "update time: 278, learning rate: 0.01932283118367195\n",
      "update time: 279, learning rate: 0.019005775451660156\n",
      "update time: 280, learning rate: 0.018690740689635277\n",
      "update time: 281, learning rate: 0.018377726897597313\n",
      "update time: 282, learning rate: 0.01806676760315895\n",
      "update time: 283, learning rate: 0.017757881432771683\n",
      "update time: 284, learning rate: 0.01745109260082245\n",
      "update time: 285, learning rate: 0.017146408557891846\n",
      "update time: 286, learning rate: 0.016843868419528008\n",
      "update time: 287, learning rate: 0.016543466597795486\n",
      "update time: 288, learning rate: 0.016245245933532715\n",
      "update time: 289, learning rate: 0.015949198976159096\n",
      "update time: 290, learning rate: 0.01565537415444851\n",
      "update time: 291, learning rate: 0.015363777056336403\n",
      "update time: 292, learning rate: 0.015074431896209717\n",
      "update time: 293, learning rate: 0.014787343330681324\n",
      "update time: 294, learning rate: 0.014502549543976784\n",
      "update time: 295, learning rate: 0.01422005333006382\n",
      "update time: 296, learning rate: 0.013939875178039074\n",
      "update time: 297, learning rate: 0.013662037439644337\n",
      "update time: 298, learning rate: 0.013386565260589123\n",
      "update time: 299, learning rate: 0.013113463297486305\n",
      "update time: 300, learning rate: 0.012842759490013123\n",
      "update time: 301, learning rate: 0.012574461288750172\n",
      "update time: 302, learning rate: 0.012308591976761818\n",
      "update time: 303, learning rate: 0.012045166455209255\n",
      "update time: 304, learning rate: 0.011784213595092297\n",
      "update time: 305, learning rate: 0.011525732465088367\n",
      "update time: 306, learning rate: 0.011269751004874706\n",
      "update time: 307, learning rate: 0.011016279458999634\n",
      "update time: 308, learning rate: 0.010765338316559792\n",
      "update time: 309, learning rate: 0.010516941547393799\n",
      "update time: 310, learning rate: 0.01027111429721117\n",
      "update time: 311, learning rate: 0.010027858428657055\n",
      "update time: 312, learning rate: 0.009787201881408691\n",
      "update time: 313, learning rate: 0.009549147449433804\n",
      "update time: 314, learning rate: 0.009313726797699928\n",
      "update time: 315, learning rate: 0.009080938063561916\n",
      "update time: 316, learning rate: 0.008850804530084133\n",
      "update time: 317, learning rate: 0.008623341098427773\n",
      "update time: 318, learning rate: 0.00839856918901205\n",
      "update time: 319, learning rate: 0.008176490664482117\n",
      "update time: 320, learning rate: 0.007957130670547485\n",
      "update time: 321, learning rate: 0.007740494795143604\n",
      "update time: 322, learning rate: 0.007526600267738104\n",
      "update time: 323, learning rate: 0.007315462920814753\n",
      "update time: 324, learning rate: 0.007107099983841181\n",
      "update time: 325, learning rate: 0.0069015147164464\n",
      "update time: 326, learning rate: 0.006698730867356062\n",
      "update time: 327, learning rate: 0.006498751230537891\n",
      "update time: 328, learning rate: 0.006301596760749817\n",
      "update time: 329, learning rate: 0.006107273977249861\n",
      "update time: 330, learning rate: 0.005915808957070112\n",
      "update time: 331, learning rate: 0.005727195646613836\n",
      "update time: 332, learning rate: 0.005541461985558271\n",
      "update time: 333, learning rate: 0.005358606576919556\n",
      "update time: 334, learning rate: 0.005178654100745916\n",
      "update time: 335, learning rate: 0.0050016045570373535\n",
      "update time: 336, learning rate: 0.004827475640922785\n",
      "update time: 337, learning rate: 0.004656278993934393\n",
      "update time: 338, learning rate: 0.004488027188926935\n",
      "update time: 339, learning rate: 0.004322725813835859\n",
      "update time: 340, learning rate: 0.004160392563790083\n",
      "update time: 341, learning rate: 0.004001027438789606\n",
      "update time: 342, learning rate: 0.003844648599624634\n",
      "update time: 343, learning rate: 0.0036912651266902685\n",
      "update time: 344, learning rate: 0.0035408884286880493\n",
      "update time: 345, learning rate: 0.0033935250248759985\n",
      "update time: 346, learning rate: 0.0032491893507540226\n",
      "update time: 347, learning rate: 0.0031078816391527653\n",
      "update time: 348, learning rate: 0.0029696195852011442\n",
      "update time: 349, learning rate: 0.0028344064485281706\n",
      "update time: 350, learning rate: 0.0027022571302950382\n",
      "update time: 351, learning rate: 0.002573177218437195\n",
      "update time: 352, learning rate: 0.0024471760261803865\n",
      "update time: 353, learning rate: 0.0023242563474923372\n",
      "update time: 354, learning rate: 0.0022044330835342407\n",
      "update time: 355, learning rate: 0.0020877064671367407\n",
      "update time: 356, learning rate: 0.001974093960598111\n",
      "update time: 357, learning rate: 0.0018635959131643176\n",
      "update time: 358, learning rate: 0.001756221055984497\n",
      "update time: 359, learning rate: 0.001651978469453752\n",
      "update time: 360, learning rate: 0.0015508740907534957\n",
      "update time: 361, learning rate: 0.001452907919883728\n",
      "update time: 362, learning rate: 0.0013580948580056429\n",
      "update time: 363, learning rate: 0.001266437815502286\n",
      "update time: 364, learning rate: 0.001177945756353438\n",
      "update time: 365, learning rate: 0.0010926186805590987\n",
      "update time: 366, learning rate: 0.0010104685788974166\n",
      "update time: 367, learning rate: 0.0009314924827776849\n",
      "update time: 368, learning rate: 0.0008557021501474082\n",
      "update time: 369, learning rate: 0.0007831007242202759\n",
      "update time: 370, learning rate: 0.0007136971107684076\n",
      "update time: 371, learning rate: 0.0006474852561950684\n",
      "update time: 372, learning rate: 0.0005844801780767739\n",
      "update time: 373, learning rate: 0.0005246788496151567\n",
      "update time: 374, learning rate: 0.0004680901765823364\n",
      "update time: 375, learning rate: 0.0004147142171859741\n",
      "update time: 376, learning rate: 0.0003645569086074829\n",
      "update time: 377, learning rate: 0.0003176182508468628\n",
      "update time: 378, learning rate: 0.0002739042101893574\n",
      "update time: 379, learning rate: 0.00023341775522567332\n",
      "update time: 380, learning rate: 0.00019616186909843236\n",
      "update time: 381, learning rate: 0.00016213358321692795\n",
      "update time: 382, learning rate: 0.00013134181790519506\n",
      "update time: 383, learning rate: 0.00010378658771514893\n",
      "update time: 384, learning rate: 7.946789264678955e-05\n",
      "update time: 385, learning rate: 5.838871220475994e-05\n",
      "update time: 386, learning rate: 4.054903911310248e-05\n",
      "update time: 387, learning rate: 2.5951861971407197e-05\n",
      "update time: 388, learning rate: 1.4600157555832993e-05\n",
      "update time: 389, learning rate: 6.487965492851799e-06\n",
      "update time: 390, learning rate: 1.62124638336536e-06\n"
     ]
    }
   ],
   "source": [
    "total_update_times = 390\n",
    "lr_scheduler = get_cosine_lr_scheduler(total_update_times= total_update_times)\n",
    "\n",
    "for update_time in range(total_update_times):\n",
    "    print(f\"update time: {update_time + 1}, learning rate: {float(lr_scheduler(update_time))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200\n",
    "datasize = 12500\n",
    "batch_size = 128\n",
    "\n",
    "total_1 = epoch * datasize / batch_size\n",
    "initial_lr = 0.1\n",
    "lr_scheduler = get_cosine_lr_scheduler(initial_learning_rate= initial_lr,\n",
    "                                       total_update_times= total_1)\n",
    "\n",
    "for update_time in range(total_update_times):\n",
    "    print(f\"update time: {update_time + 1}, learning rate: {float(lr_scheduler(update_time))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
