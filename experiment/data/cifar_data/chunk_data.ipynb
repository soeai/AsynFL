{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 03:58:54.976395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 03:58:55.575679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04736183 0.05161072 0.05689929 0.05039335 0.04208938 0.05147302\n",
      " 0.05004039 0.04254451 0.05239361 0.05142695 0.05020426 0.03729472\n",
      " 0.05356149 0.05485727 0.0554516  0.04848357 0.04788524 0.04906771\n",
      " 0.05829911 0.048662  ]\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate an array of 20 random numbers around the mean using a normal distribution\n",
    "mean = 0.05\n",
    "std_dev = 0.005  # Small standard deviation to keep numbers close to the mean\n",
    "array = np.random.normal(mean, std_dev, 20)\n",
    "\n",
    "# Clip values to range [0,1]\n",
    "array = np.clip(array, 0, 1)\n",
    "\n",
    "# Normalize the array so the sum of all elements equals 1\n",
    "array /= np.sum(array)\n",
    "\n",
    "print(array)\n",
    "print(np.sum(array))  # This should print '1.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2368, 2581, 2845, 2520, 2104, 2574, 2502, 2127, 2620, 2571, 2510,\n",
       "        1865, 2678, 2743, 2773, 2424, 2394, 2453, 2915, 2433]),\n",
       " 50000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_samples = len(train_data)\n",
    "\n",
    "# Distribute the total number of samples proportionally and round to the nearest integer\n",
    "sizes = np.round(array * total_samples).astype(int)\n",
    "\n",
    "# Adjust the last element to ensure the sum equals to total_samples\n",
    "sizes[-1] = total_samples - np.sum(sizes[:-1])\n",
    "\n",
    "sizes, np.sum(sizes)  # This should print '50000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices at which to split the training data\n",
    "split_indices = np.cumsum(sizes)[:-1]\n",
    "\n",
    "# Split the training data into chunks of different sizes\n",
    "chunks = np.split(train_data, split_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"20_chunks_iid\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_folder = \"iid\"\n",
    "iid_folder = os.path.join(folder, iid_folder)\n",
    "if not os.path.exists(iid_folder):\n",
    "    os.makedirs(iid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "\n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{iid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chunk  size                                 label_distribution  \\\n",
      "0       1  2368  {0: 237, 1: 219, 2: 243, 3: 260, 4: 245, 5: 23...   \n",
      "1       2  2581  {0: 263, 1: 252, 2: 268, 3: 242, 4: 241, 5: 26...   \n",
      "2       3  2845  {0: 288, 1: 293, 2: 284, 3: 298, 4: 281, 5: 29...   \n",
      "3       4  2520  {0: 243, 1: 302, 2: 251, 3: 252, 4: 252, 5: 22...   \n",
      "4       5  2104  {0: 238, 1: 201, 2: 225, 3: 214, 4: 209, 5: 20...   \n",
      "5       6  2574  {0: 246, 1: 291, 2: 247, 3: 263, 4: 275, 5: 25...   \n",
      "6       7  2502  {0: 237, 1: 244, 2: 250, 3: 267, 4: 246, 5: 24...   \n",
      "7       8  2127  {0: 218, 1: 198, 2: 182, 3: 208, 4: 218, 5: 23...   \n",
      "8       9  2620  {0: 279, 1: 280, 2: 240, 3: 291, 4: 257, 5: 24...   \n",
      "9      10  2571  {0: 272, 1: 243, 2: 269, 3: 277, 4: 234, 5: 23...   \n",
      "10     11  2510  {0: 258, 1: 216, 2: 279, 3: 236, 4: 264, 5: 23...   \n",
      "11     12  1865  {0: 176, 1: 171, 2: 178, 3: 188, 4: 200, 5: 19...   \n",
      "12     13  2678  {0: 277, 1: 284, 2: 301, 3: 233, 4: 271, 5: 25...   \n",
      "13     14  2743  {0: 255, 1: 247, 2: 272, 3: 265, 4: 267, 5: 30...   \n",
      "14     15  2773  {0: 233, 1: 285, 2: 265, 3: 283, 4: 287, 5: 30...   \n",
      "15     16  2424  {0: 236, 1: 265, 2: 243, 3: 226, 4: 243, 5: 24...   \n",
      "16     17  2394  {0: 253, 1: 241, 2: 221, 3: 238, 4: 232, 5: 22...   \n",
      "17     18  2453  {0: 250, 1: 230, 2: 235, 3: 246, 4: 237, 5: 24...   \n",
      "18     19  2915  {0: 292, 1: 275, 2: 301, 3: 277, 4: 301, 5: 28...   \n",
      "19     20  2433  {0: 249, 1: 263, 2: 246, 3: 236, 4: 240, 5: 25...   \n",
      "\n",
      "                                    label_proportions  \n",
      "0   {0: 0.10008445945945946, 1: 0.0924831081081081...  \n",
      "1   {0: 0.1018984889577683, 1: 0.0976365749709415,...  \n",
      "2   {0: 0.10123022847100176, 1: 0.1029876977152899...  \n",
      "3   {0: 0.09642857142857143, 1: 0.1198412698412698...  \n",
      "4   {0: 0.11311787072243346, 1: 0.0955323193916349...  \n",
      "5   {0: 0.09557109557109557, 1: 0.1130536130536130...  \n",
      "6   {0: 0.09472422062350119, 1: 0.0975219824140687...  \n",
      "7   {0: 0.10249177244945933, 1: 0.0930888575458392...  \n",
      "8   {0: 0.1064885496183206, 1: 0.10687022900763359...  \n",
      "9   {0: 0.10579541034616881, 1: 0.0945157526254375...  \n",
      "10  {0: 0.10278884462151394, 1: 0.0860557768924302...  \n",
      "11  {0: 0.09436997319034852, 1: 0.0916890080428954...  \n",
      "12  {0: 0.10343539955190441, 1: 0.1060492905153099...  \n",
      "13  {0: 0.0929639081297849, 1: 0.09004739336492891...  \n",
      "14  {0: 0.08402452217814642, 1: 0.1027767760548142...  \n",
      "15  {0: 0.09735973597359736, 1: 0.1093234323432343...  \n",
      "16  {0: 0.10568086883876357, 1: 0.1006683375104427...  \n",
      "17  {0: 0.10191602119853241, 1: 0.0937627395026498...  \n",
      "18  {0: 0.10017152658662093, 1: 0.0943396226415094...  \n",
      "19  {0: 0.10234278668310727, 1: 0.1080969995889847...  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{iid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distribution_unbalanced(chunk, seed=None):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "\n",
    "    # Get unique labels in the chunk\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Assign random weights to each unique label\n",
    "    label_weights = np.random.rand(len(unique_labels))\n",
    "\n",
    "    # Normalize weights so they sum to 1\n",
    "    label_weights /= label_weights.sum()\n",
    "\n",
    "    # Create a new chunk with an unbalanced label distribution\n",
    "    unbalanced_chunk = np.empty((0, chunk.shape[1]), dtype=chunk.dtype)\n",
    "\n",
    "    for label, weight in zip(unique_labels, label_weights):\n",
    "        # Calculate the number of samples for this label\n",
    "        num_samples = int(weight * len(chunk))\n",
    "\n",
    "        # Get all samples with this label\n",
    "        label_samples = chunk[labels == label]\n",
    "\n",
    "        # Randomly select samples of this label\n",
    "        selected_samples = np.random.choice(len(label_samples), size=num_samples)\n",
    "\n",
    "        # Add selected samples to the new chunk\n",
    "        unbalanced_chunk = np.vstack([unbalanced_chunk, label_samples[selected_samples]])\n",
    "\n",
    "    return unbalanced_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "noniid_folder = \"noniid\"\n",
    "noniid_folder = os.path.join(folder, noniid_folder)\n",
    "if not os.path.exists(noniid_folder):\n",
    "    os.makedirs(noniid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store size and label distribution of each chunk, and save each chunk as a pickle file\n",
    "unbalanced_chunk_info = []\n",
    "# Now, make the distribution unbalanced for each chunk before saving\n",
    "for i, chunk in enumerate(chunks):\n",
    "    unbalanced_chunk = make_distribution_unbalanced(chunk, seed=i)\n",
    "\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    label_distribution = get_label_distribution(unbalanced_chunk)\n",
    "    chunk_size =len(unbalanced_chunk)\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    unbalanced_chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{noniid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(unbalanced_chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk   size                                 label_distribution  \\\n",
      "0      1   4996  {0: 445, 1: 580, 2: 489, 3: 442, 4: 344, 5: 52...   \n",
      "1      2   7495  {0: 994, 1: 1717, 3: 720, 4: 349, 5: 220, 6: 4...   \n",
      "2      3   9995  {0: 1215, 1: 72, 2: 1531, 3: 1213, 4: 1171, 5:...   \n",
      "3      4  12494  {0: 1472, 1: 1893, 2: 777, 3: 1365, 4: 2387, 5...   \n",
      "4      5  14996  {0: 2507, 1: 1418, 2: 2521, 3: 1853, 4: 1808, ...   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0: 0.08907125700560449, 1: 0.1160928742994395...  \n",
      "1  {0: 0.13262174783188793, 1: 0.2290860573715810...  \n",
      "2  {0: 0.12156078039019509, 1: 0.0072036018009004...  \n",
      "3  {0: 0.11781655194493357, 1: 0.1515127261085321...  \n",
      "4  {0: 0.16717791411042945, 1: 0.0945585489463857...  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "noniid_df = pd.DataFrame(unbalanced_chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(noniid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "noniid_df.to_csv(f\"{noniid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>size</th>\n",
       "      <th>label_distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8695</td>\n",
       "      <td>{0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4347</td>\n",
       "      <td>{0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>13043</td>\n",
       "      <td>{0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10869</td>\n",
       "      <td>{0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2181</td>\n",
       "      <td>{0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk   size                                 label_distribution\n",
       "0      1   2173  {0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...\n",
       "1      2   2173  {0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...\n",
       "2      3   8695  {0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...\n",
       "3      4   2173  {0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...\n",
       "4      5   4347  {0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...\n",
       "5      6   2173  {0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...\n",
       "6      7  13043  {0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...\n",
       "7      8   2173  {0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...\n",
       "8      9  10869  {0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...\n",
       "9     10   2181  {0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...\n",
       "1    {0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...\n",
       "2    {0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...\n",
       "3    {0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...\n",
       "4    {0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...\n",
       "5    {0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...\n",
       "6    {0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...\n",
       "7    {0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...\n",
       "8    {0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...\n",
       "9    {0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24...\n",
       "Name: label_distribution, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data2/chunk_1.pkl\n",
      "Dataset saved to data2/chunk_2.pkl\n",
      "Dataset saved to data2/chunk_3.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "train_size=4463 should be either positive and smaller than the number of samples 3765 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m diff \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     label, images \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(data_dict\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 61\u001b[0m     extra_samples, data_dict[label] \u001b[39m=\u001b[39m train_test_split(images, train_size\u001b[39m=\u001b[39;49mdiff, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m     chunk_images\u001b[39m.\u001b[39mextend(extra_samples)\n\u001b[1;32m     63\u001b[0m     chunk_labels\u001b[39m.\u001b[39mextend([label]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(extra_samples))\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2230\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtest_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2220\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2221\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_size, n_samples)\n\u001b[1;32m   2222\u001b[0m     )\n\u001b[1;32m   2224\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2225\u001b[0m     train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2226\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n_samples \u001b[39mor\u001b[39;00m train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2227\u001b[0m     \u001b[39mor\u001b[39;00m train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2228\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m   2229\u001b[0m ):\n\u001b[0;32m-> 2230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2232\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2233\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size, n_samples)\n\u001b[1;32m   2234\u001b[0m     )\n\u001b[1;32m   2236\u001b[0m \u001b[39mif\u001b[39;00m train_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m train_size_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2237\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid value for train_size: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size))\n",
      "\u001b[0;31mValueError\u001b[0m: train_size=4463 should be either positive and smaller than the number of samples 3765 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the proportion of each chunk\n",
    "chunk_sizes = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Define the proportion of each category within each chunk\n",
    "category_dist_in_chunks = [\n",
    "    [0.3, 0.2, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05, 0.025, 0.025],\n",
    "    [0.1, 0.2, 0.1, 0.15, 0.1, 0.1, 0.1, 0.05, 0.05, 0.1],\n",
    "    [0.1, 0.05, 0.15, 0.1, 0.2, 0.1, 0.1, 0.05, 0.1, 0.05],\n",
    "    [0.05, 0.1, 0.05, 0.2, 0.1, 0.15, 0.1, 0.1, 0.05, 0.1],\n",
    "    [0.1, 0.05, 0.1, 0.05, 0.15, 0.1, 0.2, 0.1, 0.1, 0.05]\n",
    "]\n",
    "\n",
    "# Normalize the proportions within each chunk\n",
    "category_dist_in_chunks = [np.array(dist)/sum(dist) for dist in category_dist_in_chunks]\n",
    "\n",
    "def save_to_pkl(images, labels, path):\n",
    "    \"\"\"Save the images and labels to a Pickle file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        # Combine images and labels into one list\n",
    "        data = [np.append(image.flatten(), label) for image, label in zip(images, labels)]\n",
    "        pickle.dump(data, f)\n",
    "    print(f'Dataset saved to {path}')\n",
    "\n",
    "# Load CIFAR10 data\n",
    "(train_images, train_labels), _ = datasets.cifar10.load_data()\n",
    "\n",
    "# Create a dictionary where the keys are the labels and the values are the images\n",
    "data_dict = defaultdict(list)\n",
    "for image, label in zip(train_images, train_labels):\n",
    "    data_dict[label[0]].append(image)\n",
    "\n",
    "chunk_info = []\n",
    "# Create chunks\n",
    "for i in range(5):\n",
    "    chunk_images = []\n",
    "    chunk_labels = []\n",
    "    total_samples = sum([len(images) for images in data_dict.values()])\n",
    "    chunk_size = int(total_samples * chunk_sizes[i])\n",
    "    \n",
    "    for label, images in data_dict.items():\n",
    "        n_samples = int(len(images) * category_dist_in_chunks[i][label])\n",
    "        # Split data for this category\n",
    "        chunk, data_dict[label] = train_test_split(images, train_size=n_samples, shuffle=True)\n",
    "        chunk_images.extend(chunk)\n",
    "        chunk_labels.extend([label]*len(chunk))\n",
    "    \n",
    "    # Correct for rounding errors\n",
    "    diff = chunk_size - len(chunk_images)\n",
    "    if diff != 0:\n",
    "        label, images = max(data_dict.items(), key=lambda x: len(x[1]))\n",
    "        extra_samples, data_dict[label] = train_test_split(images, train_size=diff, shuffle=True)\n",
    "        chunk_images.extend(extra_samples)\n",
    "        chunk_labels.extend([label]*len(extra_samples))\n",
    "    \n",
    "    # Save the chunk\n",
    "    save_to_pkl(chunk_images, chunk_labels, f'data2/chunk_{i+1}.pkl')\n",
    "\n",
    "    # Collect the chunk information\n",
    "    chunk_info.append({\n",
    "        'chunk': i+1,\n",
    "        'size': len(chunk_images),\n",
    "        'label_distribution': dict(Counter(chunk_labels))\n",
    "    })\n",
    "\n",
    "# Save the chunk information into a CSV file\n",
    "df = pd.DataFrame(chunk_info)\n",
    "df.to_csv('data2/chunk_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
