{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 16:27:07.580455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 16:27:08.321894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(x_train)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n"
     ]
    }
   ],
   "source": [
    "def count_classes(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Testing the function\n",
    "class_counts = count_classes(y_train)\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Number of chunks\n",
    "# n_chunks = 5\n",
    "\n",
    "# # Generate sizes following a power-law distribution\n",
    "# sizes = np.random.zipf(1.5, n_chunks)\n",
    "\n",
    "# # Normalize the sizes so that their sum equals the number of training samples\n",
    "# sizes = (sizes / sizes.sum() * len(train_data)).astype(int)\n",
    "\n",
    "# # Ensure that the sum of sizes is equal to the total number of training samples\n",
    "# sizes[-1] += len(train_data) - sizes.sum()\n",
    "\n",
    "\n",
    "\n",
    "# sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5000,  7500, 10000, 12500, 15000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_sizes = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "sizes = np.array([int(chunk_size*data_size) for chunk_size in chunk_sizes])\n",
    "\n",
    "\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "{0: 507, 1: 485, 2: 484, 3: 553, 4: 480, 5: 479, 6: 512, 7: 511, 8: 478, 9: 511}\n",
      "Chunk 2:\n",
      "{0: 761, 1: 760, 2: 781, 3: 733, 4: 738, 5: 754, 6: 757, 7: 729, 8: 760, 9: 727}\n",
      "Chunk 3:\n",
      "{0: 994, 1: 1013, 2: 1024, 3: 980, 4: 966, 5: 1034, 6: 1005, 7: 1048, 8: 984, 9: 952}\n",
      "Chunk 4:\n",
      "{0: 1258, 1: 1302, 2: 1242, 3: 1236, 4: 1202, 5: 1270, 6: 1213, 7: 1210, 8: 1321, 9: 1246}\n",
      "Chunk 5:\n",
      "{0: 1480, 1: 1478, 2: 1532, 3: 1517, 4: 1505, 5: 1494, 6: 1548, 7: 1487, 8: 1487, 9: 1472}\n"
     ]
    }
   ],
   "source": [
    "def split_data_unbalanced(data, labels, sizes):\n",
    "    total_samples = len(labels)\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # # Convert sizes from proportions to absolute values\n",
    "    # sizes = [int(size * total_samples) for size in sizes]\n",
    "\n",
    "    # Indices of each class in the data\n",
    "    indices = [np.where(labels == c)[0].tolist() for c in classes]\n",
    "\n",
    "    # Number of chunks\n",
    "    num_chunks = len(sizes)\n",
    "    \n",
    "    # List to store chunks of data and labels\n",
    "    chunks_data, chunks_labels = [], []\n",
    "\n",
    "    # Iterate over each chunk\n",
    "    for i, size in enumerate(sizes):\n",
    "        chunk_data, chunk_labels = [], []\n",
    "        \n",
    "        # Ensure each class is represented in each chunk\n",
    "        for c in range(num_classes):\n",
    "            if len(indices[c]) > 0:\n",
    "                idx = indices[c].pop(0)\n",
    "                chunk_data.append(data[idx])\n",
    "                chunk_labels.append(labels[idx])\n",
    "\n",
    "        # Fill the rest of the chunk with randomly selected samples from all classes\n",
    "        all_indices = [idx for class_indices in indices for idx in class_indices]\n",
    "        np.random.shuffle(all_indices)\n",
    "        for _ in range(size - num_classes):\n",
    "            if len(all_indices) > 0:\n",
    "                idx = all_indices.pop(0)\n",
    "                chunk_data.append(data[idx])\n",
    "                chunk_labels.append(labels[idx])\n",
    "\n",
    "        # Add the current chunk to the list of chunks\n",
    "        chunks_data.append(np.array(chunk_data))\n",
    "        chunks_labels.append(np.array(chunk_labels))\n",
    "\n",
    "    return chunks_data, chunks_labels\n",
    "\n",
    "# Testing the function\n",
    "chunks_data, chunks_labels = split_data_unbalanced(x_train, y_train, sizes)\n",
    "\n",
    "# Checking the distribution of labels in each chunk\n",
    "for i, chunk_labels in enumerate(chunks_labels):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    unique, counts = np.unique(chunk_labels, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "{0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Chunk 2:\n",
      "{0: 750, 1: 750, 2: 750, 3: 750, 4: 750, 5: 750, 6: 750, 7: 750, 8: 750, 9: 750}\n",
      "Chunk 3:\n",
      "{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n",
      "Chunk 4:\n",
      "{0: 1250, 1: 1250, 2: 1250, 3: 1250, 4: 1250, 5: 1250, 6: 1250, 7: 1250, 8: 1250, 9: 1250}\n",
      "Chunk 5:\n",
      "{0: 1500, 1: 1500, 2: 1500, 3: 1500, 4: 1500, 5: 1500, 6: 1500, 7: 1500, 8: 1500, 9: 1500}\n"
     ]
    }
   ],
   "source": [
    "def split_data_unbalanced(data, labels, sizes):\n",
    "    total_samples = len(labels)\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # # Convert sizes from proportions to absolute values\n",
    "    # sizes = [int(size * total_samples) for size in sizes]\n",
    "\n",
    "    # Indices of each class in the data\n",
    "    indices = [np.where(labels == c)[0].tolist() for c in classes]\n",
    "\n",
    "    # List to store chunks of data and labels\n",
    "    chunks_data, chunks_labels = [], []\n",
    "\n",
    "    # Iterate over each chunk\n",
    "    for i, size in enumerate(sizes):\n",
    "        chunk_data, chunk_labels = [], []\n",
    "\n",
    "        # Iterate over each class\n",
    "        for c in range(num_classes):\n",
    "            # Make sure we have enough samples for this class\n",
    "            num_samples = min(len(indices[c]), max(int(size / num_classes), 1))\n",
    "\n",
    "            # If we don't have enough samples, raise an error\n",
    "            if num_samples > len(indices[c]):\n",
    "                raise ValueError(f\"Not enough samples in class {c} for chunk {i}\")\n",
    "            \n",
    "            sample_indices = np.random.choice(indices[c], num_samples, replace=False)\n",
    "            indices[c] = list(set(indices[c]) - set(sample_indices))\n",
    "            \n",
    "            chunk_data.extend(data[sample_indices])\n",
    "            chunk_labels.extend(labels[sample_indices])\n",
    "\n",
    "        chunks_data.append(np.array(chunk_data))\n",
    "        chunks_labels.append(np.array(chunk_labels))\n",
    "\n",
    "    return chunks_data, chunks_labels\n",
    "\n",
    "# Testing the function\n",
    "chunks_data, chunks_labels = split_data_unbalanced(x_train, y_train, sizes)\n",
    "\n",
    "# Checking the distribution of labels in each chunk\n",
    "for i, chunk_labels in enumerate(chunks_labels):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    unique, counts = np.unique(chunk_labels, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n",
      "Chunk 2:\n",
      "{}\n",
      "Chunk 3:\n",
      "{}\n",
      "Chunk 4:\n",
      "{}\n",
      "Chunk 5:\n",
      "{}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_train\n",
    "labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]),\n",
       " 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_samples = len(labels)\n",
    "classes, counts = np.unique(labels, return_counts=True)\n",
    "num_classes = len(classes)\n",
    "\n",
    "total_samples, classes, counts, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5000,  7500, 10000, 12500, 15000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks_data, chunks_labels\n\u001b[1;32m     46\u001b[0m \u001b[39m# Testing the function\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m chunks_data, chunks_labels \u001b[39m=\u001b[39m split_data_unbalanced(x_train, y_train, sizes)\n\u001b[1;32m     49\u001b[0m \u001b[39m# Checking the distribution of labels in each chunk\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m i, chunk_labels \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks_labels):\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36msplit_data_unbalanced\u001b[0;34m(data, labels, sizes)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mwhile\u001b[39;00m size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_classes):\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mif\u001b[39;00m size \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[1;32m     37\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(indices[c]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data_unbalanced(data, labels, sizes):\n",
    "    # total_samples = len(labels)\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Convert sizes from proportions to absolute values\n",
    "    # sizes = [int(size * total_samples) for size in sizes]\n",
    "\n",
    "    # Indices of each class in the data\n",
    "    indices = [np.where(labels == c)[0].tolist() for c in classes]\n",
    "\n",
    "    # List to store chunks of data and labels\n",
    "    chunks_data, chunks_labels = [], []\n",
    "\n",
    "    # Minimum number of samples of each class in each chunk\n",
    "    min_samples = [int(0.05 * size) for size in sizes]\n",
    "\n",
    "    # Initial distribution of samples to each chunk\n",
    "    for c in range(num_classes):\n",
    "        np.random.shuffle(indices[c])\n",
    "        start = 0\n",
    "        for i, size in enumerate(sizes):\n",
    "            end = start + min_samples[i]\n",
    "            chunks_data.append(data[indices[c][start:end]])\n",
    "            chunks_labels.append(labels[indices[c][start:end]])\n",
    "            start = end\n",
    "        indices[c] = indices[c][start:]  # Remaining samples of this class\n",
    "\n",
    "    # Distribute the remaining samples\n",
    "    remaining_sizes = [size - len(chunk) for size, chunk in zip(sizes, chunks_data)]\n",
    "    for i, size in enumerate(remaining_sizes):\n",
    "        while size > 0:\n",
    "            for c in range(num_classes):\n",
    "                if size <= 0:\n",
    "                    break\n",
    "                if len(indices[c]) > 0:\n",
    "                    chunks_data[i] = np.append(chunks_data[i], data[indices[c][0]])\n",
    "                    chunks_labels[i] = np.append(chunks_labels[i], labels[indices[c][0]])\n",
    "                    indices[c] = indices[c][1:]\n",
    "                    size -= 1\n",
    "\n",
    "    return chunks_data, chunks_labels\n",
    "\n",
    "# Testing the function\n",
    "chunks_data, chunks_labels = split_data_unbalanced(x_train, y_train, sizes)\n",
    "\n",
    "# Checking the distribution of labels in each chunk\n",
    "for i, chunk_labels in enumerate(chunks_labels):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    unique, counts = np.unique(chunk_labels, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of each class in the data\n",
    "indices = [np.where(labels == c)[0].tolist() for c in classes]\n",
    "\n",
    "# List to store chunks of data and labels\n",
    "chunks_data, chunks_labels = [], []\n",
    "\n",
    "# Minimum number of samples of each class in each chunk\n",
    "min_samples = [int(0.05 * size) for size in sizes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks_data, chunks_labels\n\u001b[1;32m     47\u001b[0m \u001b[39m# Testing the function\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m chunks_data, chunks_labels \u001b[39m=\u001b[39m split_data_unbalanced(x_train, y_train, sizes)\n\u001b[1;32m     50\u001b[0m \u001b[39m# Checking the distribution of labels in each chunk\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m i, chunk_labels \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks_labels):\n",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m, in \u001b[0;36msplit_data_unbalanced\u001b[0;34m(data, labels, sizes)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(indices[c]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     37\u001b[0m     sample_index \u001b[39m=\u001b[39m indices[c]\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     chunks_data[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(chunks_data[i], data[sample_index])\n\u001b[1;32m     39\u001b[0m     chunks_labels[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(chunks_labels[i], labels[sample_index])\n\u001b[1;32m     40\u001b[0m     remaining_size \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/numpy/lib/function_base.py:5444\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[1;32m   5443\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 5444\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data_unbalanced(data, labels, sizes):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "    total_samples = len(labels)\n",
    "\n",
    "    # Calculate minimum samples per class per chunk based on the 5% rule\n",
    "    min_samples_per_chunk = [int(0.05 * size) for size in sizes]\n",
    "\n",
    "    # Indices of each class in the data\n",
    "    indices = [np.where(labels == c)[0] for c in classes]\n",
    "\n",
    "    # List to store chunks of data and labels\n",
    "    chunks_data, chunks_labels = [], []\n",
    "\n",
    "    # Distribute minimum number of samples of each class to each chunk\n",
    "    for i, size in enumerate(sizes):\n",
    "        chunk_data, chunk_labels = [], []\n",
    "        for c in range(num_classes):\n",
    "            if len(indices[c]) < min_samples_per_chunk[i]:\n",
    "                raise ValueError(f\"Not enough samples in class {c} for chunk {i}\")\n",
    "            sample_indices = np.random.choice(indices[c], min_samples_per_chunk[i], replace=False)\n",
    "            indices[c] = list(set(indices[c]) - set(sample_indices))\n",
    "            chunk_data.extend(data[sample_indices])\n",
    "            chunk_labels.extend(labels[sample_indices])\n",
    "        chunks_data.append(np.array(chunk_data))\n",
    "        chunks_labels.append(np.array(chunk_labels))\n",
    "\n",
    "    # Distribute remaining samples to chunks\n",
    "    remaining_samples = total_samples - sum([len(chunk) for chunk in chunks_data])\n",
    "    for i, size in enumerate(sizes):\n",
    "        remaining_size = size - len(chunks_data[i])\n",
    "        while remaining_size > 0 and remaining_samples > 0:\n",
    "            for c in range(num_classes):\n",
    "                if len(indices[c]) > 0:\n",
    "                    sample_index = indices[c].pop(0)\n",
    "                    chunks_data[i] = np.append(chunks_data[i], data[sample_index])\n",
    "                    chunks_labels[i] = np.append(chunks_labels[i], labels[sample_index])\n",
    "                    remaining_size -= 1\n",
    "                    remaining_samples -= 1\n",
    "                if remaining_size <= 0:\n",
    "                    break\n",
    "\n",
    "    return chunks_data, chunks_labels\n",
    "\n",
    "# Testing the function\n",
    "chunks_data, chunks_labels = split_data_unbalanced(x_train, y_train, sizes)\n",
    "\n",
    "# Checking the distribution of labels in each chunk\n",
    "for i, chunk_labels in enumerate(chunks_labels):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    unique, counts = np.unique(chunk_labels, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "{0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Chunk 2:\n",
      "{0: 750, 1: 750, 2: 750, 3: 750, 4: 750, 5: 750, 6: 750, 7: 750, 8: 750, 9: 750}\n",
      "Chunk 3:\n",
      "{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n",
      "Chunk 4:\n",
      "{0: 1250, 1: 1250, 2: 1250, 3: 1250, 4: 1250, 5: 1250, 6: 1250, 7: 1250, 8: 1250, 9: 1250}\n",
      "Chunk 5:\n",
      "{0: 1500, 1: 1500, 2: 1500, 3: 1500, 4: 1500, 5: 1500, 6: 1500, 7: 1500, 8: 1500, 9: 1500}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data_unbalanced(data, labels, sizes):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "    total_samples = len(labels)\n",
    "\n",
    "    # Calculate minimum samples per class per chunk based on the 5% rule\n",
    "    min_samples_per_chunk = [int(0.05 * size) for size in sizes]\n",
    "\n",
    "    # Indices of each class in the data\n",
    "    indices = [np.where(labels == c)[0] for c in classes]\n",
    "\n",
    "    # List to store chunks of data and labels\n",
    "    chunks_data, chunks_labels = [], []\n",
    "\n",
    "    # Distribute minimum number of samples of each class to each chunk\n",
    "    for i, size in enumerate(sizes):\n",
    "        chunk_data, chunk_labels = [], []\n",
    "        for c in range(num_classes):\n",
    "            if len(indices[c]) < min_samples_per_chunk[i]:\n",
    "                raise ValueError(f\"Not enough samples in class {c} for chunk {i}\")\n",
    "            sample_indices = np.random.choice(indices[c], min_samples_per_chunk[i], replace=False)\n",
    "            indices[c] = list(set(indices[c]) - set(sample_indices))\n",
    "            chunk_data.extend(data[sample_indices])\n",
    "            chunk_labels.extend(labels[sample_indices])\n",
    "        chunks_data.append(np.array(chunk_data))\n",
    "        chunks_labels.append(np.array(chunk_labels))\n",
    "\n",
    "    # Distribute remaining samples to chunks\n",
    "    remaining_samples = total_samples - sum([len(chunk) for chunk in chunks_data])\n",
    "    for i, size in enumerate(sizes):\n",
    "        remaining_size = size - len(chunks_data[i])\n",
    "        while remaining_size > 0 and remaining_samples > 0:\n",
    "            for c in range(num_classes):\n",
    "                if len(indices[c]) > 0:\n",
    "                    sample_index = indices[c].pop(0)\n",
    "                    chunks_data[i] = np.append(chunks_data[i], data[sample_index])\n",
    "                    chunks_labels[i] = np.append(chunks_labels[i], labels[sample_index])\n",
    "                    remaining_size -= 1\n",
    "                    remaining_samples -= 1\n",
    "                if remaining_size <= 0:\n",
    "                    break\n",
    "\n",
    "    return chunks_data, chunks_labels\n",
    "\n",
    "# Testing the function\n",
    "chunks_data, chunks_labels = split_data_unbalanced(x_train, y_train, sizes)\n",
    "\n",
    "# Checking the distribution of labels in each chunk\n",
    "for i, chunk_labels in enumerate(chunks_labels):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    unique, counts = np.unique(chunk_labels, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices at which to split the training data\n",
    "split_indices = np.cumsum(sizes)[:-1]\n",
    "\n",
    "# Split the training data into chunks of different sizes\n",
    "chunks = np.split(train_data, split_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"5_chunks_2\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noniid_folder = \"noniid\"\n",
    "noniid_folder = os.path.join(folder, noniid_folder)\n",
    "if not os.path.exists(noniid_folder):\n",
    "    os.makedirs(noniid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Now, make the distribution unbalanced for each chunk before saving\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i, chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks):\n\u001b[0;32m----> 5\u001b[0m     unbalanced_chunk \u001b[39m=\u001b[39m make_distribution_unbalanced(chunk, seed\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m      7\u001b[0m     info \u001b[39m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m     info[\u001b[39m'\u001b[39m\u001b[39mchunk\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mmake_distribution_unbalanced\u001b[0;34m(chunk, min_prop, seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m label_samples \u001b[39m=\u001b[39m chunk[labels \u001b[39m==\u001b[39m label]\n\u001b[1;32m     33\u001b[0m \u001b[39m# Randomly select samples of this label\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m selected_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mlen\u001b[39;49m(label_samples), size\u001b[39m=\u001b[39;49mnum_samples, replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Add selected samples to the new chunk\u001b[39;00m\n\u001b[1;32m     37\u001b[0m unbalanced_chunk \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([unbalanced_chunk, label_samples[selected_samples]])\n",
      "File \u001b[0;32mmtrand.pyx:965\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store size and label distribution of each chunk, and save each chunk as a pickle file\n",
    "unbalanced_chunk_info = []\n",
    "# Now, make the distribution unbalanced for each chunk before saving\n",
    "for i, unbalanced_chunk in enumerate(chunks):\n",
    "\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    label_distribution = get_label_distribution(unbalanced_chunk)\n",
    "    chunk_size =len(unbalanced_chunk)\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    unbalanced_chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{noniid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(unbalanced_chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "noniid_df = pd.DataFrame(unbalanced_chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(noniid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "noniid_df.to_csv(f\"{noniid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>size</th>\n",
       "      <th>label_distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8695</td>\n",
       "      <td>{0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4347</td>\n",
       "      <td>{0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>13043</td>\n",
       "      <td>{0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2173</td>\n",
       "      <td>{0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10869</td>\n",
       "      <td>{0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2181</td>\n",
       "      <td>{0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk   size                                 label_distribution\n",
       "0      1   2173  {0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...\n",
       "1      2   2173  {0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...\n",
       "2      3   8695  {0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...\n",
       "3      4   2173  {0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...\n",
       "4      5   4347  {0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...\n",
       "5      6   2173  {0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...\n",
       "6      7  13043  {0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...\n",
       "7      8   2173  {0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...\n",
       "8      9  10869  {0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...\n",
       "9     10   2181  {0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {0: 211, 1: 226, 2: 215, 3: 207, 4: 232, 5: 21...\n",
       "1    {0: 209, 1: 212, 2: 228, 3: 230, 4: 223, 5: 24...\n",
       "2    {0: 867, 1: 915, 2: 852, 3: 840, 4: 858, 5: 87...\n",
       "3    {0: 203, 1: 201, 2: 211, 3: 242, 4: 230, 5: 20...\n",
       "4    {0: 447, 1: 425, 2: 397, 3: 417, 4: 462, 5: 45...\n",
       "5    {0: 216, 1: 210, 2: 226, 3: 224, 4: 209, 5: 19...\n",
       "6    {0: 1231, 1: 1256, 2: 1333, 3: 1319, 4: 1328, ...\n",
       "7    {0: 228, 1: 240, 2: 221, 3: 188, 4: 210, 5: 19...\n",
       "8    {0: 1166, 1: 1086, 2: 1085, 3: 1120, 4: 1056, ...\n",
       "9    {0: 222, 1: 229, 2: 232, 3: 213, 4: 192, 5: 24...\n",
       "Name: label_distribution, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data2/chunk_1.pkl\n",
      "Dataset saved to data2/chunk_2.pkl\n",
      "Dataset saved to data2/chunk_3.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "train_size=4463 should be either positive and smaller than the number of samples 3765 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m diff \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     label, images \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(data_dict\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 61\u001b[0m     extra_samples, data_dict[label] \u001b[39m=\u001b[39m train_test_split(images, train_size\u001b[39m=\u001b[39;49mdiff, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m     chunk_images\u001b[39m.\u001b[39mextend(extra_samples)\n\u001b[1;32m     63\u001b[0m     chunk_labels\u001b[39m.\u001b[39mextend([label]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(extra_samples))\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2230\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtest_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2220\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2221\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_size, n_samples)\n\u001b[1;32m   2222\u001b[0m     )\n\u001b[1;32m   2224\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2225\u001b[0m     train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2226\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n_samples \u001b[39mor\u001b[39;00m train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2227\u001b[0m     \u001b[39mor\u001b[39;00m train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2228\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m   2229\u001b[0m ):\n\u001b[0;32m-> 2230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2232\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2233\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size, n_samples)\n\u001b[1;32m   2234\u001b[0m     )\n\u001b[1;32m   2236\u001b[0m \u001b[39mif\u001b[39;00m train_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m train_size_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2237\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid value for train_size: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size))\n",
      "\u001b[0;31mValueError\u001b[0m: train_size=4463 should be either positive and smaller than the number of samples 3765 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the proportion of each chunk\n",
    "chunk_sizes = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Define the proportion of each category within each chunk\n",
    "category_dist_in_chunks = [\n",
    "    [0.3, 0.2, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05, 0.025, 0.025],\n",
    "    [0.1, 0.2, 0.1, 0.15, 0.1, 0.1, 0.1, 0.05, 0.05, 0.1],\n",
    "    [0.1, 0.05, 0.15, 0.1, 0.2, 0.1, 0.1, 0.05, 0.1, 0.05],\n",
    "    [0.05, 0.1, 0.05, 0.2, 0.1, 0.15, 0.1, 0.1, 0.05, 0.1],\n",
    "    [0.1, 0.05, 0.1, 0.05, 0.15, 0.1, 0.2, 0.1, 0.1, 0.05]\n",
    "]\n",
    "\n",
    "# Normalize the proportions within each chunk\n",
    "category_dist_in_chunks = [np.array(dist)/sum(dist) for dist in category_dist_in_chunks]\n",
    "\n",
    "def save_to_pkl(images, labels, path):\n",
    "    \"\"\"Save the images and labels to a Pickle file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        # Combine images and labels into one list\n",
    "        data = [np.append(image.flatten(), label) for image, label in zip(images, labels)]\n",
    "        pickle.dump(data, f)\n",
    "    print(f'Dataset saved to {path}')\n",
    "\n",
    "# Load CIFAR10 data\n",
    "(train_images, train_labels), _ = datasets.cifar10.load_data()\n",
    "\n",
    "# Create a dictionary where the keys are the labels and the values are the images\n",
    "data_dict = defaultdict(list)\n",
    "for image, label in zip(train_images, train_labels):\n",
    "    data_dict[label[0]].append(image)\n",
    "\n",
    "chunk_info = []\n",
    "# Create chunks\n",
    "for i in range(5):\n",
    "    chunk_images = []\n",
    "    chunk_labels = []\n",
    "    total_samples = sum([len(images) for images in data_dict.values()])\n",
    "    chunk_size = int(total_samples * chunk_sizes[i])\n",
    "    \n",
    "    for label, images in data_dict.items():\n",
    "        n_samples = int(len(images) * category_dist_in_chunks[i][label])\n",
    "        # Split data for this category\n",
    "        chunk, data_dict[label] = train_test_split(images, train_size=n_samples, shuffle=True)\n",
    "        chunk_images.extend(chunk)\n",
    "        chunk_labels.extend([label]*len(chunk))\n",
    "    \n",
    "    # Correct for rounding errors\n",
    "    diff = chunk_size - len(chunk_images)\n",
    "    if diff != 0:\n",
    "        label, images = max(data_dict.items(), key=lambda x: len(x[1]))\n",
    "        extra_samples, data_dict[label] = train_test_split(images, train_size=diff, shuffle=True)\n",
    "        chunk_images.extend(extra_samples)\n",
    "        chunk_labels.extend([label]*len(extra_samples))\n",
    "    \n",
    "    # Save the chunk\n",
    "    save_to_pkl(chunk_images, chunk_labels, f'data2/chunk_{i+1}.pkl')\n",
    "\n",
    "    # Collect the chunk information\n",
    "    chunk_info.append({\n",
    "        'chunk': i+1,\n",
    "        'size': len(chunk_images),\n",
    "        'label_distribution': dict(Counter(chunk_labels))\n",
    "    })\n",
    "\n",
    "# Save the chunk information into a CSV file\n",
    "df = pd.DataFrame(chunk_info)\n",
    "df.to_csv('data2/chunk_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vtn_ubuntu/ttu/spring23/working_project/AsynFl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "root = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(root)\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asynfed.client.local_model_upload_info import LocalModelUpdateInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_info = LocalModelUpdateInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'local_weight_path': './123.pkl',\n",
       " 'remote_weight_path': 'cloud/123.pkl',\n",
       " 'filename': '123.pkl',\n",
       " 'global_version_used': 10,\n",
       " 'new_update': True,\n",
       " 'train_acc': 0.2,\n",
       " 'train_loss': 0.6}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_info.update(remote_weight_path= \"cloud/123.pkl\", local_weight_path='./123.pkl', filename= '123.pkl',\n",
    "                  global_version_used= 10, train_acc= 0.20, train_loss= 0.6)\n",
    "\n",
    "first_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'local_weight_path': './123.pkl',\n",
       " 'remote_weight_path': 'cloud/123.pkl',\n",
       " 'filename': '123.pkl',\n",
       " 'global_version_used': 10,\n",
       " 'new_update': True,\n",
       " 'train_acc': 0.2,\n",
       " 'train_loss': 0.6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_info_clone = LocalModelUpdateInfo(**first_info.to_dict())\n",
    "\n",
    "first_info_clone.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'local_weight_path': './123.pkl',\n",
       "  'remote_weight_path': 'cloud/123.pkl',\n",
       "  'filename': '123.pkl',\n",
       "  'global_version_used': 10,\n",
       "  'new_update': True,\n",
       "  'train_acc': 0.2,\n",
       "  'train_loss': 0.6},\n",
       " {'local_weight_path': './124.pkl',\n",
       "  'remote_weight_path': 'cloud/124.pkl',\n",
       "  'filename': '124.pkl',\n",
       "  'global_version_used': 10,\n",
       "  'new_update': True,\n",
       "  'train_acc': 0.1,\n",
       "  'train_loss': 0.5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_info.update(remote_weight_path= \"cloud/124.pkl\", local_weight_path='./124.pkl', filename= '124.pkl',\n",
    "                  global_version_used= 10, train_acc= 0.10, train_loss= 0.5)\n",
    "\n",
    "first_info_clone.to_dict(), first_info.to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
