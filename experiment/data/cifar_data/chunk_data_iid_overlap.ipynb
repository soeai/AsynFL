{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 10:10:08.748088: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-27 10:10:08.782126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 10:10:09.277009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data separately\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train from 4D to 2D array (number of samples, width*height*channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Reshape y_train to 1D array\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Combine training data and labels into a single numpy array for easier manipulation\n",
    "train_data = np.column_stack((x_train, y_train))\n",
    "\n",
    "# Randomly shuffle the training data\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_with_overlap(data, num_chunks=10, min_chunk_size=0.2, max_chunk_size=0.4):\n",
    "    num_samples = len(data)\n",
    "    num_classes = 10  # cifar10 has 10 classes\n",
    "\n",
    "    # Select random chunk sizes that are multiples of the number of classes\n",
    "    chunk_sizes = [np.random.choice(np.arange(int(min_chunk_size * num_samples), \n",
    "                                              int(max_chunk_size * num_samples)+1, \n",
    "                                              num_classes)) for _ in range(num_chunks)]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Split data by classes\n",
    "    data_by_class = {i: [] for i in range(num_classes)}\n",
    "    for row in data:\n",
    "        data_by_class[int(row[-1])].append(row)\n",
    "\n",
    "    # Ensure each sample is included at least once\n",
    "    for i in range(num_classes):\n",
    "        np.random.shuffle(data_by_class[i])\n",
    "\n",
    "    # Copy of the original class samples before popping\n",
    "    data_by_class_original = {i: list(data) for i, data in data_by_class.items()}\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        chunk = []\n",
    "        for _ in range(size // num_classes):\n",
    "            for c in range(num_classes):\n",
    "                if data_by_class[c]:  # if there are samples left\n",
    "                    chunk.append(data_by_class[c].pop())\n",
    "                else:  # if all samples of this class have been used, start reusing\n",
    "                    chunk.append(data_by_class_original[c][np.random.choice(len(data_by_class_original[c]))])\n",
    "        chunks.append(np.array(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Split train_data into 10 chunks\n",
    "chunks = get_chunks_with_overlap(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1: 15380 vs 15380 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 2: 17450 vs 17450 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 3: 13990 vs 13990 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 4: 10570 vs 9615 --> additional sample = [106, 88, 97, 90, 101, 94, 99, 107, 88, 85]\n",
      "additional sample list: [106  88  97  90 101  94  99 107  88  85]\n",
      "chunk 5: 18220 vs 15208 --> additional sample = [309, 292, 303, 304, 302, 297, 307, 307, 282, 309]\n",
      "additional sample list: [309 292 303 304 302 297 307 307 282 309]\n",
      "chunk 6: 17190 vs 14469 --> additional sample = [281, 275, 259, 270, 240, 263, 282, 272, 295, 284]\n",
      "additional sample list: [281 275 259 270 240 263 282 272 295 284]\n",
      "chunk 7: 19530 vs 16079 --> additional sample = [345, 332, 340, 351, 343, 330, 362, 336, 362, 350]\n",
      "additional sample list: [345 332 340 351 343 330 362 336 362 350]\n",
      "chunk 8: 19380 vs 16087 --> additional sample = [331, 348, 329, 326, 316, 334, 351, 317, 312, 329]\n",
      "additional sample list: [331 348 329 326 316 334 351 317 312 329]\n",
      "chunk 9: 13620 vs 11868 --> additional sample = [173, 184, 177, 159, 185, 198, 156, 194, 169, 157]\n",
      "additional sample list: [173 184 177 159 185 198 156 194 169 157]\n",
      "chunk 10: 10080 vs 9199 --> additional sample = [88, 94, 92, 88, 94, 82, 86, 82, 89, 86]\n",
      "additional sample list: [88 94 92 88 94 82 86 82 89 86]\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_sample_from_chunk(chunks):\n",
    "    unique_chunks = []\n",
    "    additional_samples = []\n",
    "    num_classes = 10  # cifar10 has 10 classes\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        unique_samples = np.unique(chunk, axis=0)\n",
    "        unique_chunks.append(unique_samples)\n",
    "\n",
    "        # Count the number of samples from each category in the original and unique chunks\n",
    "        original_counts = np.bincount(chunk[:,-1], minlength=num_classes)\n",
    "        unique_counts = np.bincount(unique_samples[:,-1], minlength=num_classes)\n",
    "\n",
    "        # Compute the additional_samples for each category as the difference between the original and unique counts\n",
    "        additional_sample = original_counts - unique_counts\n",
    "        additional_samples.append(additional_sample.tolist())\n",
    "        print(f\"chunk {i+1}: {len(chunk)} vs {len(unique_samples)} --> additional sample = {additional_sample.tolist()}\")\n",
    "        print(f\"additional sample list: {additional_sample}\")\n",
    "        \n",
    "    return zip(unique_chunks, additional_samples)\n",
    "\n",
    "requirement = remove_duplicate_sample_from_chunk(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_chunks(requirement, chunks, subset_factor=2):\n",
    "    num_classes = 10  # cifar10 has 10 classes\n",
    "\n",
    "    for i, (unique_chunk, additional_samples) in enumerate(requirement):\n",
    "        # Create a set for faster membership tests\n",
    "        unique_set = set(map(tuple, unique_chunk))\n",
    "\n",
    "        # Convert the list of additional samples into a numpy array for easier manipulation\n",
    "        additional_samples = np.array(additional_samples)\n",
    "\n",
    "        while np.any(additional_samples > 0):\n",
    "            # Randomly select a chunk different from the current one\n",
    "            while True:\n",
    "                random_chunk_index = np.random.choice(len(chunks))\n",
    "                if random_chunk_index != i:\n",
    "                    break\n",
    "\n",
    "            random_chunk = chunks[random_chunk_index]\n",
    "            np.random.shuffle(random_chunk)\n",
    "\n",
    "            for sample in random_chunk:\n",
    "                # If sample is not already in unique_set and there are still missing samples for its category\n",
    "                if tuple(sample) not in unique_set and additional_samples[int(sample[-1])] > 0:\n",
    "                    # Add sample to unique_chunk\n",
    "                    unique_chunk = np.vstack([unique_chunk, sample])\n",
    "                    # Add sample to unique_set\n",
    "                    unique_set.add(tuple(sample))\n",
    "                    # Decrease the count of missing samples for this category\n",
    "                    additional_samples[int(sample[-1])] -= 1\n",
    "\n",
    "            # Replace the old chunk with the new one in the chunks list\n",
    "            chunks[i] = unique_chunk\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Use the fill_chunks function\n",
    "filled_chunks = fill_chunks(requirement, chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1: 15380 vs 15380 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 2: 17450 vs 17450 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 3: 13990 vs 13990 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 4: 10570 vs 10570 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 5: 18220 vs 18220 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 6: 17190 vs 17190 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 7: 19530 vs 19530 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 8: 19380 vs 19380 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 9: 13620 vs 13620 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n",
      "chunk 10: 10080 vs 10080 --> additional sample = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "additional sample list: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "result = remove_duplicate_sample_from_chunk(filled_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15380, 3073)\n",
      "(17450, 3073)\n",
      "(13990, 3073)\n",
      "(10570, 3073)\n",
      "(18220, 3073)\n",
      "(17190, 3073)\n",
      "(19530, 3073)\n",
      "(19380, 3073)\n",
      "(13620, 3073)\n",
      "(10080, 3073)\n"
     ]
    }
   ],
   "source": [
    "# Unzip the result to get chunks and additional_samples separately\n",
    "chunks, _ = zip(*result)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get label distribution in a chunk\n",
    "def get_label_distribution(chunk):\n",
    "    # The label is in the last column\n",
    "    labels = chunk[:, -1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique_labels, counts))\n",
    "\n",
    "\n",
    "# Function to get label proportions in a chunk\n",
    "def get_label_proportions(label_distribution, chunk_size):\n",
    "    proportions = {}\n",
    "    for label, count in label_distribution.items():\n",
    "        proportions[label] = count / chunk_size\n",
    "    return proportions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"10_chunks_overlap\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_folder = \"iid\"\n",
    "iid_folder = os.path.join(folder, iid_folder)\n",
    "if not os.path.exists(iid_folder):\n",
    "    os.makedirs(iid_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid distribution\n",
    "\n",
    "chunk_info = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "\n",
    "    label_distribution = get_label_distribution(chunk)\n",
    "    chunk_size =len(chunk)\n",
    "    # save info\n",
    "    info = {}\n",
    "    info['chunk'] = i+1\n",
    "    info['size'] = chunk_size\n",
    "    info['label_distribution'] = label_distribution\n",
    "    proportions = get_label_proportions(label_distribution, chunk_size)\n",
    "    info['label_proportions'] = proportions\n",
    "\n",
    "    chunk_info.append(info)\n",
    "\n",
    "    # Save unbalanced chunk as a pickle file\n",
    "    with open(f'{iid_folder}/chunk_{i+1}.pickle', 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunk   size                                 label_distribution  \\\n",
      "0      1  15380  {0: 1538, 1: 1538, 2: 1538, 3: 1538, 4: 1538, ...   \n",
      "1      2  17450  {0: 1745, 1: 1745, 2: 1745, 3: 1745, 4: 1745, ...   \n",
      "2      3  13990  {0: 1399, 1: 1399, 2: 1399, 3: 1399, 4: 1399, ...   \n",
      "3      4  10570  {0: 1057, 1: 1057, 2: 1057, 3: 1057, 4: 1057, ...   \n",
      "4      5  18220  {0: 1822, 1: 1822, 2: 1822, 3: 1822, 4: 1822, ...   \n",
      "5      6  17190  {0: 1719, 1: 1719, 2: 1719, 3: 1719, 4: 1719, ...   \n",
      "6      7  19530  {0: 1953, 1: 1953, 2: 1953, 3: 1953, 4: 1953, ...   \n",
      "7      8  19380  {0: 1938, 1: 1938, 2: 1938, 3: 1938, 4: 1938, ...   \n",
      "8      9  13620  {0: 1362, 1: 1362, 2: 1362, 3: 1362, 4: 1362, ...   \n",
      "9     10  10080  {0: 1008, 1: 1008, 2: 1008, 3: 1008, 4: 1008, ...   \n",
      "\n",
      "                                   label_proportions  \n",
      "0  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "1  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "2  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "3  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "4  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "5  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "6  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "7  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "8  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n",
      "9  {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0....  \n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to DataFrame for better visualization\n",
    "df = pd.DataFrame(chunk_info)\n",
    "\n",
    "# print dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv(f\"{iid_folder}/chunks_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
