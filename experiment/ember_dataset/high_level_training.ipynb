{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 09:32:39.444437: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-04 09:32:39.478759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 09:32:40.079572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bin/bash: /home/student02/anaconda3/envs/asynfed/lib/libtinfo.so.6: no version information available (required by /bin/bash)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!export TF_GPU_ALLOCATOR=cuda_malloc_async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EmberModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim= 257, maxlen= 2381, embedding_size=8, \n",
    "                 batch_size=64, num_epochs = 200, data_size = 600000):\n",
    "        super().__init__()\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.data_size = data_size\n",
    "\n",
    "        self.epoch: int = 0 \n",
    "        self.create_model(input_dim= input_dim, maxlen= maxlen, \n",
    "                          embedding_size= embedding_size, batch_size= batch_size)\n",
    "        self.compile()\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "    def create_model(self, input_dim= 257, maxlen= 2381, embedding_size=8, batch_size=128):\n",
    "        self.input_dim = input_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.inp = tf.keras.layers.Input(shape=(self.maxlen,))\n",
    "        self.emb = tf.keras.layers.Embedding(self.input_dim, self.embedding_size)(self.inp)\n",
    "        self.filt = tf.keras.layers.Conv1D(filters=128, kernel_size=500, strides=500, use_bias=True, activation='relu', padding='valid')(self.emb)\n",
    "        self.attn = tf.keras.layers.Conv1D(filters=128, kernel_size=500, strides=500, use_bias=True, activation='sigmoid', padding='valid')(self.emb)\n",
    "        self.gated = tf.keras.layers.Multiply()([self.filt, self.attn])\n",
    "        self.feat = tf.keras.layers.GlobalMaxPooling1D()(self.gated)\n",
    "        self.dense = tf.keras.layers.Dense(128, activation='relu')(self.feat)\n",
    "        self.outp = tf.keras.layers.Dense(1, activation='sigmoid')(self.dense)\n",
    "\n",
    "        self.model = tf.keras.models.Model(self.inp, self.outp)\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "    def compile(self):\n",
    "        learning_rate_fn=tf.keras.experimental.CosineDecay(initial_learning_rate= 0.1, decay_steps=self.num_epochs * self.data_size / self.batch_size)\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "        self.model.compile(optimizer= self.optimizer,\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "        \n",
    "    def train(self, training_data= [], validation_data= []):\n",
    "        self.epoch += 1\n",
    "        X, y = training_data\n",
    "        X_val, y_val = validation_data\n",
    "        print(f\"In epoch {self.epoch}, learning rate is: {self.optimizer.lr.numpy()}\")\n",
    "        self.model.fit(X, y, validation_data=(X_val, y_val), batch_size= self.batch_size)\n",
    "        # self.model.fit(X, y, epochs=self.num_epochs, validation_data=(X_val, y_val), batch_size= self.batch_size)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (600000, 2381) -- y shape: (600000,)\n",
      "X val shape: (200000, 2381) -- y val shape: (200000,)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "data_folder = \"/home/student02/thaile/working_with_ember_dataset/data\"\n",
    "for i in range(10):\n",
    "    X_chunk, y_chunk = load_data(f'{data_folder}/chunk_{i}.pickle')\n",
    "    X.append(X_chunk)\n",
    "    y.append(y_chunk)\n",
    "\n",
    "X_val, y_val = load_data(f'{data_folder}/test_set.pickle')\n",
    "\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.concatenate(y, axis=0)\n",
    "print('X shape:', X.shape, '-- y shape:', y.shape)\n",
    "print('X val shape:', X_val.shape, '-- y val shape:', y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2381)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 2381, 8)      2056        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4, 128)       512128      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4, 128)       512128      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 128)       0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 128)         0           ['multiply[0][0]']               \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,042,953\n",
      "Trainable params: 1,042,953\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 09:32:47.486823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22301 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:19:00.0, compute capability: 8.6\n",
      "2023-08-04 09:32:47.487352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22301 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:8d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "model = EmberModel(num_epochs= num_epochs, data_size= len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 1, learning rate is: 0.10000000149011612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 09:32:59.049170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-08-04 09:32:59.617242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-04 09:32:59.619008: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f7ba1131f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-04 09:32:59.619020: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-08-04 09:32:59.619024: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-08-04 09:32:59.688367: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 [==============================] - 105s 11ms/step - loss: nan - accuracy: 0.5003 - val_loss: nan - val_accuracy: 0.5000\n",
      "In epoch 2, learning rate is: 0.09999383985996246\n",
      "9375/9375 [==============================] - 70s 7ms/step - loss: nan - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5000\n",
      "In epoch 3, learning rate is: 0.0999753326177597\n",
      "9375/9375 [==============================] - 68s 7ms/step - loss: nan - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5000\n",
      "In epoch 4, learning rate is: 0.09994449466466904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 09:37:21.275603: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.32GiB (rounded to 5714400000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-08-04 09:37:21.275654: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-08-04 09:37:21.275697: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 49, Chunks in use: 46. 12.2KiB allocated for chunks. 11.5KiB in use in bin. 1.2KiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275712: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 9, Chunks in use: 8. 4.5KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275723: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275735: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275746: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275759: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 18.5KiB allocated for chunks. 18.5KiB in use in bin. 16.1KiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275769: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275779: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275792: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 2, Chunks in use: 2. 191.0KiB allocated for chunks. 191.0KiB in use in bin. 128.0KiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275802: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275811: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275823: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 3, Chunks in use: 3. 2.29MiB allocated for chunks. 2.29MiB in use in bin. 2.29MiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275834: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 4. 7.81MiB allocated for chunks. 7.81MiB in use in bin. 7.81MiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275845: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 3, Chunks in use: 3. 8.30MiB allocated for chunks. 8.30MiB in use in bin. 6.87MiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275855: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275865: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275875: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275884: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275894: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275904: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275920: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 7, Chunks in use: 6. 21.76GiB allocated for chunks. 21.29GiB in use in bin. 21.29GiB client-requested in use in bin.\n",
      "2023-08-04 09:37:21.275932: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 5.32GiB was 256.00MiB, Chunk State: \n",
      "2023-08-04 09:37:21.275953: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 484.11MiB | Requested Size: 4B | in_use: 0 | bin_num: 20, prev:   Size: 781.2KiB | Requested Size: 781.2KiB | in_use: 1 | bin_num: -1\n",
      "2023-08-04 09:37:21.275963: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 23384752128\n",
      "2023-08-04 09:37:21.275975: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000000 of size 256 next 1\n",
      "2023-08-04 09:37:21.275985: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000100 of size 1280 next 2\n",
      "2023-08-04 09:37:21.275993: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000600 of size 256 next 3\n",
      "2023-08-04 09:37:21.276001: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000700 of size 256 next 4\n",
      "2023-08-04 09:37:21.276009: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000800 of size 256 next 6\n",
      "2023-08-04 09:37:21.276017: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000900 of size 256 next 7\n",
      "2023-08-04 09:37:21.276025: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000a00 of size 256 next 11\n",
      "2023-08-04 09:37:21.276034: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000b00 of size 512 next 5\n",
      "2023-08-04 09:37:21.276045: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000d00 of size 512 next 15\n",
      "2023-08-04 09:37:21.276053: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a000f00 of size 256 next 8\n",
      "2023-08-04 09:37:21.276061: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001000 of size 256 next 14\n",
      "2023-08-04 09:37:21.276069: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001100 of size 512 next 19\n",
      "2023-08-04 09:37:21.276077: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001300 of size 256 next 17\n",
      "2023-08-04 09:37:21.276085: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001400 of size 256 next 18\n",
      "2023-08-04 09:37:21.276093: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001500 of size 256 next 23\n",
      "2023-08-04 09:37:21.276101: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001600 of size 256 next 24\n",
      "2023-08-04 09:37:21.276109: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001700 of size 256 next 22\n",
      "2023-08-04 09:37:21.276117: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001800 of size 256 next 34\n",
      "2023-08-04 09:37:21.276125: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001900 of size 256 next 28\n",
      "2023-08-04 09:37:21.276134: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001a00 of size 256 next 29\n",
      "2023-08-04 09:37:21.276142: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001b00 of size 256 next 26\n",
      "2023-08-04 09:37:21.276150: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001c00 of size 512 next 27\n",
      "2023-08-04 09:37:21.276157: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001e00 of size 256 next 30\n",
      "2023-08-04 09:37:21.276165: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a001f00 of size 256 next 31\n",
      "2023-08-04 09:37:21.276174: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a002000 of size 256 next 32\n",
      "2023-08-04 09:37:21.276182: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a002100 of size 256 next 33\n",
      "2023-08-04 09:37:21.276190: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a002200 of size 256 next 35\n",
      "2023-08-04 09:37:21.276199: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a002300 of size 256 next 36\n",
      "2023-08-04 09:37:21.276208: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a002400 of size 10496 next 9\n",
      "2023-08-04 09:37:21.276216: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a004d00 of size 8448 next 10\n",
      "2023-08-04 09:37:21.276224: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a006e00 of size 512 next 38\n",
      "2023-08-04 09:37:21.276232: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a007000 of size 512 next 40\n",
      "2023-08-04 09:37:21.276241: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a007200 of size 130048 next 21\n",
      "2023-08-04 09:37:21.276251: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a026e00 of size 65536 next 20\n",
      "2023-08-04 09:37:21.276260: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a036e00 of size 3899392 next 13\n",
      "2023-08-04 09:37:21.276268: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a3eee00 of size 2048000 next 12\n",
      "2023-08-04 09:37:21.276278: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a5e2e00 of size 2048000 next 16\n",
      "2023-08-04 09:37:21.276287: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7f1a7d6e00 of size 5714400000 next 25\n",
      "2023-08-04 09:37:21.276295: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f184100 of size 2048000 next 37\n",
      "2023-08-04 09:37:21.276303: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f378100 of size 2048000 next 39\n",
      "2023-08-04 09:37:21.276311: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c100 of size 512 next 41\n",
      "2023-08-04 09:37:21.276319: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c300 of size 512 next 42\n",
      "2023-08-04 09:37:21.276327: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c500 of size 256 next 43\n",
      "2023-08-04 09:37:21.276336: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c600 of size 256 next 44\n",
      "2023-08-04 09:37:21.276344: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c700 of size 256 next 45\n",
      "2023-08-04 09:37:21.276352: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c800 of size 256 next 46\n",
      "2023-08-04 09:37:21.276359: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56c900 of size 256 next 47\n",
      "2023-08-04 09:37:21.276368: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56ca00 of size 256 next 48\n",
      "2023-08-04 09:37:21.276376: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56cb00 of size 256 next 49\n",
      "2023-08-04 09:37:21.276384: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56cc00 of size 256 next 50\n",
      "2023-08-04 09:37:21.276392: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56cd00 of size 256 next 51\n",
      "2023-08-04 09:37:21.276400: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56ce00 of size 256 next 52\n",
      "2023-08-04 09:37:21.276408: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56cf00 of size 256 next 53\n",
      "2023-08-04 09:37:21.276416: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d000 of size 256 next 54\n",
      "2023-08-04 09:37:21.276424: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d100 of size 256 next 55\n",
      "2023-08-04 09:37:21.276432: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d200 of size 256 next 56\n",
      "2023-08-04 09:37:21.276440: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d300 of size 256 next 65\n",
      "2023-08-04 09:37:21.276448: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f806f56d400 of size 256 next 58\n",
      "2023-08-04 09:37:21.276456: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d500 of size 256 next 59\n",
      "2023-08-04 09:37:21.276464: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d600 of size 256 next 72\n",
      "2023-08-04 09:37:21.276474: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d700 of size 256 next 74\n",
      "2023-08-04 09:37:21.276482: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f806f56d800 of size 256 next 68\n",
      "2023-08-04 09:37:21.276490: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f806f56d900 of size 1904800000 next 80\n",
      "2023-08-04 09:37:21.276498: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f80e0dfca00 of size 800000 next 69\n",
      "2023-08-04 09:37:21.276508: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f80e0ebff00 of size 256 next 60\n",
      "2023-08-04 09:37:21.276516: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f80e0ec0000 of size 256 next 81\n",
      "2023-08-04 09:37:21.276524: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f80e0ec0100 of size 512 next 88\n",
      "2023-08-04 09:37:21.276532: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f80e0ec0300 of size 256 next 94\n",
      "2023-08-04 09:37:21.276540: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f80e0ec0400 of size 256 next 57\n",
      "2023-08-04 09:37:21.276548: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f80e0ec0500 of size 256 next 86\n",
      "2023-08-04 09:37:21.276556: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f80e0ec0600 of size 5714400000 next 64\n",
      "2023-08-04 09:37:21.276565: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f823586d900 of size 2400000 next 83\n",
      "2023-08-04 09:37:21.276573: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8235ab7800 of size 256 next 98\n",
      "2023-08-04 09:37:21.276582: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8235ab7900 of size 1904800000 next 89\n",
      "2023-08-04 09:37:21.276590: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f82a7346a00 of size 800000 next 73\n",
      "2023-08-04 09:37:21.276598: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f82a7409f00 of size 5714400000 next 100\n",
      "2023-08-04 09:37:21.276606: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f83fbdb7200 of size 2400000 next 99\n",
      "2023-08-04 09:37:21.276614: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f83fc001100 of size 1904800000 next 93\n",
      "2023-08-04 09:37:21.276623: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f846d890200 of size 800000 next 63\n",
      "2023-08-04 09:37:21.276631: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f846d953700 of size 507627776 next 18446744073709551615\n",
      "2023-08-04 09:37:21.276639: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-08-04 09:37:21.276651: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 46 Chunks of size 256 totalling 11.5KiB\n",
      "2023-08-04 09:37:21.276660: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 8 Chunks of size 512 totalling 4.0KiB\n",
      "2023-08-04 09:37:21.276670: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-08-04 09:37:21.276679: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 8448 totalling 8.2KiB\n",
      "2023-08-04 09:37:21.276689: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 10496 totalling 10.2KiB\n",
      "2023-08-04 09:37:21.276698: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 65536 totalling 64.0KiB\n",
      "2023-08-04 09:37:21.276708: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 130048 totalling 127.0KiB\n",
      "2023-08-04 09:37:21.276717: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 800000 totalling 2.29MiB\n",
      "2023-08-04 09:37:21.276726: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 2048000 totalling 7.81MiB\n",
      "2023-08-04 09:37:21.276735: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2400000 totalling 4.58MiB\n",
      "2023-08-04 09:37:21.276744: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3899392 totalling 3.72MiB\n",
      "2023-08-04 09:37:21.276753: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1904800000 totalling 5.32GiB\n",
      "2023-08-04 09:37:21.276764: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 5714400000 totalling 15.97GiB\n",
      "2023-08-04 09:37:21.276774: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 21.31GiB\n",
      "2023-08-04 09:37:21.276783: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 23384752128 memory_limit_: 23384752128 available bytes: 0 curr_region_allocation_bytes_: 46769504256\n",
      "2023-08-04 09:37:21.276798: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                     23384752128\n",
      "InUse:                     22877123072\n",
      "MaxInUse:                  22891235328\n",
      "NumAllocs:                     2597098\n",
      "MaxAllocSize:               5714400000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-08-04 09:37:21.276819: W tensorflow/tsl/framework/bfc_allocator.cc:497] **************************************************************************************************__\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 2\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(training_data\u001b[39m=\u001b[39;49m [X, y], validation_data\u001b[39m=\u001b[39;49m [X_val, y_val])\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36mEmberModel.train\u001b[0;34m(self, training_data, validation_data)\u001b[0m\n\u001b[1;32m     46\u001b[0m X_val, y_val \u001b[39m=\u001b[39m validation_data\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn epoch \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m}\u001b[39;00m\u001b[39m, learning rate is: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mlr\u001b[39m.\u001b[39mnumpy()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(X, y, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), batch_size\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/asynfed/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/asynfed/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train(training_data= [X, y], validation_data= [X_val, y_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
