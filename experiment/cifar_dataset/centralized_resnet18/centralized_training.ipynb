{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 03:13:17.203446: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-26 03:13:17.232771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-26 03:13:17.670479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# run locally without install asynfed package\n",
    "root = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "# root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n",
    "sys.path.append(root)\n",
    "\n",
    "\n",
    "# tensorflow \n",
    "from asynfed.client.frameworks.tensorflow.tensorflow_framework import TensorflowFramework\n",
    "from resnet18 import Resnet18\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('conf.json', 'r') as json_file:\n",
    "    config = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "config tensorflow using gpu successfully\n",
      "********************\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 03:13:29.386257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 03:13:29.403495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 03:13:29.403535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[config['gpu_index']], 'GPU')\n",
    "    print(\"config tensorflow using gpu successfully\")\n",
    "else:\n",
    "    print(\"There is no gpu or your tensorflow is not built in with gpu support\")\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "\n",
    "\n",
    "epoch = config['training_params']['epoch']\n",
    "batch_size = config['training_params']['batch_size']\n",
    "patience = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data...\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      " 44474368/170498071 [======>.......................] - ETA: 5:06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m==> Preparing data...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_images, train_labels, test_images, test_labels \u001b[39m=\u001b[39m get_dataset()\n\u001b[1;32m      3\u001b[0m data_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_images)\n\u001b[1;32m      5\u001b[0m mean, std \u001b[39m=\u001b[39m get_mean_and_std(train_images)\n",
      "File \u001b[0;32m~/working_dir/AsynFL/experiment/cifar_dataset/centralized_resnet18/utils.py:21\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset\u001b[39m():\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download, parse and process a dataset to unit scale and one-hot labels.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     (train_images, train_labels), (test_images, test_labels) \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mcifar10\u001b[39m.\u001b[39;49mload_data()\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Normalize pixel values to be between 0 and 1\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     train_images, test_images \u001b[39m=\u001b[39m train_images \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m, test_images \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/site-packages/keras/datasets/cifar10.py:81\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m dirname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcifar-10-batches-py\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m origin \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 81\u001b[0m path \u001b[39m=\u001b[39m get_file(\n\u001b[1;32m     82\u001b[0m     dirname,\n\u001b[1;32m     83\u001b[0m     origin\u001b[39m=\u001b[39;49morigin,\n\u001b[1;32m     84\u001b[0m     untar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     85\u001b[0m     file_hash\u001b[39m=\u001b[39;49m(  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     87\u001b[0m     ),\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m num_train_samples \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m\n\u001b[1;32m     92\u001b[0m x_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((num_train_samples, \u001b[39m3\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/site-packages/keras/utils/data_utils.py:346\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m         urlretrieve(origin, fpath, DLProgbar())\n\u001b[1;32m    347\u001b[0m     \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    348\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/site-packages/keras/utils/data_utils.py:87\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m     85\u001b[0m response \u001b[39m=\u001b[39m urlopen(url, data)\n\u001b[1;32m     86\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fd:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunk_read(response, reporthook\u001b[39m=\u001b[39mreporthook):\n\u001b[1;32m     88\u001b[0m         fd\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/site-packages/keras/utils/data_utils.py:76\u001b[0m, in \u001b[0;36murlretrieve.<locals>.chunk_read\u001b[0;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[1;32m     74\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     chunk \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mread(chunk_size)\n\u001b[1;32m     77\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m reporthook \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/asynfed/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print('==> Preparing data...')\n",
    "train_images, train_labels, test_images, test_labels = get_dataset()\n",
    "data_size = len(train_images)\n",
    "\n",
    "mean, std = get_mean_and_std(train_images)\n",
    "train_images = normalize(train_images, mean, std)\n",
    "test_images = normalize(test_images, mean, std)\n",
    "\n",
    "train_ds = dataset_generator(train_images, train_labels, batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).\\\n",
    "        batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "print('==> Data Ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model\n",
    "model = Resnet18(input_features= (32, 32, 3), \n",
    "                 output_features= 10,\n",
    "                 lr=config['training_params']['learning_rate'],\n",
    "                 decay_steps=int(epoch * data_size / batch_size))\n",
    "\n",
    "# Define framework\n",
    "tensorflow_framework = TensorflowFramework(model=model, \n",
    "                                           data_size= data_size, \n",
    "                                           train_ds= train_ds, \n",
    "                                           test_ds= test_ds, \n",
    "                                           config=config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize variables for early stopping check\n",
    "best_val_loss = float(\"inf\")\n",
    "# Number of epochs to wait before stopping training when performance worsens\n",
    "# already set patience above\n",
    "waiting = 0\n",
    "\n",
    "# training with 200 epoch or early stopping\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "print(f\"Training for the total number of epoch {epoch} with batch_size {batch_size} for datasize of {data_size}\")\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "for epoch in range(epoch):\n",
    "    tensorflow_framework.model.train_loss.reset_states()\n",
    "    tensorflow_framework.model.train_performance.reset_states()\n",
    "    tensorflow_framework.model.test_loss.reset_states()\n",
    "    tensorflow_framework.model.test_performance.reset_states()\n",
    "\n",
    "    for images, labels in tensorflow_framework.train_ds:\n",
    "        train_acc, train_loss= tensorflow_framework.fit(images, labels)\n",
    "\n",
    "    for test_images, test_labels in tensorflow_framework.test_ds:\n",
    "        test_acc, test_loss = tensorflow_framework.evaluate(test_images, test_labels)\n",
    "\n",
    "    print(\"Epoch {} - Train Acc: {:.2f} -- Train Loss {} Test Acc {:.2f}  Test Loss {}\".format(epoch+1,\n",
    "                                                                                       train_acc * 100,\n",
    "                                                                                       train_loss,\n",
    "                                                                                       test_acc * 100,\n",
    "                                                                                       test_loss))\n",
    "    \n",
    "    # After each epoch, check the validation loss\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        waiting = 0\n",
    "    else:\n",
    "        waiting += 1\n",
    "\n",
    "    if waiting >= patience:\n",
    "        print(\"Early stopping triggered - ending training.\")\n",
    "        break\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "config tensorflow using gpu successfully\n",
      "********************\n",
      "********************\n",
      "==> Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 06:23:42.596178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22301 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:19:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "Training for the total number of epoch 200 with batch_size 128 for datasize of 50000\n",
      "********************\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 06:23:44.834980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-02 06:23:44.835227: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-02 06:23:48.962951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-02 06:23:49.994582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-02 06:23:50.969598: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f83465b0980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-02 06:23:50.969620: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-07-02 06:23:51.054159: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-07-02 06:24:07.620519: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [10000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Acc: 30.67 -- Train Loss 4.157647132873535 Test Acc 36.56  Test Loss 1.8064653873443604\n",
      "Epoch 2 - Train Acc: 46.36 -- Train Loss 2.94303035736084 Test Acc 45.22  Test Loss 1.5222740173339844\n",
      "Epoch 3 - Train Acc: 56.67 -- Train Loss 2.2669119834899902 Test Acc 58.00  Test Loss 1.1961127519607544\n",
      "Epoch 4 - Train Acc: 64.33 -- Train Loss 1.7996292114257812 Test Acc 62.35  Test Loss 1.0761693716049194\n",
      "Epoch 5 - Train Acc: 69.51 -- Train Loss 1.4914476871490479 Test Acc 57.81  Test Loss 1.244343876838684\n",
      "Epoch 6 - Train Acc: 73.79 -- Train Loss 1.2730768918991089 Test Acc 62.25  Test Loss 1.0836973190307617\n",
      "Epoch 7 - Train Acc: 77.22 -- Train Loss 1.1181416511535645 Test Acc 67.88  Test Loss 0.9717263579368591\n",
      "Epoch 8 - Train Acc: 79.10 -- Train Loss 1.0270195007324219 Test Acc 76.68  Test Loss 0.6771790385246277\n",
      "Epoch 9 - Train Acc: 80.58 -- Train Loss 0.9676506519317627 Test Acc 69.56  Test Loss 0.9019517302513123\n",
      "Epoch 10 - Train Acc: 81.66 -- Train Loss 0.9308469891548157 Test Acc 77.73  Test Loss 0.6521296501159668\n",
      "Epoch 11 - Train Acc: 82.74 -- Train Loss 0.8997418880462646 Test Acc 74.70  Test Loss 0.7835298180580139\n",
      "Epoch 12 - Train Acc: 83.27 -- Train Loss 0.8901016116142273 Test Acc 75.97  Test Loss 0.7349611520767212\n",
      "Epoch 13 - Train Acc: 83.81 -- Train Loss 0.8761307001113892 Test Acc 77.06  Test Loss 0.687477707862854\n",
      "Epoch 14 - Train Acc: 84.23 -- Train Loss 0.8683035373687744 Test Acc 74.53  Test Loss 0.7896942496299744\n",
      "Epoch 15 - Train Acc: 84.52 -- Train Loss 0.8604285717010498 Test Acc 73.31  Test Loss 0.8249528408050537\n",
      "Epoch 16 - Train Acc: 84.95 -- Train Loss 0.8616200089454651 Test Acc 75.25  Test Loss 0.7639822363853455\n",
      "Epoch 17 - Train Acc: 85.41 -- Train Loss 0.8556752800941467 Test Acc 75.46  Test Loss 0.7531030774116516\n",
      "Epoch 18 - Train Acc: 85.74 -- Train Loss 0.8510074019432068 Test Acc 73.44  Test Loss 0.8784980177879333\n",
      "Epoch 19 - Train Acc: 86.07 -- Train Loss 0.8473992943763733 Test Acc 67.85  Test Loss 1.0856178998947144\n",
      "Epoch 20 - Train Acc: 86.16 -- Train Loss 0.8452523946762085 Test Acc 70.71  Test Loss 0.9304863810539246\n",
      "Epoch 21 - Train Acc: 86.34 -- Train Loss 0.8427237868309021 Test Acc 77.65  Test Loss 0.6822800040245056\n",
      "Epoch 22 - Train Acc: 86.60 -- Train Loss 0.8429446220397949 Test Acc 79.56  Test Loss 0.6482306718826294\n",
      "Epoch 23 - Train Acc: 86.75 -- Train Loss 0.8419408202171326 Test Acc 79.37  Test Loss 0.6257410049438477\n",
      "Epoch 24 - Train Acc: 86.73 -- Train Loss 0.8451777100563049 Test Acc 75.07  Test Loss 0.733458399772644\n",
      "Epoch 25 - Train Acc: 87.22 -- Train Loss 0.8378165364265442 Test Acc 71.97  Test Loss 0.8859087824821472\n",
      "Epoch 26 - Train Acc: 87.42 -- Train Loss 0.8362324237823486 Test Acc 81.90  Test Loss 0.5570471286773682\n",
      "Epoch 27 - Train Acc: 87.45 -- Train Loss 0.8353975415229797 Test Acc 77.30  Test Loss 0.6966061592102051\n",
      "Epoch 28 - Train Acc: 87.69 -- Train Loss 0.8289188742637634 Test Acc 78.28  Test Loss 0.6775467991828918\n",
      "Epoch 29 - Train Acc: 87.71 -- Train Loss 0.834120512008667 Test Acc 75.95  Test Loss 0.7852389812469482\n",
      "Epoch 30 - Train Acc: 87.85 -- Train Loss 0.8295255303382874 Test Acc 74.08  Test Loss 0.8226975798606873\n",
      "Epoch 31 - Train Acc: 87.98 -- Train Loss 0.8332961797714233 Test Acc 76.53  Test Loss 0.7424544095993042\n",
      "Epoch 32 - Train Acc: 88.15 -- Train Loss 0.826277494430542 Test Acc 78.51  Test Loss 0.6614564061164856\n",
      "Epoch 33 - Train Acc: 88.39 -- Train Loss 0.8165563941001892 Test Acc 76.39  Test Loss 0.737977921962738\n",
      "Epoch 34 - Train Acc: 88.08 -- Train Loss 0.8228139281272888 Test Acc 80.07  Test Loss 0.6006514430046082\n",
      "Epoch 35 - Train Acc: 88.35 -- Train Loss 0.823886513710022 Test Acc 79.26  Test Loss 0.6616610884666443\n",
      "Epoch 36 - Train Acc: 88.59 -- Train Loss 0.8170524835586548 Test Acc 73.98  Test Loss 0.8646494150161743\n",
      "Epoch 37 - Train Acc: 88.44 -- Train Loss 0.8233004212379456 Test Acc 82.66  Test Loss 0.5215778350830078\n",
      "Epoch 38 - Train Acc: 88.94 -- Train Loss 0.8086414933204651 Test Acc 75.35  Test Loss 0.8097707033157349\n",
      "Epoch 39 - Train Acc: 88.68 -- Train Loss 0.8154035210609436 Test Acc 84.02  Test Loss 0.49323025345802307\n",
      "Epoch 40 - Train Acc: 88.81 -- Train Loss 0.8094939589500427 Test Acc 80.44  Test Loss 0.5931276082992554\n",
      "Epoch 41 - Train Acc: 88.92 -- Train Loss 0.8074226379394531 Test Acc 75.47  Test Loss 0.7630712985992432\n",
      "Epoch 42 - Train Acc: 88.97 -- Train Loss 0.8069203495979309 Test Acc 76.24  Test Loss 0.7073524594306946\n",
      "Epoch 43 - Train Acc: 88.99 -- Train Loss 0.80428147315979 Test Acc 83.38  Test Loss 0.5063807368278503\n",
      "Epoch 44 - Train Acc: 89.18 -- Train Loss 0.8013858199119568 Test Acc 81.17  Test Loss 0.5652511119842529\n",
      "Epoch 45 - Train Acc: 89.31 -- Train Loss 0.7934423685073853 Test Acc 81.80  Test Loss 0.5518453121185303\n",
      "Epoch 46 - Train Acc: 89.31 -- Train Loss 0.7922852635383606 Test Acc 78.64  Test Loss 0.6544385552406311\n",
      "Epoch 47 - Train Acc: 89.23 -- Train Loss 0.7993849515914917 Test Acc 83.95  Test Loss 0.4815163314342499\n",
      "Epoch 48 - Train Acc: 89.27 -- Train Loss 0.7960648536682129 Test Acc 79.58  Test Loss 0.633378803730011\n",
      "Epoch 49 - Train Acc: 89.66 -- Train Loss 0.7859864830970764 Test Acc 79.40  Test Loss 0.6454396843910217\n",
      "Epoch 50 - Train Acc: 89.57 -- Train Loss 0.7897605299949646 Test Acc 79.63  Test Loss 0.6618654727935791\n",
      "Epoch 51 - Train Acc: 89.63 -- Train Loss 0.7850046157836914 Test Acc 76.54  Test Loss 0.7982673048973083\n",
      "Epoch 52 - Train Acc: 89.91 -- Train Loss 0.7766395807266235 Test Acc 76.47  Test Loss 0.7847604155540466\n",
      "Epoch 53 - Train Acc: 89.80 -- Train Loss 0.7816286087036133 Test Acc 82.30  Test Loss 0.5288740992546082\n",
      "Epoch 54 - Train Acc: 90.09 -- Train Loss 0.7714741230010986 Test Acc 77.85  Test Loss 0.6524938941001892\n",
      "Epoch 55 - Train Acc: 89.79 -- Train Loss 0.7758945226669312 Test Acc 82.85  Test Loss 0.5159552693367004\n",
      "Epoch 56 - Train Acc: 89.81 -- Train Loss 0.770145833492279 Test Acc 72.46  Test Loss 0.8523297905921936\n",
      "Epoch 57 - Train Acc: 90.09 -- Train Loss 0.770742654800415 Test Acc 83.37  Test Loss 0.5226868987083435\n",
      "Epoch 58 - Train Acc: 90.32 -- Train Loss 0.7603833675384521 Test Acc 73.72  Test Loss 0.9062860012054443\n",
      "Epoch 59 - Train Acc: 89.98 -- Train Loss 0.7699427008628845 Test Acc 85.54  Test Loss 0.43242132663726807\n",
      "Epoch 60 - Train Acc: 90.21 -- Train Loss 0.7623250484466553 Test Acc 77.91  Test Loss 0.6954001188278198\n",
      "Epoch 61 - Train Acc: 90.27 -- Train Loss 0.760082483291626 Test Acc 76.07  Test Loss 0.7477604150772095\n",
      "Epoch 62 - Train Acc: 90.49 -- Train Loss 0.7571088075637817 Test Acc 79.24  Test Loss 0.6857366561889648\n",
      "Epoch 63 - Train Acc: 90.35 -- Train Loss 0.7585380673408508 Test Acc 78.29  Test Loss 0.6745443344116211\n",
      "Epoch 64 - Train Acc: 90.69 -- Train Loss 0.750283420085907 Test Acc 74.41  Test Loss 0.8192946314811707\n",
      "Epoch 65 - Train Acc: 90.52 -- Train Loss 0.7548121213912964 Test Acc 75.79  Test Loss 0.7496591806411743\n",
      "Epoch 66 - Train Acc: 90.80 -- Train Loss 0.7463207840919495 Test Acc 81.12  Test Loss 0.5975368618965149\n",
      "Epoch 67 - Train Acc: 90.70 -- Train Loss 0.74383544921875 Test Acc 69.79  Test Loss 1.0222307443618774\n",
      "Epoch 68 - Train Acc: 90.68 -- Train Loss 0.7408168911933899 Test Acc 74.28  Test Loss 0.8230713605880737\n",
      "Epoch 69 - Train Acc: 90.90 -- Train Loss 0.7389659285545349 Test Acc 82.12  Test Loss 0.5592200756072998\n",
      "Epoch 70 - Train Acc: 90.84 -- Train Loss 0.7338340878486633 Test Acc 81.91  Test Loss 0.5623576641082764\n",
      "Epoch 71 - Train Acc: 91.17 -- Train Loss 0.7239954471588135 Test Acc 78.95  Test Loss 0.6918695569038391\n",
      "Epoch 72 - Train Acc: 91.24 -- Train Loss 0.7225087881088257 Test Acc 82.09  Test Loss 0.5956820249557495\n",
      "Epoch 73 - Train Acc: 91.12 -- Train Loss 0.7207825779914856 Test Acc 83.41  Test Loss 0.5172387361526489\n",
      "Epoch 74 - Train Acc: 91.21 -- Train Loss 0.7188150882720947 Test Acc 80.53  Test Loss 0.6046117544174194\n",
      "Epoch 75 - Train Acc: 91.28 -- Train Loss 0.7149138450622559 Test Acc 83.48  Test Loss 0.5225694179534912\n",
      "Epoch 76 - Train Acc: 91.47 -- Train Loss 0.7093188166618347 Test Acc 83.67  Test Loss 0.47989311814308167\n",
      "Epoch 77 - Train Acc: 91.42 -- Train Loss 0.712002158164978 Test Acc 80.83  Test Loss 0.6163317561149597\n",
      "Epoch 78 - Train Acc: 91.36 -- Train Loss 0.7131296396255493 Test Acc 83.06  Test Loss 0.5182679295539856\n",
      "Epoch 79 - Train Acc: 91.74 -- Train Loss 0.7024763822555542 Test Acc 87.02  Test Loss 0.4246382415294647\n",
      "Epoch 80 - Train Acc: 91.98 -- Train Loss 0.6926257014274597 Test Acc 80.19  Test Loss 0.632271945476532\n",
      "Epoch 81 - Train Acc: 92.05 -- Train Loss 0.683224081993103 Test Acc 82.61  Test Loss 0.553468644618988\n",
      "Epoch 82 - Train Acc: 91.87 -- Train Loss 0.6903784275054932 Test Acc 85.71  Test Loss 0.4363524913787842\n",
      "Epoch 83 - Train Acc: 92.05 -- Train Loss 0.6813958883285522 Test Acc 85.29  Test Loss 0.47912412881851196\n",
      "Epoch 84 - Train Acc: 92.21 -- Train Loss 0.67722487449646 Test Acc 84.14  Test Loss 0.5047849416732788\n",
      "Epoch 85 - Train Acc: 92.07 -- Train Loss 0.6775149703025818 Test Acc 85.21  Test Loss 0.46656399965286255\n",
      "Epoch 86 - Train Acc: 92.36 -- Train Loss 0.6696703433990479 Test Acc 75.62  Test Loss 0.8081008195877075\n",
      "Epoch 87 - Train Acc: 92.39 -- Train Loss 0.6644637584686279 Test Acc 83.21  Test Loss 0.5262613296508789\n",
      "Epoch 88 - Train Acc: 92.41 -- Train Loss 0.6665929555892944 Test Acc 81.47  Test Loss 0.5914959907531738\n",
      "Epoch 89 - Train Acc: 92.52 -- Train Loss 0.6616292595863342 Test Acc 85.44  Test Loss 0.4398767948150635\n",
      "Epoch 90 - Train Acc: 92.53 -- Train Loss 0.6565771102905273 Test Acc 82.21  Test Loss 0.548926830291748\n",
      "Epoch 91 - Train Acc: 92.79 -- Train Loss 0.6484997272491455 Test Acc 84.82  Test Loss 0.48831963539123535\n",
      "Epoch 92 - Train Acc: 92.82 -- Train Loss 0.6457425355911255 Test Acc 84.82  Test Loss 0.4793325960636139\n",
      "Epoch 93 - Train Acc: 92.71 -- Train Loss 0.644520103931427 Test Acc 83.12  Test Loss 0.556542694568634\n",
      "Epoch 94 - Train Acc: 92.81 -- Train Loss 0.6416159868240356 Test Acc 83.79  Test Loss 0.523944616317749\n",
      "Epoch 95 - Train Acc: 93.04 -- Train Loss 0.633732795715332 Test Acc 82.47  Test Loss 0.5525033473968506\n",
      "Epoch 96 - Train Acc: 93.17 -- Train Loss 0.627973198890686 Test Acc 83.63  Test Loss 0.5458935499191284\n",
      "Epoch 97 - Train Acc: 93.34 -- Train Loss 0.618259847164154 Test Acc 81.02  Test Loss 0.6901801228523254\n",
      "Epoch 98 - Train Acc: 93.14 -- Train Loss 0.6228577494621277 Test Acc 84.87  Test Loss 0.48288094997406006\n",
      "Epoch 99 - Train Acc: 93.43 -- Train Loss 0.6139841079711914 Test Acc 86.46  Test Loss 0.42833781242370605\n",
      "Epoch 100 - Train Acc: 93.43 -- Train Loss 0.6090407371520996 Test Acc 84.24  Test Loss 0.4819914400577545\n",
      "Epoch 101 - Train Acc: 93.85 -- Train Loss 0.5944076776504517 Test Acc 84.31  Test Loss 0.5282032489776611\n",
      "Epoch 102 - Train Acc: 93.45 -- Train Loss 0.6048526763916016 Test Acc 85.08  Test Loss 0.47595930099487305\n",
      "Epoch 103 - Train Acc: 93.67 -- Train Loss 0.5977239012718201 Test Acc 84.23  Test Loss 0.5213364958763123\n",
      "Epoch 104 - Train Acc: 93.91 -- Train Loss 0.5845828056335449 Test Acc 85.49  Test Loss 0.45222797989845276\n",
      "Epoch 105 - Train Acc: 93.95 -- Train Loss 0.5826911926269531 Test Acc 84.34  Test Loss 0.5190537571907043\n",
      "Epoch 106 - Train Acc: 94.02 -- Train Loss 0.580058217048645 Test Acc 82.75  Test Loss 0.5888996124267578\n",
      "Epoch 107 - Train Acc: 94.15 -- Train Loss 0.5763378143310547 Test Acc 80.44  Test Loss 0.6752796173095703\n",
      "Epoch 108 - Train Acc: 94.28 -- Train Loss 0.5689484477043152 Test Acc 87.10  Test Loss 0.40487435460090637\n",
      "Epoch 109 - Train Acc: 94.46 -- Train Loss 0.5591973662376404 Test Acc 79.86  Test Loss 0.6902414560317993\n",
      "Epoch 110 - Train Acc: 94.39 -- Train Loss 0.5582733750343323 Test Acc 82.88  Test Loss 0.5879238247871399\n",
      "Epoch 111 - Train Acc: 94.62 -- Train Loss 0.550851583480835 Test Acc 86.70  Test Loss 0.39938846230506897\n",
      "Epoch 112 - Train Acc: 94.50 -- Train Loss 0.5535032749176025 Test Acc 88.09  Test Loss 0.373214453458786\n",
      "Epoch 113 - Train Acc: 94.93 -- Train Loss 0.5366528034210205 Test Acc 88.95  Test Loss 0.33880728483200073\n",
      "Epoch 114 - Train Acc: 94.82 -- Train Loss 0.5391860008239746 Test Acc 88.78  Test Loss 0.36034056544303894\n",
      "Epoch 115 - Train Acc: 95.12 -- Train Loss 0.5247575044631958 Test Acc 82.13  Test Loss 0.6261553168296814\n",
      "Epoch 116 - Train Acc: 95.40 -- Train Loss 0.5153762698173523 Test Acc 85.58  Test Loss 0.48451465368270874\n",
      "Epoch 117 - Train Acc: 95.50 -- Train Loss 0.5118792057037354 Test Acc 87.90  Test Loss 0.39049890637397766\n",
      "Epoch 118 - Train Acc: 95.09 -- Train Loss 0.5166968107223511 Test Acc 87.70  Test Loss 0.38863080739974976\n",
      "Epoch 119 - Train Acc: 95.39 -- Train Loss 0.5052968263626099 Test Acc 83.85  Test Loss 0.5419937372207642\n",
      "Epoch 120 - Train Acc: 95.29 -- Train Loss 0.5044914484024048 Test Acc 85.83  Test Loss 0.46062183380126953\n",
      "Epoch 121 - Train Acc: 95.57 -- Train Loss 0.4947599768638611 Test Acc 85.15  Test Loss 0.5120819211006165\n",
      "Epoch 122 - Train Acc: 95.68 -- Train Loss 0.4889620840549469 Test Acc 89.51  Test Loss 0.34311443567276\n",
      "Epoch 123 - Train Acc: 95.90 -- Train Loss 0.47993800044059753 Test Acc 87.41  Test Loss 0.41989096999168396\n",
      "Epoch 124 - Train Acc: 95.90 -- Train Loss 0.47554075717926025 Test Acc 90.21  Test Loss 0.3243366777896881\n",
      "Epoch 125 - Train Acc: 96.01 -- Train Loss 0.4732413589954376 Test Acc 89.14  Test Loss 0.3571587800979614\n",
      "Epoch 126 - Train Acc: 96.19 -- Train Loss 0.46340852975845337 Test Acc 89.64  Test Loss 0.34031379222869873\n",
      "Epoch 127 - Train Acc: 96.33 -- Train Loss 0.4549708962440491 Test Acc 86.15  Test Loss 0.48085036873817444\n",
      "Epoch 128 - Train Acc: 96.38 -- Train Loss 0.44823509454727173 Test Acc 83.51  Test Loss 0.5805894136428833\n",
      "Epoch 129 - Train Acc: 96.41 -- Train Loss 0.4419340193271637 Test Acc 86.25  Test Loss 0.5230729579925537\n",
      "Epoch 130 - Train Acc: 96.42 -- Train Loss 0.43917950987815857 Test Acc 88.37  Test Loss 0.3682098686695099\n",
      "Epoch 131 - Train Acc: 96.51 -- Train Loss 0.4333638548851013 Test Acc 90.63  Test Loss 0.31296849250793457\n",
      "Epoch 132 - Train Acc: 96.87 -- Train Loss 0.4230082929134369 Test Acc 85.73  Test Loss 0.5131758451461792\n",
      "Epoch 133 - Train Acc: 96.88 -- Train Loss 0.41554030776023865 Test Acc 89.57  Test Loss 0.36035656929016113\n",
      "Epoch 134 - Train Acc: 96.94 -- Train Loss 0.40887731313705444 Test Acc 88.43  Test Loss 0.41632431745529175\n",
      "Epoch 135 - Train Acc: 97.28 -- Train Loss 0.3965189456939697 Test Acc 88.68  Test Loss 0.39372551441192627\n",
      "Epoch 136 - Train Acc: 97.27 -- Train Loss 0.3921656012535095 Test Acc 90.10  Test Loss 0.3428998291492462\n",
      "Epoch 137 - Train Acc: 97.41 -- Train Loss 0.3848905861377716 Test Acc 90.35  Test Loss 0.3143356144428253\n",
      "Epoch 138 - Train Acc: 97.51 -- Train Loss 0.3752736449241638 Test Acc 89.75  Test Loss 0.3458099663257599\n",
      "Epoch 139 - Train Acc: 97.67 -- Train Loss 0.3685656189918518 Test Acc 87.32  Test Loss 0.49256810545921326\n",
      "Epoch 140 - Train Acc: 97.63 -- Train Loss 0.3635496497154236 Test Acc 89.53  Test Loss 0.3516466021537781\n",
      "Epoch 141 - Train Acc: 97.82 -- Train Loss 0.3543332815170288 Test Acc 89.19  Test Loss 0.3864608108997345\n",
      "Epoch 142 - Train Acc: 97.94 -- Train Loss 0.345856249332428 Test Acc 89.25  Test Loss 0.3855137526988983\n",
      "Epoch 143 - Train Acc: 97.84 -- Train Loss 0.34424644708633423 Test Acc 90.50  Test Loss 0.31442737579345703\n",
      "Epoch 144 - Train Acc: 98.10 -- Train Loss 0.3346816599369049 Test Acc 91.02  Test Loss 0.33766108751296997\n",
      "Epoch 145 - Train Acc: 98.13 -- Train Loss 0.3289002776145935 Test Acc 89.47  Test Loss 0.39115390181541443\n",
      "Epoch 146 - Train Acc: 98.39 -- Train Loss 0.31662124395370483 Test Acc 91.62  Test Loss 0.30151695013046265\n",
      "Epoch 147 - Train Acc: 98.57 -- Train Loss 0.3061000406742096 Test Acc 89.72  Test Loss 0.3605870008468628\n",
      "Epoch 148 - Train Acc: 98.35 -- Train Loss 0.3091195821762085 Test Acc 91.08  Test Loss 0.33012598752975464\n",
      "Epoch 149 - Train Acc: 98.49 -- Train Loss 0.301063597202301 Test Acc 91.22  Test Loss 0.30253884196281433\n",
      "Epoch 150 - Train Acc: 98.60 -- Train Loss 0.29196062684059143 Test Acc 91.41  Test Loss 0.30047690868377686\n",
      "Epoch 151 - Train Acc: 98.71 -- Train Loss 0.28458595275878906 Test Acc 91.27  Test Loss 0.310665488243103\n",
      "Epoch 152 - Train Acc: 98.90 -- Train Loss 0.27440977096557617 Test Acc 92.10  Test Loss 0.27771854400634766\n",
      "Epoch 153 - Train Acc: 98.86 -- Train Loss 0.27033108472824097 Test Acc 92.11  Test Loss 0.2889026701450348\n",
      "Epoch 154 - Train Acc: 99.02 -- Train Loss 0.26104965806007385 Test Acc 90.98  Test Loss 0.3435467481613159\n",
      "Epoch 155 - Train Acc: 99.22 -- Train Loss 0.25099483132362366 Test Acc 92.77  Test Loss 0.2645835280418396\n",
      "Epoch 156 - Train Acc: 99.35 -- Train Loss 0.2421969175338745 Test Acc 93.23  Test Loss 0.24919140338897705\n",
      "Epoch 157 - Train Acc: 99.44 -- Train Loss 0.23430505394935608 Test Acc 92.83  Test Loss 0.26974648237228394\n",
      "Epoch 158 - Train Acc: 99.51 -- Train Loss 0.22789667546749115 Test Acc 93.55  Test Loss 0.252645879983902\n",
      "Epoch 159 - Train Acc: 99.55 -- Train Loss 0.21965943276882172 Test Acc 93.23  Test Loss 0.2636156380176544\n",
      "Epoch 160 - Train Acc: 99.55 -- Train Loss 0.21555203199386597 Test Acc 93.27  Test Loss 0.24519647657871246\n",
      "Epoch 161 - Train Acc: 99.66 -- Train Loss 0.20828206837177277 Test Acc 94.24  Test Loss 0.22919966280460358\n",
      "Epoch 162 - Train Acc: 99.73 -- Train Loss 0.20128631591796875 Test Acc 93.88  Test Loss 0.23495735228061676\n",
      "Epoch 163 - Train Acc: 99.76 -- Train Loss 0.19611181318759918 Test Acc 94.28  Test Loss 0.21741507947444916\n",
      "Epoch 164 - Train Acc: 99.84 -- Train Loss 0.18916794657707214 Test Acc 94.43  Test Loss 0.21204714477062225\n",
      "Epoch 165 - Train Acc: 99.87 -- Train Loss 0.18382857739925385 Test Acc 94.48  Test Loss 0.20279118418693542\n",
      "Epoch 166 - Train Acc: 99.91 -- Train Loss 0.17894358932971954 Test Acc 94.68  Test Loss 0.19388486444950104\n",
      "Epoch 167 - Train Acc: 99.91 -- Train Loss 0.17557135224342346 Test Acc 94.84  Test Loss 0.19653566181659698\n",
      "Epoch 168 - Train Acc: 99.95 -- Train Loss 0.17075203359127045 Test Acc 94.80  Test Loss 0.19303616881370544\n",
      "Epoch 169 - Train Acc: 99.96 -- Train Loss 0.16694936156272888 Test Acc 95.02  Test Loss 0.19443489611148834\n",
      "Epoch 170 - Train Acc: 99.96 -- Train Loss 0.16402629017829895 Test Acc 94.99  Test Loss 0.18919400870800018\n",
      "Epoch 171 - Train Acc: 99.97 -- Train Loss 0.16081073880195618 Test Acc 95.00  Test Loss 0.18773113191127777\n",
      "Epoch 172 - Train Acc: 99.98 -- Train Loss 0.157882422208786 Test Acc 95.16  Test Loss 0.1784931719303131\n",
      "Epoch 173 - Train Acc: 99.99 -- Train Loss 0.15516726672649384 Test Acc 95.04  Test Loss 0.18437297642230988\n",
      "Epoch 174 - Train Acc: 99.99 -- Train Loss 0.15281468629837036 Test Acc 95.39  Test Loss 0.17600677907466888\n",
      "Epoch 175 - Train Acc: 99.99 -- Train Loss 0.1508127599954605 Test Acc 95.46  Test Loss 0.17911894619464874\n",
      "Epoch 176 - Train Acc: 99.99 -- Train Loss 0.14897358417510986 Test Acc 95.34  Test Loss 0.177137091755867\n",
      "Epoch 177 - Train Acc: 99.99 -- Train Loss 0.1472029685974121 Test Acc 95.33  Test Loss 0.1741502434015274\n",
      "Epoch 178 - Train Acc: 99.99 -- Train Loss 0.14564064145088196 Test Acc 95.36  Test Loss 0.17204707860946655\n",
      "Epoch 179 - Train Acc: 99.99 -- Train Loss 0.14433221518993378 Test Acc 95.33  Test Loss 0.17429094016551971\n",
      "Epoch 180 - Train Acc: 99.99 -- Train Loss 0.14295557141304016 Test Acc 95.47  Test Loss 0.17154820263385773\n",
      "Epoch 181 - Train Acc: 100.00 -- Train Loss 0.1417892575263977 Test Acc 95.41  Test Loss 0.17311379313468933\n",
      "Epoch 182 - Train Acc: 100.00 -- Train Loss 0.14086568355560303 Test Acc 95.37  Test Loss 0.17127953469753265\n",
      "Epoch 183 - Train Acc: 100.00 -- Train Loss 0.13993272185325623 Test Acc 95.36  Test Loss 0.17145656049251556\n",
      "Epoch 184 - Train Acc: 100.00 -- Train Loss 0.1390005648136139 Test Acc 95.34  Test Loss 0.17074374854564667\n",
      "Epoch 185 - Train Acc: 100.00 -- Train Loss 0.13852065801620483 Test Acc 95.42  Test Loss 0.17025168240070343\n",
      "Epoch 186 - Train Acc: 100.00 -- Train Loss 0.1378139704465866 Test Acc 95.33  Test Loss 0.17082320153713226\n",
      "Epoch 187 - Train Acc: 100.00 -- Train Loss 0.13725227117538452 Test Acc 95.44  Test Loss 0.16942098736763\n",
      "Epoch 188 - Train Acc: 100.00 -- Train Loss 0.13687558472156525 Test Acc 95.42  Test Loss 0.16858693957328796\n",
      "Epoch 189 - Train Acc: 100.00 -- Train Loss 0.13642795383930206 Test Acc 95.38  Test Loss 0.16918684542179108\n",
      "Epoch 190 - Train Acc: 99.99 -- Train Loss 0.1362329125404358 Test Acc 95.46  Test Loss 0.1695697158575058\n",
      "Epoch 191 - Train Acc: 99.99 -- Train Loss 0.13593406975269318 Test Acc 95.37  Test Loss 0.16917601227760315\n",
      "Epoch 192 - Train Acc: 100.00 -- Train Loss 0.13570477068424225 Test Acc 95.35  Test Loss 0.16895692050457\n",
      "Epoch 193 - Train Acc: 99.99 -- Train Loss 0.13552087545394897 Test Acc 95.38  Test Loss 0.1688326895236969\n",
      "Epoch 194 - Train Acc: 100.00 -- Train Loss 0.13545836508274078 Test Acc 95.40  Test Loss 0.1686403602361679\n",
      "Epoch 195 - Train Acc: 100.00 -- Train Loss 0.13532958924770355 Test Acc 95.41  Test Loss 0.16849853098392487\n",
      "Epoch 196 - Train Acc: 100.00 -- Train Loss 0.135283425450325 Test Acc 95.40  Test Loss 0.1685391664505005\n",
      "Epoch 197 - Train Acc: 99.99 -- Train Loss 0.1352589875459671 Test Acc 95.42  Test Loss 0.16837459802627563\n",
      "Epoch 198 - Train Acc: 99.99 -- Train Loss 0.135139599442482 Test Acc 95.41  Test Loss 0.16827909648418427\n",
      "Epoch 199 - Train Acc: 99.99 -- Train Loss 0.13521330058574677 Test Acc 95.44  Test Loss 0.1681525558233261\n",
      "Epoch 200 - Train Acc: 100.00 -- Train Loss 0.13518787920475006 Test Acc 95.40  Test Loss 0.16846320033073425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize variables for early stopping check\n",
    "best_val_loss = float(\"inf\")\n",
    "# Number of epochs to wait before stopping training when performance worsens\n",
    "# already set patience above\n",
    "waiting = 0\n",
    "\n",
    "# training with 200 epoch or early stopping\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "print(f\"Training for the total number of epoch {epoch} with batch_size {batch_size} for datasize of {data_size}\")\n",
    "print(\"*\" * 20)\n",
    "print(\"*\" * 20)\n",
    "for epoch in range(epoch):\n",
    "    tensorflow_framework.model.train_loss.reset_states()\n",
    "    tensorflow_framework.model.train_performance.reset_states()\n",
    "    tensorflow_framework.model.test_loss.reset_states()\n",
    "    tensorflow_framework.model.test_performance.reset_states()\n",
    "\n",
    "    for images, labels in tensorflow_framework.train_ds:\n",
    "        train_acc, train_loss= tensorflow_framework.fit(images, labels)\n",
    "\n",
    "    for test_images, test_labels in tensorflow_framework.test_ds:\n",
    "        test_acc, test_loss = tensorflow_framework.evaluate(test_images, test_labels)\n",
    "\n",
    "    print(\"Epoch {} - Train Acc: {:.2f} -- Train Loss {} Test Acc {:.2f}  Test Loss {}\".format(epoch+1,\n",
    "                                                                                       train_acc * 100,\n",
    "                                                                                       train_loss,\n",
    "                                                                                       test_acc * 100,\n",
    "                                                                                       test_loss))\n",
    "    \n",
    "    # After each epoch, check the validation loss\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        waiting = 0\n",
    "    else:\n",
    "        waiting += 1\n",
    "\n",
    "    if waiting >= patience:\n",
    "        print(\"Early stopping triggered - ending training.\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "save_location = \"weights2.pkl\"\n",
    "weights = model.get_weights()\n",
    "\n",
    "with open(save_location, 'wb') as f:\n",
    "    import pickle\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asynfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
